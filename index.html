<!doctype html>



  


<html class="theme-next pisces use-motion">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  
    
      
    

    
  

  
    
      
    

    
  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Monda:300,300italic,400,400italic,700,700italic|Lobster Two:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.0.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.0.1" />






<meta name="description" content="学习总结 思想感悟">
<meta property="og:type" content="website">
<meta property="og:title" content="Skye's Blog">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Skye's Blog">
<meta property="og:description" content="学习总结 思想感悟">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Skye's Blog">
<meta name="twitter:description" content="学习总结 思想感悟">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Pisces',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: 0,
      author: '博主'
    }
  };
</script>




  <link rel="canonical" href="http://yoursite.com/"/>

  <title> Skye's Blog </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  










  
  
    
  

  <div class="container one-collumn sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">Skye's Blog</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle">Forever youthful,forever weeping</p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      

      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/07/19/亚马逊EC2搭建Shadowsocks/" itemprop="url">
                  亚马逊EC2搭建Shadowsocks
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-07-19T11:26:26+08:00" content="2016-07-19">
              2016-07-19
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/计算机技巧/" itemprop="url" rel="index">
                    <span itemprop="name">计算机技巧</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/07/19/亚马逊EC2搭建Shadowsocks/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2016/07/19/亚马逊EC2搭建Shadowsocks/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>在网上寻找优秀的科学上网方式中发现，自己搭建Shadowscoks是非常不错的方式，并且AWS有一年的免费使用期限，到期后再买别的VPS就行了。<br><a href="https://segmentfault.com/a/1190000003101075" target="_blank" rel="external">https://segmentfault.com/a/1190000003101075</a><br>上面连接的教程从注册申请AWS到搭建服务已经非常详细，自己再做一些补充。</p>
<h2 id="服务器端配置"><a href="#服务器端配置" class="headerlink" title="服务器端配置"></a>服务器端配置</h2><p>在EC2上创建好Linux服务器后（本人为Ubuntu）需要对其安装环境<br>参考官方文档<a href="http://docs.aws.amazon.com/zh_cn/AWSEC2/latest/UserGuide/putty.html" target="_blank" rel="external">使用 PuTTY 从 Windows 连接到 Linux 实例</a>连接服务器</p>
<h3 id="安装shadowsocks依赖"><a href="#安装shadowsocks依赖" class="headerlink" title="安装shadowsocks依赖"></a>安装shadowsocks依赖</h3><ol>
<li><code>sudo -s // 获取超级管理员权限</code></li>
<li><code>apt-get update // 更新apt-get</code></li>
<li><code>apt-get install python-pip // 安装python包管理工具pip</code></li>
<li><code>pip install shadowsocks // 安装shadowsocks</code></li>
</ol>
<h3 id="配置shadowsocks"><a href="#配置shadowsocks" class="headerlink" title="配置shadowsocks"></a>配置shadowsocks</h3><p><code>vim /etc/shadowsocks.json</code></p>
<p>单一端口配置<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">&quot;server&quot;:&quot;server_ip&quot;, #EC2实例的IP，注意这里我们不能填写公有IP，需要填写私有IP或者0.0.0.0  填0.0.0.0即可</span><br><span class="line">&quot;server_port&quot;:8388, #server端监听的端口，需要在EC2实例中开放此端口</span><br><span class="line">&quot;local_address&quot;: &quot;127.0.0.1&quot;,</span><br><span class="line">&quot;local_port&quot;:1080,</span><br><span class="line">&quot;password&quot;:&quot;password&quot;, #密码</span><br><span class="line">&quot;timeout&quot;:300,</span><br><span class="line">&quot;method&quot;:&quot;aes-256-cfb&quot;, #加密方式</span><br><span class="line">&quot;fast_open&quot;: false #是否开启fast open</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>如果想要把VPN分享给其它人而不泄露自己的密码，也可以在配置文件中设置多端口+多密码的模式，如：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">&quot;server&quot;:&quot;server_ip&quot;, #EC2实例的IP，注意这里我们不能填写公有IP，需要填写私有IP或者0.0.0.0</span><br><span class="line">&quot;local_address&quot;: &quot;127.0.0.1&quot;,</span><br><span class="line">&quot;local_port&quot;:1080,</span><br><span class="line">&quot;port_password&quot;:</span><br><span class="line">&#123;</span><br><span class="line">&quot;8088”: “password8088”,</span><br><span class="line">&quot;8089”: &quot;password8089”</span><br><span class="line">&#125;</span><br><span class="line">&quot;timeout&quot;:300,</span><br><span class="line">&quot;method&quot;:&quot;aes-256-cfb&quot;, #加密方式</span><br><span class="line">&quot;fast_open&quot;: false #是否开启fast open</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="配置完成后启动Shawdowsocks"><a href="#配置完成后启动Shawdowsocks" class="headerlink" title="配置完成后启动Shawdowsocks"></a>配置完成后启动Shawdowsocks</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">启动：ssserver -c /etc/shadowsocks.json -d start</span><br><span class="line">停止：ssserver -c /etc/shadowsocks.json -d stop</span><br><span class="line">重启：ssserver -c /etc/shadowsocks.json -d restart</span><br><span class="line">查看状态：ssserver -c /etc/shadowsocks.json -d status</span><br></pre></td></tr></table></figure>
<h3 id="关闭服务器防火墙"><a href="#关闭服务器防火墙" class="headerlink" title="关闭服务器防火墙"></a>关闭服务器防火墙</h3><p><code>sudo ufw disable</code></p>
<h3 id="开启AWS入站端口"><a href="#开启AWS入站端口" class="headerlink" title="开启AWS入站端口"></a>开启AWS入站端口</h3><p>配置好shaodowsocks后，还需要将配置中的端口打开,这样客户端的服务才能链接得上EC2中的shadowsocks服务。<br>在EC2网页中编辑入站规则将配置文件中的端口号（如8388）加入入站规则。<br>服务器端配置完毕。</p>
<h2 id="客户端下载"><a href="#客户端下载" class="headerlink" title="客户端下载"></a>客户端下载</h2><p><a href="https://www.gaotizi.com/knowledgebase/2/shadowsocks.html" target="_blank" rel="external">shadowsocks下载地址1</a><br><a href="https://shadowsocks.com/client.html" target="_blank" rel="external">shadowsocks下载地址2</a><br>windows10尽量使用2.3版本，否则可能出现500或者502错误<br><a href="http://pan.baidu.com/s/1hqIk4mS" target="_blank" rel="external">http://pan.baidu.com/s/1hqIk4mS</a></p>
<h3 id="500或者502错误"><a href="#500或者502错误" class="headerlink" title="500或者502错误"></a>500或者502错误</h3><ul>
<li>使用2.3版本</li>
<li>或者尝试以下命令<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">netsh interface ipv4  reset</span><br><span class="line">netsh interface ipv6  reset</span><br><span class="line">netsh winsock reset</span><br></pre></td></tr></table></figure></li>
</ul>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/07/13/window下搭建eclipse运行MapReduce环境/" itemprop="url">
                  window下搭建eclipse运行MapReduce环境
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-07-13T09:47:21+08:00" content="2016-07-13">
              2016-07-13
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/大数据/" itemprop="url" rel="index">
                    <span itemprop="name">大数据</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/07/13/window下搭建eclipse运行MapReduce环境/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2016/07/13/window下搭建eclipse运行MapReduce环境/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="系统环境及所需文件"><a href="#系统环境及所需文件" class="headerlink" title="系统环境及所需文件"></a>系统环境及所需文件</h2><ul>
<li>eclipse-jee-mars-2</li>
<li>hadoop2.7.2</li>
<li><a href="https://github.com/winghc/hadoop2x-eclipse-plugin" target="_blank" rel="external">hadoop-eclipse-plugin</a></li>
<li><a href="http://download.csdn.net/download/chenxf10/9564165" target="_blank" rel="external">hadoop.dll &amp; winutils.exe</a></li>
</ul>
<h2 id="修改Master节点的hdfs-site-xml"><a href="#修改Master节点的hdfs-site-xml" class="headerlink" title="修改Master节点的hdfs-site.xml"></a>修改Master节点的hdfs-site.xml</h2><pre><code>&lt;property&gt;      
    &lt;name&gt;dfs.permissions&lt;/name&gt;      
    &lt;value&gt;false&lt;/value&gt;  
&lt;/property&gt; 
</code></pre><p>旨在取消权限检查</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt; </span><br><span class="line">	&lt;name&gt;dfs.web.ugi&lt;/name&gt; </span><br><span class="line">	&lt;value&gt;Skye,supergroup&lt;/value&gt; </span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>
<h2 id="配置Hadoop插件"><a href="#配置Hadoop插件" class="headerlink" title="配置Hadoop插件"></a>配置Hadoop插件</h2><ol>
<li>windows下载hadoop-2.7.2解压到某目录下，如：E:\hadoop\hadoop-2.7.2</li>
<li>下载hadoop-eclipse-plugin插件hadoop-eclipse-plugin，将release目录下的hadoop-eclipse-plugin-2.6.0.jar拷贝到eclipse/plugins，重启eclipse。</li>
<li>插件配置windows-&gt;show view-&gt;other 显示mapreduce视图</li>
<li>window-&gt;preferences-&gt;hadoop map/reduce 指定windows上的hadoop根目录（即：E:\hadoop\hadoop-2.7.2）</li>
<li>在Map/Reduce Locations 面板中，点击小象图标定义hadoop<br><img src="http://i.imgur.com/61yzGm9.png" alt=""><br>解释：<br>MapReduce Master<br>Host：虚拟机hadoop master对应ip<br>Port：hdfs-site.xml中dfs.datanode.ipc.address指定的的端口号。此处填9001<br>DFS Master中Port：core-site.xml中fs.defaultFS指定的端口。应填9000<br>User name：linux中运行hadoop的用户。 </li>
</ol>
<h2 id="配置完毕查看结果"><a href="#配置完毕查看结果" class="headerlink" title="配置完毕查看结果"></a>配置完毕查看结果</h2><p><img src="http://i.imgur.com/zVOXsbw.png" alt=""></p>
<h2 id="windows下运行环境配置"><a href="#windows下运行环境配置" class="headerlink" title="windows下运行环境配置"></a>windows下运行环境配置</h2><ol>
<li>在系统环境变量中增加HADOOP_HOME，并在Path中加入%HADOOP_HOME%\bin</li>
<li>将下载下来的hadoop.dll,winutils.exe拷贝到HADOOP_HOME/bin目录下</li>
</ol>
<h2 id="创建-MapReduce工程并运行"><a href="#创建-MapReduce工程并运行" class="headerlink" title="创建 MapReduce工程并运行"></a>创建 MapReduce工程并运行</h2><p>需要拷贝 服务器hadoop中的log4j.properties文件到工程的src目录</p>
<p><code>run on hadoop</code></p>
<p>运行时报如下错误，弄了好长一段时间，发现原因是服务器通过内网ip访问，外网无法解析。用虚拟机连接成功.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line">16/07/13 10:42:38 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.</span><br><span class="line">16/07/13 10:42:39 INFO mapreduce.Job: Job job_local510776960_0001 running in uber mode : false</span><br><span class="line">16/07/13 10:42:39 INFO mapreduce.Job:  map 0% reduce 0%</span><br><span class="line">16/07/13 10:42:39 INFO mapred.Task:  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@3bfe5dd7</span><br><span class="line">16/07/13 10:42:39 INFO mapred.MapTask: Processing split: hdfs://Master:9000/test/test3.txt:0+259</span><br><span class="line">16/07/13 10:42:39 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)</span><br><span class="line">16/07/13 10:42:39 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100</span><br><span class="line">16/07/13 10:42:39 INFO mapred.MapTask: soft limit at 83886080</span><br><span class="line">16/07/13 10:42:39 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600</span><br><span class="line">16/07/13 10:42:39 INFO mapred.MapTask: kvstart = 26214396; length = 6553600</span><br><span class="line">16/07/13 10:42:39 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer</span><br><span class="line">16/07/13 10:43:00 WARN hdfs.BlockReaderFactory: I/O error constructing remote block reader.</span><br><span class="line">java.net.ConnectException: Connection timed out: no further information</span><br><span class="line">	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)</span><br><span class="line">	at sun.nio.ch.SocketChannelImpl.finishConnect(Unknown Source)</span><br><span class="line">	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)</span><br><span class="line">	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)</span><br><span class="line">	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)</span><br><span class="line">	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)</span><br><span class="line">	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)</span><br><span class="line">	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)</span><br><span class="line">	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:656)</span><br><span class="line">	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:882)</span><br><span class="line">	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)</span><br><span class="line">	at java.io.DataInputStream.read(Unknown Source)</span><br><span class="line">	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:59)</span><br><span class="line">	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)</span><br><span class="line">	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)</span><br><span class="line">	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:91)</span><br><span class="line">	at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:144)</span><br><span class="line">	at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:184)</span><br><span class="line">	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:556)</span><br><span class="line">	at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)</span><br><span class="line">	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)</span><br><span class="line">	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145)</span><br><span class="line">	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)</span><br><span class="line">	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)</span><br><span class="line">	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)</span><br><span class="line">	at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)</span><br><span class="line">	at java.util.concurrent.FutureTask.run(Unknown Source)</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)</span><br><span class="line">	at java.lang.Thread.run(Unknown Source)</span><br><span class="line">16/07/13 10:43:00 WARN hdfs.DFSClient: Failed to connect to /10.0.0.14:50010 for block, add to deadNodes and continue. java.net.ConnectException: Connection timed out: no further information</span><br><span class="line">java.net.ConnectException: Connection timed out: no further information</span><br><span class="line">	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)</span><br><span class="line">	at sun.nio.ch.SocketChannelImpl.finishConnect(Unknown Source)</span><br><span class="line">	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)</span><br><span class="line">	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)</span><br><span class="line">	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)</span><br><span class="line">	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)</span><br><span class="line">	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)</span><br><span class="line">	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)</span><br><span class="line">	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:656)</span><br><span class="line">	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:882)</span><br><span class="line">	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)</span><br><span class="line">	at java.io.DataInputStream.read(Unknown Source)</span><br><span class="line">	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:59)</span><br><span class="line">	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)</span><br><span class="line">	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)</span><br><span class="line">	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:91)</span><br><span class="line">	at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:144)</span><br><span class="line">	at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:184)</span><br><span class="line">	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:556)</span><br><span class="line">	at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)</span><br><span class="line">	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)</span><br><span class="line">	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145)</span><br><span class="line">	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)</span><br><span class="line">	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)</span><br><span class="line">	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)</span><br><span class="line">	at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)</span><br><span class="line">	at java.util.concurrent.FutureTask.run(Unknown Source)</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)</span><br><span class="line">	at java.lang.Thread.run(Unknown Source)</span><br></pre></td></tr></table></figure></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/07/11/mapreduce任务运行时shuffle Error/" itemprop="url">
                  mapreduce任务运行时shuffle Error
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-07-11T12:36:36+08:00" content="2016-07-11">
              2016-07-11
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/大数据/" itemprop="url" rel="index">
                    <span itemprop="name">大数据</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/07/11/mapreduce任务运行时shuffle Error/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2016/07/11/mapreduce任务运行时shuffle Error/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><em>本文引用参考</em>：<a href="http://blog.csdn.net/dslztx/article/details/46445725" target="_blank" rel="external">MapReduce任务Shuffle Error错误</a><br><em>相关参考连接</em>：<a href="http://blog.csdn.net/stark_summer/article/details/48494391" target="_blank" rel="external"> yarn &amp; mapreduce 配置参数总结</a></p>
<h2 id="错误描述"><a href="#错误描述" class="headerlink" title="错误描述"></a>错误描述</h2><p>在运行MapReduce任务的时候，出现如下错误：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">Error: org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#1</span><br><span class="line">        at org.apache.hadoop.mapreduce.task.reduce.Shuffle.run(Shuffle.java:134)</span><br><span class="line">        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:376)</span><br><span class="line">        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)</span><br><span class="line">        at java.security.AccessController.doPrivileged(Native Method)</span><br><span class="line">        at javax.security.auth.Subject.doAs(Subject.java:396)</span><br><span class="line">        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)</span><br><span class="line">        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)</span><br><span class="line">Caused by: java.lang.OutOfMemoryError: Java heap space</span><br><span class="line">        at org.apache.hadoop.io.BoundedByteArrayOutputStream.&lt;init&gt;(BoundedByteArrayOutputStream.java:56)</span><br><span class="line">        at org.apache.hadoop.io.BoundedByteArrayOutputStream.&lt;init&gt;(BoundedByteArrayOutputStream.java:46)</span><br><span class="line">        at org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput.&lt;init&gt;(InMemoryMapOutput.java:63)</span><br><span class="line">        at org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl.unconditionalReserve(MergeManagerImpl.java:297)</span><br><span class="line">        at org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl.reserve(MergeManagerImpl.java:287)</span><br><span class="line">        at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyMapOutput(Fetcher.java:411)</span><br><span class="line">        at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyFromHost(Fetcher.java:341)</span><br><span class="line">        at org.apache.hadoop.mapreduce.task.reduce.Fetcher.run(Fetcher.java:165)</span><br></pre></td></tr></table></figure></p>
<h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>  根据《Hadoop:The Definitive Guide 4th　Edition》所述(P203-219)，map任务和reduce任务之间要经过一个shuffle过程，该过程复制map任务的输出作为reduce任务的输入。<br>  具体的来说，shuffle过程的输入是：map任务的输出文件，它的输出接收者是：运行reduce任务的机子上的内存buffer，并且shuffle过程以并行方式运行。<br>  参数mapreduce.reduce.shuffle.input.buffer.percent控制运行reduce任务的机子上多少比例的内存用作上述buffer(默认值为0.70)，参数mapreduce.reduce.shuffle.parallelcopies控制shuffle过程的并行度(默认值为5)。那么”mapreduce.reduce.shuffle.input.buffer.percent” * “mapreduce.reduce.shuffle.parallelcopies” 必须小于等于1，否则就会出现如上错误<br>因此，我将mapreduce.reduce.shuffle.input.buffer.percent设置成值为0.1，就可以正常运行了（设置成0.2，还是会抛同样的错）</p>
<p><code>job.getConfiguration().setStrings(&quot;mapreduce.reduce.shuffle.input.buffer.percent&quot;, &quot;0.1&quot;);</code><br>或者在<code>maperd-site.xml</code>中修改<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">   &lt;name&gt;mapreduce.reduce.input.buffer.percent&lt;/name&gt;</span><br><span class="line">   &lt;value&gt;0.0&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></p>
<p>  另外，可以发现如果使用两个参数的默认值，那么两者乘积为3.5，大大大于1了，为什么没有经常抛出以上的错误呢？<br>1)首先，把默认值设为比较大，主要是基于性能考虑，将它们设为比较大，可以大大加快从map复制数据的速度<br>2)其次，要抛出如上异常，还需满足另外一个条件，就是map任务的数据一下子准备好了等待shuffle去复制，在这种情况下，就会导致shuffle过程的“线程数量”和“内存buffer使用量”都是满负荷的值，自然就造成了内存不足的错误；而如果map任务的数据是断断续续完成的，那么没有一个时刻shuffle过程的“线程数量”和“内存buffer使用量”是满负荷值的，自然也就不会抛出如上错误</p>
<p>另外，如果在设置以上参数后，还是出现错误，那么有可能是运行Reduce任务的进程的内存总量不足，可以通过mapred.child.java.opts参数来调节，比如设置mapred.child.java.opts=-Xmx2024m</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/06/16/HBase集群搭建/" itemprop="url">
                  HBase集群搭建
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-06-16T16:26:16+08:00" content="2016-06-16">
              2016-06-16
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/大数据/" itemprop="url" rel="index">
                    <span itemprop="name">大数据</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/06/16/HBase集群搭建/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2016/06/16/HBase集群搭建/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="1-上传hbase安装包"><a href="#1-上传hbase安装包" class="headerlink" title="1. 上传hbase安装包"></a>1. 上传hbase安装包</h2><h2 id="2-解压"><a href="#2-解压" class="headerlink" title="2. 解压"></a>2. 解压</h2><h2 id="3-配置hbase集群，要修改3个文件（首先zk集群已经安装好了）"><a href="#3-配置hbase集群，要修改3个文件（首先zk集群已经安装好了）" class="headerlink" title="3. 配置hbase集群，要修改3个文件（首先zk集群已经安装好了）"></a>3. 配置hbase集群，要修改3个文件（首先zk集群已经安装好了）</h2><p><em>注意：要把hadoop的hdfs-site.xml和core-site.xml 放到hbase/conf下</em></p>
<h4 id="vim-hbase-env-sh"><a href="#vim-hbase-env-sh" class="headerlink" title="vim hbase-env.sh"></a>vim hbase-env.sh</h4><pre><code>export JAVA_HOME=/usr/java/jdk1.7.0_55
//告诉hbase使用外部的zk 
export HBASE_MANAGES_ZK=false
</code></pre><h4 id="vim-hbase-site-xml"><a href="#vim-hbase-site-xml" class="headerlink" title="vim hbase-site.xml"></a>vim hbase-site.xml</h4><pre><code> &lt;configuration&gt;
    &lt;!-- 指定hbase在HDFS上存储的路径 --&gt;
    &lt;property&gt;
            &lt;name&gt;hbase.rootdir&lt;/name&gt;
            &lt;value&gt;hdfs://ns1/hbase&lt;/value&gt;
    &lt;/property&gt;
    &lt;!-- 指定hbase是分布式的 --&gt;
    &lt;property&gt;
            &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;
            &lt;value&gt;true&lt;/value&gt;
    &lt;/property&gt;
    &lt;!-- 指定zk的地址，多个用“,”分割 --&gt;
    &lt;property&gt;
            &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;
            &lt;value&gt;itcast04:2181,itcast05:2181,itcast06:2181&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;
</code></pre><h4 id="vim-regionservers"><a href="#vim-regionservers" class="headerlink" title="vim regionservers"></a>vim regionservers</h4><pre><code>itcast03
itcast04
itcast05
itcast06
</code></pre><h2 id="4-将配置好的HBase拷贝到每一个节点并同步时间"><a href="#4-将配置好的HBase拷贝到每一个节点并同步时间" class="headerlink" title="4. 将配置好的HBase拷贝到每一个节点并同步时间"></a>4. 将配置好的HBase拷贝到每一个节点并同步时间</h2><pre><code>scp -r /itcast/hbase-0.96.2-hadoop2/ itcast02:/itcast/
scp -r /itcast/hbase-0.96.2-hadoop2/ itcast03:/itcast/
scp -r /itcast/hbase-0.96.2-hadoop2/ itcast04:/itcast/
scp -r /itcast/hbase-0.96.2-hadoop2/ itcast05:/itcast/
scp -r /itcast/hbase-0.96.2-hadoop2/ itcast06:/itcast/
</code></pre><h2 id="5-启动所有的hbase"><a href="#5-启动所有的hbase" class="headerlink" title="5. 启动所有的hbase"></a>5. 启动所有的hbase</h2><pre><code>分别启动zk
    ./zkServer.sh start
启动hbase集群
    start-dfs.sh
启动hbase，在主节点上运行：
    start-hbase.sh
</code></pre><h2 id="6-通过浏览器访问hbase管理页面"><a href="#6-通过浏览器访问hbase管理页面" class="headerlink" title="6.通过浏览器访问hbase管理页面"></a>6.通过浏览器访问hbase管理页面</h2><pre><code>192.168.1.201:60010
</code></pre><h2 id="7-为保证集群的可靠性，要启动多个HMaster-在itcast02上启动"><a href="#7-为保证集群的可靠性，要启动多个HMaster-在itcast02上启动" class="headerlink" title="7.为保证集群的可靠性，要启动多个HMaster,在itcast02上启动"></a>7.为保证集群的可靠性，要启动多个HMaster,在itcast02上启动</h2><pre><code>hbase-daemon.sh start master
</code></pre>
          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/06/16/MapReduce 多文件输入/" itemprop="url">
                  MapReduce 多文件输入
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-06-16T16:26:16+08:00" content="2016-06-16">
              2016-06-16
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/大数据/" itemprop="url" rel="index">
                    <span itemprop="name">大数据</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/06/16/MapReduce 多文件输入/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2016/06/16/MapReduce 多文件输入/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="多路径输入"><a href="#多路径输入" class="headerlink" title="多路径输入"></a>多路径输入</h2><ol>
<li><p>FileInputFormat.addInputPath 多次调用加载不同路径</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">FileInputFormat.addInputPath(job, new Path(args[0]));</span><br><span class="line">FileInputFormat.addInputPath(job, new Path(args[1]));</span><br></pre></td></tr></table></figure>
</li>
<li><p>FileInputFormat.addInputPaths一次调用加载 多路径字符串用逗号隔开</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">FileInputFormat.addInputPaths(job, &quot;hdfs://master:9000/cs/path1,hdfs://RS5-112:9000/cs/path2&quot;);</span><br></pre></td></tr></table></figure>
</li>
<li><p>多种输入**MultipleInputs可以加载不同路径的输入文件，并且每个路径可用不同的</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">maperMultipleInputs.addInputPath(job, new Path(&quot;hdfs://master:9000/cs/path1&quot;), TextInputFormat.class,MultiTypeFileInput1Mapper.class);</span><br><span class="line">MultipleInputs.addInputPath(job, new Path(&quot;hdfs://master:9000/cs/path3&quot;), TextInputFormat.class,MultiTypeFileInput3Mapper.class);</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h2 id="网上例子："><a href="#网上例子：" class="headerlink" title="网上例子："></a>网上例子：</h2><pre><code>package example;

import java.io.IOException;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.MultipleInputs;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
/**
* 多类型文件输入
* @author lijl
*
*/

public class MultiTypeFileInputMR {
static class MultiTypeFileInput1Mapper extends Mapper&lt;LongWritable, Text, Text, Text&gt;{
public void map(LongWritable key,Text value,Context context){
try {
String[] str = value.toString().split(&quot;\\|&quot;);
context.write(new Text(str[0]), new Text(str[1]));
} catch (IOException e) {
e.printStackTrace();
} catch (InterruptedException e) {
e.printStackTrace();
}
}
}
static class MultiTypeFileInput3Mapper extends Mapper&lt;LongWritable, Text, Text, Text&gt;{
public void map(LongWritable key,Text value,Context context){
try {
String[] str = value.toString().split(&quot;&quot;);
context.write(new Text(str[0]), new Text(str[1]));
} catch (IOException e) {
e.printStackTrace();
} catch (InterruptedException e) {
e.printStackTrace();
}
}
}
static class MultiTypeFileInputReducer extends Reducer&lt;Text, Text, Text, Text&gt;{
public void reduce(Text key,Iterable&lt;Text&gt; values,Context context){
try {
for(Text value:values){
context.write(key,value);
}

} catch (IOException e) {
e.printStackTrace();
} catch (InterruptedException e) {
e.printStackTrace();
}
}
}

public static void main(String[] args) throws IOException, InterruptedException, ClassNotFoundException {
Configuration conf = new Configuration();
conf.set(&quot;mapred.textoutputformat.separator&quot;, &quot;,&quot;);
Job job = new Job(conf,&quot;MultiPathFileInput&quot;);
job.setJarByClass(MultiTypeFileInputMR.class);
FileOutputFormat.setOutputPath(job, new Path(&quot;hdfs://RS5-112:9000/cs/path6&quot;));

job.setMapOutputKeyClass(Text.class);
job.setMapOutputValueClass(Text.class);
job.setOutputKeyClass(Text.class);
job.setOutputValueClass(Text.class);

job.setReducerClass(MultiTypeFileInputReducer.class);
job.setNumReduceTasks(1);
MultipleInputs.addInputPath(job, new Path(&quot;hdfs://RS5-112:9000/cs/path1&quot;), TextInputFormat.class,MultiTypeFileInput1Mapper.class);
MultipleInputs.addInputPath(job, new Path(&quot;hdfs://RS5-112:9000/cs/path3&quot;), TextInputFormat.class,MultiTypeFileInput3Mapper.class);
System.exit(job.waitForCompletion(true)?0:1);
}

}
</code></pre><h2 id="自己例子"><a href="#自己例子" class="headerlink" title="自己例子"></a>自己例子</h2><p>QLMapper.java<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">package com.hdu.mr;</span><br><span class="line"></span><br><span class="line">import java.io.IOException;</span><br><span class="line"></span><br><span class="line">import org.apache.hadoop.io.LongWritable;</span><br><span class="line">import org.apache.hadoop.io.Text;</span><br><span class="line">import org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line">public class QLMapper extends Mapper&lt;LongWritable, Text, Text, LongWritable&gt; &#123;</span><br><span class="line">	String[] mbUnlike = &#123; &quot;盒子&quot;, &quot;助手&quot;, &quot;输入法&quot;, &quot;平台&quot; &#125;;</span><br><span class="line">	String mbdylxLike = &quot;游戏&quot;;</span><br><span class="line">	String mbdylxUnlike = &quot;网页游戏&quot;;</span><br><span class="line">	String delWeb = &quot;访问网站&quot;;</span><br><span class="line">	Text outputValue = new Text();</span><br><span class="line"></span><br><span class="line">	@Override</span><br><span class="line">	protected void map(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, LongWritable&gt;.Context context)</span><br><span class="line">			throws IOException, InterruptedException &#123;</span><br><span class="line">		// 接收数据v1</span><br><span class="line">		String line = value.toString();</span><br><span class="line">		// 切分数据</span><br><span class="line">		String[] words = line.split(&quot;&quot;);</span><br><span class="line">		// String[] words = line.split(&quot;\t&quot;);</span><br><span class="line"></span><br><span class="line">		boolean flag = true;</span><br><span class="line"></span><br><span class="line">		for (int i = 0; i &lt; 4; i++) &#123;</span><br><span class="line">			if (words.length &lt; 5) &#123; // 过滤 长度小于4的信息 即访问网站等</span><br><span class="line">				flag = false;</span><br><span class="line">				break;</span><br><span class="line">			&#125;</span><br><span class="line">			if (words[3].indexOf(mbUnlike[i]) != -1) &#123; // 有其中一个则为false</span><br><span class="line">				flag = false;</span><br><span class="line">				break;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		if (flag == true) &#123;</span><br><span class="line">			if (words[4].indexOf(mbdylxUnlike) != -1) &#123; // 有网页游戏则为false</span><br><span class="line">				flag = false;</span><br><span class="line">			&#125; else if (words[4].indexOf(mbdylxLike) == -1) &#123; // 没有游戏则为false</span><br><span class="line">				flag = false;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">		if (flag == true) &#123;</span><br><span class="line">			outputValue.set(line);</span><br><span class="line">			context.write(outputValue, new LongWritable(1L));</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>QLReducer.java<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">package com.hdu.mr;</span><br><span class="line"></span><br><span class="line">import java.io.IOException;</span><br><span class="line"></span><br><span class="line">import org.apache.hadoop.io.LongWritable;</span><br><span class="line">import org.apache.hadoop.io.NullWritable;</span><br><span class="line">import org.apache.hadoop.io.Text;</span><br><span class="line">import org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line">public class QLReducer extends Reducer&lt;Text, LongWritable, Text, NullWritable&gt; &#123;</span><br><span class="line"></span><br><span class="line">	@Override</span><br><span class="line">	protected void reduce(Text key, Iterable&lt;LongWritable&gt; values,</span><br><span class="line">			Reducer&lt;Text, LongWritable, Text, NullWritable&gt;.Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">		// 接收数据</span><br><span class="line"></span><br><span class="line">		// 输出</span><br><span class="line">		context.write(key, NullWritable.get());</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>DataClean.java<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">package com.hdu.mr;</span><br><span class="line"></span><br><span class="line">import java.io.IOException;</span><br><span class="line"></span><br><span class="line">import org.apache.hadoop.io.LongWritable;</span><br><span class="line">import org.apache.hadoop.io.NullWritable;</span><br><span class="line">import org.apache.hadoop.io.Text;</span><br><span class="line">import org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line">public class QLReducer extends Reducer&lt;Text, LongWritable, Text, NullWritable&gt; &#123;</span><br><span class="line"></span><br><span class="line">	@Override</span><br><span class="line">	protected void reduce(Text key, Iterable&lt;LongWritable&gt; values,</span><br><span class="line">			Reducer&lt;Text, LongWritable, Text, NullWritable&gt;.Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">		// 接收数据</span><br><span class="line"></span><br><span class="line">		// 输出</span><br><span class="line">		context.write(key, NullWritable.get());</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  


          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel  sidebar-panel-active ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.png"
               alt="Skye" />
          <p class="site-author-name" itemprop="name">Skye</p>
          <p class="site-description motion-element" itemprop="description">学习总结 思想感悟</p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">5</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          
            <div class="site-state-item site-state-categories">
              <a href="/categories">
                <span class="site-state-item-count">2</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">3</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="http://weibo.com/1811544100" target="_blank" title="Weibo">
                  
                    <i class="fa fa-fw fa-weibo"></i>
                  
                  Weibo
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://github.com/Skyexu" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
          
        </div>

        
        

        
        

      </section>

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2016</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Skye</span>
</div>

<div class="powered-by">
  由 <a class="theme-link" href="http://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Pisces
  </a>
</div>

        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.0.1"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.0.1"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.0.1"></script>



  

  
    
  

  <script type="text/javascript">
    var duoshuoQuery = {short_name:"skyexu"};
    (function() {
      var ds = document.createElement('script');
      ds.type = 'text/javascript';ds.async = true;
      ds.id = 'duoshuo-script';
      ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
      ds.charset = 'UTF-8';
      (document.getElementsByTagName('head')[0]
      || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
  </script>

  
    
  






  
  
  

  

  

</body>
</html>
