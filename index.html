<!doctype html>



  


<html class="theme-next pisces use-motion">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  
    
      
    

    
  

  
    
      
    

    
  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Monda:300,300italic,400,400italic,700,700italic|Lobster Two:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.0.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.0.1" />






<meta name="description" content="学习总结 思想感悟">
<meta property="og:type" content="website">
<meta property="og:title" content="Skye's Blog">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Skye's Blog">
<meta property="og:description" content="学习总结 思想感悟">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Skye's Blog">
<meta name="twitter:description" content="学习总结 思想感悟">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Pisces',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: 0,
      author: '博主'
    }
  };
</script>




  <link rel="canonical" href="http://yoursite.com/"/>

  <title> Skye's Blog </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  










  
  
    
  

  <div class="container one-collumn sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">Skye's Blog</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle">Forever youthful,forever weeping</p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      

      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/07/25/Mahout Item-based推荐分布式实现/" itemprop="url">
                  Mahout Item-based推荐的分布式实现
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-07-25T16:02:36+08:00" content="2016-07-25">
              2016-07-25
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/大数据/" itemprop="url" rel="index">
                    <span itemprop="name">大数据</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/07/25/Mahout Item-based推荐分布式实现/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2016/07/25/Mahout Item-based推荐分布式实现/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Mahout API地址：<a href="http://apache.github.io/mahout/0.10.1/docs/mahout-mr/overview-summary.html" target="_blank" rel="external">http://apache.github.io/mahout/0.10.1/docs/mahout-mr/overview-summary.html</a></p>
<h2 id="Mahout算法框架自带的推荐器有下面这些："><a href="#Mahout算法框架自带的推荐器有下面这些：" class="headerlink" title="Mahout算法框架自带的推荐器有下面这些："></a>Mahout算法框架自带的推荐器有下面这些：</h2><ul>
<li>GenericUserBasedRecommender：基于用户的推荐器，用户数量少时速度快；</li>
<li>GenericItemBasedRecommender：基于商品推荐器，商品数量少时速度快，尤其当外部提供了商品相似度数据后效率更好；</li>
<li>SlopeOneRecommender：基于slope-one算法的推荐器，在线推荐或更新较快，需要事先大量预处理运算，物品数量少时较好；</li>
<li>SVDRecommender：奇异值分解，推荐效果较好，但之前需要大量预处理运算；</li>
<li>KnnRecommender：基于k近邻算法(KNN)，适合于物品数量较小时；</li>
<li>TreeClusteringRecommender：基于聚类的推荐器，在线推荐较快，之前需要大量预处理运算，用户数量较少时效果好；</li>
<li>Mahout最常用的三个推荐器是上述的前三个，本文主要讨论前两种的使用。</li>
</ul>
<h2 id="接口相关介绍"><a href="#接口相关介绍" class="headerlink" title="接口相关介绍"></a>接口相关介绍</h2><p>基于用户或物品的推荐器主要包括以下几个接口：</p>
<p><code>DataModel</code> 是用户喜好信息的抽象接口，它的具体实现支持从任意类型的数据源抽取用户喜好信息。Taste 默认提供 JDBCDataModel 和 FileDataModel，分别支持从数据库和文件中读取用户的喜好信息。<br><code>UserSimilarity</code> 和 <code>ItemSimilarity</code>。UserSimilarity 用于定义两个用户间的相似度，它是基于协同过滤的推荐引擎的核心部分，可以用来计算用户的“邻居”，这里我们将与当前用户口味相似的用户称为他的邻居。ItemSimilarity 类似的，计算内容之间的相似度。<br><code>UserNeighborhood</code> 用于基于用户相似度的推荐方法中，推荐的内容是基于找到与当前用户喜好相似的邻居用户的方式产生的。UserNeighborhood 定义了确定邻居用户的方法，具体实现一般是基于 UserSimilarity 计算得到的。<br><code>Recommender</code> 是推荐引擎的抽象接口，Taste 中的核心组件。程序中，为它提供一个 DataModel，它可以计算出对不同用户的推荐内容。实际应用中，主要使用它的实现类 <code>GenericUserBasedRecommender</code> 或者 <code>GenericItemBasedRecommender</code>，分别实现基于用户相似度的推荐引擎或者基于内容的推荐引擎。<br><code>RecommenderEvaluator</code>：评分器。<br><code>RecommenderIRStatsEvaluator</code>：搜集推荐性能相关的指标，包括准确率、召回率等等。<br>目前，Mahout为DataModel提供了以下几种实现：</p>
<ul>
<li>org.apache.mahout.cf.taste.impl.model.GenericDataModel</li>
<li>org.apache.mahout.cf.taste.impl.model.GenericBooleanPrefDataModel</li>
<li>org.apache.mahout.cf.taste.impl.model.PlusAnonymousUserDataModel</li>
<li>org.apache.mahout.cf.taste.impl.model.file.FileDataModel</li>
<li>org.apache.mahout.cf.taste.impl.model.hbase.HBaseDataModel</li>
<li>org.apache.mahout.cf.taste.impl.model.cassandra.CassandraDataModel</li>
<li>org.apache.mahout.cf.taste.impl.model.mongodb.MongoDBDataModel</li>
<li>org.apache.mahout.cf.taste.impl.model.jdbc.SQL92JDBCDataModel</li>
<li>org.apache.mahout.cf.taste.impl.model.jdbc.MySQLJDBCDataModel</li>
<li>org.apache.mahout.cf.taste.impl.model.jdbc.PostgreSQLJDBCDataModel</li>
<li>org.apache.mahout.cf.taste.impl.model.jdbc.GenericJDBCDataModel</li>
<li>org.apache.mahout.cf.taste.impl.model.jdbc.SQL92BooleanPrefJDBCDataModel</li>
<li>org.apache.mahout.cf.taste.impl.model.jdbc.MySQLBooleanPrefJDBCDataModel</li>
<li>org.apache.mahout.cf.taste.impl.model.jdbc.PostgreBooleanPrefSQLJDBCDataModel</li>
<li>org.apache.mahout.cf.taste.impl.model.jdbc.ReloadFromJDBCDataModel<br>从类名上就可以大概猜出来每个DataModel的用途，奇怪的是竟然没有HDFS的DataModel，有人实现了一个，请参考<a href="https://issues.apache.org/jira/browse/MAHOUT-1579" target="_blank" rel="external">MAHOUT-1579</a>。</li>
</ul>
<p><code>UserSimilarity</code> 和 <code>ItemSimilarity</code> 相似度实现有以下几种：</p>
<ul>
<li><code>CityBlockSimilarity</code>：基于Manhattan距离相似度</li>
<li><code>EuclideanDistanceSimilarity</code>：基于欧几里德距离计算相似度</li>
<li><code>LogLikelihoodSimilarity</code>：基于对数似然比的相似度</li>
<li><code>PearsonCorrelationSimilarity</code>：基于皮尔逊相关系数计算相似度</li>
<li><code>SpearmanCorrelationSimilarity</code>：基于皮尔斯曼相关系数相似度</li>
<li><code>TanimotoCoefficientSimilarity</code>：基于谷本系数计算相似度</li>
<li><code>UncenteredCosineSimilarity</code>：计算 Cosine 相似度<br>以上相似度的说明，请参考Mahout推荐引擎介绍。</li>
</ul>
<p>UserNeighborhood 主要实现有两种：</p>
<ul>
<li>NearestNUserNeighborhood：对每个用户取固定数量N个最近邻居</li>
<li>ThresholdUserNeighborhood：对每个用户基于一定的限制，取落在相似度限制以内的所有用户为邻居</li>
</ul>
<p>Recommender分为以下几种实现：</p>
<ul>
<li>GenericUserBasedRecommender：基于用户的推荐引擎</li>
<li>GenericBooleanPrefUserBasedRecommender：基于用户的无偏好值推荐引擎</li>
<li>GenericItemBasedRecommender：基于物品的推荐引擎</li>
<li>GenericBooleanPrefItemBasedRecommender：基于物品的无偏好值推荐引擎</li>
</ul>
<p>RecommenderEvaluator有以下几种实现：</p>
<ul>
<li>AverageAbsoluteDifferenceRecommenderEvaluator：计算平均差值</li>
<li>RMSRecommenderEvaluator：计算均方根差</li>
<li>RecommenderIRStatsEvaluator的实现类是GenericRecommenderIRStatsEvaluator。</li>
</ul>
<h2 id="单机运行"><a href="#单机运行" class="headerlink" title="单机运行"></a>单机运行</h2><p>首先，需要在maven中加入对mahout的依赖：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.mahout&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;mahout-core&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;0.9&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.mahout&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;mahout-integration&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;0.9&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.mahout&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;mahout-math&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;0.9&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.mahout&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;mahout-examples&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;0.9&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure></p>
<p>基于用户的推荐，以FileDataModel为例：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">File modelFile modelFile = new File(&quot;intro.csv&quot;);</span><br><span class="line"></span><br><span class="line">DataModel model = new FileDataModel(modelFile);</span><br><span class="line"></span><br><span class="line">//用户相似度，使用基于皮尔逊相关系数计算相似度</span><br><span class="line">UserSimilarity similarity = new PearsonCorrelationSimilarity(model);</span><br><span class="line"></span><br><span class="line">//选择邻居用户，使用NearestNUserNeighborhood实现UserNeighborhood接口，选择邻近的4个用户</span><br><span class="line">UserNeighborhood neighborhood = new NearestNUserNeighborhood(4, similarity, model);</span><br><span class="line"></span><br><span class="line">Recommender recommender = new GenericUserBasedRecommender(model, neighborhood, similarity);</span><br><span class="line"></span><br><span class="line">//给用户1推荐4个物品</span><br><span class="line">List&lt;RecommendedItem&gt; recommendations = recommender.recommend(1, 4);</span><br><span class="line"></span><br><span class="line">for (RecommendedItem recommendation : recommendations) &#123;</span><br><span class="line">    System.out.println(recommendation);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意： FileDataModel要求输入文件中的字段分隔符为逗号或者制表符，如果你想使用其他分隔符，你可以扩展一个FileDataModel的实现，例如，mahout中已经提供了一个解析MoiveLens的数据集（分隔符为::）的实现GroupLensDataModel。</p>
</blockquote>
<p><code>GenericUserBasedRecommender</code>是基于用户的简单推荐器实现类，推荐主要参照传入的DataModel和UserNeighborhood，总体是三个步骤：</p>
<ol>
<li>从UserNeighborhood获取当前用户Ui最相似的K个用户集合{U1, U2, …Uk}；</li>
<li>从这K个用户集合排除Ui的偏好商品，剩下的Item集合为{Item0, Item1, …Itemm}；</li>
<li>对Item集合里每个Itemj计算Ui可能偏好程度值pref(Ui, Itemj)，并把Item按此数值从高到低排序，前N个item推荐给用户Ui。</li>
<li>对相同用户重复获得推荐结果，我们可以改用CachingRecommender来包装GenericUserBasedRecommender对象，将推荐结果缓存起来：</li>
</ol>
<p><code>Recommender cachingRecommender = new CachingRecommender(recommender);</code></p>
<p>上面代码可以在main方法中直接运行，然后，我们可以获取推荐模型的评分：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">//使用平均绝对差值获得评分</span><br><span class="line">RecommenderEvaluator evaluator = new AverageAbsoluteDifferenceRecommenderEvaluator();</span><br><span class="line">// 用RecommenderBuilder构建推荐引擎</span><br><span class="line">RecommenderBuilder recommenderBuilder = new RecommenderBuilder() &#123;</span><br><span class="line">    @Override</span><br><span class="line">    public Recommender buildRecommender(DataModel model) throws TasteException &#123;</span><br><span class="line">        UserSimilarity similarity = new PearsonCorrelationSimilarity(model);</span><br><span class="line">        UserNeighborhood neighborhood = new NearestNUserNeighborhood(4, similarity, model);</span><br><span class="line">        return new GenericUserBasedRecommender(model, neighborhood, similarity);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line">// Use 70% of the data to train; test using the other 30%.</span><br><span class="line">double score = evaluator.evaluate(recommenderBuilder, null, model, 0.7, 1.0);</span><br><span class="line">System.out.println(score);</span><br></pre></td></tr></table></figure></p>
<p>接下来，可以获取推荐结果的查准率和召回率：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">RecommenderIRStatsEvaluator statsEvaluator = new GenericRecommenderIRStatsEvaluator();</span><br><span class="line">// Build the same recommender for testing that we did last time:</span><br><span class="line">RecommenderBuilder recommenderBuilder = new RecommenderBuilder() &#123;</span><br><span class="line">    @Override</span><br><span class="line">    public Recommender buildRecommender(DataModel model) throws TasteException &#123;</span><br><span class="line">        UserSimilarity similarity = new PearsonCorrelationSimilarity(model);</span><br><span class="line">        UserNeighborhood neighborhood = new NearestNUserNeighborhood(4, similarity, model);</span><br><span class="line">        return new GenericUserBasedRecommender(model, neighborhood, similarity);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line">// 计算推荐4个结果时的查准率和召回率</span><br><span class="line">IRStatistics stats = statsEvaluator.evaluate(recommenderBuilder,null, model, null, 4,</span><br><span class="line">        GenericRecommenderIRStatsEvaluator.CHOOSE_THRESHOLD,1.0);</span><br><span class="line">System.out.println(stats.getPrecision());</span><br><span class="line">System.out.println(stats.getRecall());</span><br></pre></td></tr></table></figure></p>
<p>如果是基于物品的推荐，代码大体相似，只是没有了UserNeighborhood，然后将上面代码中的User换成Item即可，完整代码如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">File modelFile modelFile = new File(&quot;intro.csv&quot;);</span><br><span class="line"></span><br><span class="line">DataModel model = new FileDataModel(new File(file));</span><br><span class="line"></span><br><span class="line">// Build the same recommender for testing that we did last time:</span><br><span class="line">RecommenderBuilder recommenderBuilder = new RecommenderBuilder() &#123;</span><br><span class="line">    @Override</span><br><span class="line">    public Recommender buildRecommender(DataModel model) throws TasteException &#123;</span><br><span class="line">        ItemSimilarity similarity = new PearsonCorrelationSimilarity(model);</span><br><span class="line">        return new GenericItemBasedRecommender(model, similarity);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">//获取推荐结果</span><br><span class="line">List&lt;RecommendedItem&gt; recommendations = recommenderBuilder.buildRecommender(model).recommend(1, 4);</span><br><span class="line"></span><br><span class="line">for (RecommendedItem recommendation : recommendations) &#123;</span><br><span class="line">    System.out.println(recommendation);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">//计算评分</span><br><span class="line">RecommenderEvaluator evaluator =</span><br><span class="line">        new AverageAbsoluteDifferenceRecommenderEvaluator();</span><br><span class="line">// Use 70% of the data to train; test using the other 30%.</span><br><span class="line">double score = evaluator.evaluate(recommenderBuilder, null, model, 0.7, 1.0);</span><br><span class="line">System.out.println(score);</span><br><span class="line"></span><br><span class="line">//计算查全率和查准率</span><br><span class="line">RecommenderIRStatsEvaluator statsEvaluator = new GenericRecommenderIRStatsEvaluator();</span><br><span class="line"></span><br><span class="line">// Evaluate precision and recall &quot;at 2&quot;:</span><br><span class="line">IRStatistics stats = statsEvaluator.evaluate(recommenderBuilder,</span><br><span class="line">        null, model, null, 4,</span><br><span class="line">        GenericRecommenderIRStatsEvaluator.CHOOSE_THRESHOLD,</span><br><span class="line">        1.0);</span><br><span class="line">System.out.println(stats.getPrecision());</span><br><span class="line">System.out.println(stats.getRecall());</span><br></pre></td></tr></table></figure></p>
<h2 id="在Spark中运行"><a href="#在Spark中运行" class="headerlink" title="在Spark中运行"></a>在Spark中运行</h2><p>在Spark中运行，需要将Mahout相关的jar添加到Spark的classpath中，修改/etc/spark/conf/spark-env.sh，添加下面两行代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">SPARK_DIST_CLASSPATH=&quot;$SPARK_DIST_CLASSPATH:/usr/lib/mahout/lib/*&quot;</span><br><span class="line">SPARK_DIST_CLASSPATH=&quot;$SPARK_DIST_CLASSPATH:/usr/lib/mahout/*&quot;</span><br></pre></td></tr></table></figure>
<p>然后，以本地模式在spark-shell中运行下面代码交互测试：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">//注意：这里是本地目录</span><br><span class="line">val model = new FileDataModel(new File(&quot;intro.csv&quot;))</span><br><span class="line"></span><br><span class="line">val evaluator = new RMSRecommenderEvaluator()</span><br><span class="line">val recommenderBuilder = new RecommenderBuilder &#123;</span><br><span class="line">  override def buildRecommender(dataModel: DataModel): Recommender = &#123;</span><br><span class="line">    val similarity = new LogLikelihoodSimilarity(dataModel)</span><br><span class="line">    new GenericItemBasedRecommender(dataModel, similarity)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">val score = evaluator.evaluate(recommenderBuilder, null, model, 0.95, 0.05)</span><br><span class="line">println(s&quot;Score=$score&quot;)</span><br><span class="line"></span><br><span class="line">val recommender=recommenderBuilder.buildRecommender(model)</span><br><span class="line">val users=trainingRatings.map(_.user).distinct().take(20)</span><br><span class="line"></span><br><span class="line">import scala.collection.JavaConversions._</span><br><span class="line"></span><br><span class="line">val result=users.par.map&#123;user=&gt;</span><br><span class="line">  user+&quot;,&quot;+recommender.recommend(user,40).map(_.getItemID).mkString(&quot;,&quot;)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p><a href="https://github.com/sujitpal/mia-scala-examples上面有一个评估基于物品或是用户的各种相似度下的评分的类，叫做" target="_blank" rel="external">https://github.com/sujitpal/mia-scala-examples上面有一个评估基于物品或是用户的各种相似度下的评分的类，叫做</a> RecommenderEvaluator，供大家学习参考。</p>
<h2 id="分布式运行"><a href="#分布式运行" class="headerlink" title="分布式运行"></a>分布式运行</h2><p>Mahout提供了<code>org.apache.mahout.cf.taste.hadoop.item.RecommenderJob</code>类以MapReduce的方式来实现基于物品的协同过滤，查看该类的使用说明：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">ubuntu@Master:~/data$ mahout org.apache.mahout.cf.taste.hadoop.item.RecommenderJob</span><br><span class="line">MAHOUT_LOCAL is not set; adding HADOOP_CONF_DIR to classpath.</span><br><span class="line">Running on hadoop, using /home/ubuntu/hadoop/bin/hadoop and HADOOP_CONF_DIR=/home/ubuntu/hadoop/etc/hadoop</span><br><span class="line">MAHOUT-JOB: /home/ubuntu/apache-mahout-distribution-0.10.1/mahout-examples-0.10.1-job.jar</span><br><span class="line">16/07/25 07:41:32 WARN driver.MahoutDriver: No org.apache.mahout.cf.taste.hadoop.item.RecommenderJob.props found on classpath, will use command-line arguments only</span><br><span class="line">16/07/25 07:41:33 ERROR common.AbstractJob: Missing required option --similarityClassname</span><br><span class="line">Missing required option --similarityClassname                                   </span><br><span class="line">Usage:                                                                          </span><br><span class="line"> [--input &lt;input&gt; --output &lt;output&gt; --numRecommendations &lt;numRecommendations&gt;   </span><br><span class="line">--usersFile &lt;usersFile&gt; --itemsFile &lt;itemsFile&gt; --filterFile &lt;filterFile&gt;       </span><br><span class="line">--userItemFile &lt;userItemFile&gt; --booleanData &lt;booleanData&gt; --maxPrefsPerUser     </span><br><span class="line">&lt;maxPrefsPerUser&gt; --minPrefsPerUser &lt;minPrefsPerUser&gt; --maxSimilaritiesPerItem  </span><br><span class="line">&lt;maxSimilaritiesPerItem&gt; --maxPrefsInItemSimilarity &lt;maxPrefsInItemSimilarity&gt;  </span><br><span class="line">--similarityClassname &lt;similarityClassname&gt; --threshold &lt;threshold&gt;             </span><br><span class="line">--outputPathForSimilarityMatrix &lt;outputPathForSimilarityMatrix&gt; --randomSeed    </span><br><span class="line">&lt;randomSeed&gt; --sequencefileOutput --help --tempDir &lt;tempDir&gt; --startPhase       </span><br><span class="line">&lt;startPhase&gt; --endPhase &lt;endPhase&gt;]                                             </span><br><span class="line">--similarityClassname (-s) similarityClassname    Name of distributed           </span><br><span class="line">                                                  similarity measures class to  </span><br><span class="line">                                                  instantiate, alternatively    </span><br><span class="line">                                                  use one of the predefined     </span><br><span class="line">                                                  similarities                  </span><br><span class="line">                                                  ([SIMILARITY_COOCCURRENCE,    </span><br><span class="line">                                                  SIMILARITY_LOGLIKELIHOOD,     </span><br><span class="line">                                                  SIMILARITY_TANIMOTO_COEFFICIEN</span><br><span class="line">                                                  T, SIMILARITY_CITY_BLOCK,     </span><br><span class="line">                                                  SIMILARITY_COSINE,            </span><br><span class="line">                                                  SIMILARITY_PEARSON_CORRELATION</span><br><span class="line">                                                  ,                             </span><br><span class="line">                                                  SIMILARITY_EUCLIDEAN_DISTANCE]</span><br><span class="line">                                                  )</span><br></pre></td></tr></table></figure>
<p>也可输入<code>mahout org.apache.mahout.cf.taste.hadoop.item.RecommenderJob  --help</code>查看详细说明</p>
<p>可见，该类可以接收的命令行参数如下：</p>
<p><code>--input(path)(-i)</code>: 存储用户偏好数据的目录，该目录下可以包含一个或多个存储用户偏好数据的文本文件；<br><code>--output(path)(-o)</code>: 结算结果的输出目录<br><code>--numRecommendations (integer)</code>: 为每个用户推荐的item数量，默认为10<br><code>--usersFile (path)</code>: 指定一个包含了一个或多个存储userID的文件路径，仅为该路径下所有文件包含的userID做推荐计算 (该选项可选)<br><code>--itemsFile (path)</code>: 指定一个包含了一个或多个存储itemID的文件路径，仅为该路径下所有文件包含的itemID做推荐计算 (该选项可选)<br><code>--filterFile (path)</code>: 指定一个路径，该路径下的文件包含了[userID,itemID]值对，userID和itemID用逗号分隔。计算结果将不会为user推荐[userID,itemID]值对中包含的item (该选项可选)<br><code>--booleanData (boolean)</code>: 如果输入数据不包含偏好数值，则将该参数设置为true，默认为false<br><code>--maxPrefsPerUser (integer)</code>: 在最后计算推荐结果的阶段，针对每一个user使用的偏好数据的最大数量，默认为10<br><code>--minPrefsPerUser (integer)</code>: 在相似度计算中，忽略所有偏好数据量少于该值的用户，默认为1<br><code>--maxSimilaritiesPerItem (integer)</code>: 针对每个item的相似度最大值，默认为100<br><code>--maxPrefsPerUserInItemSimilarity (integer)</code>: 在item相似度计算阶段，针对每个用户考虑的偏好数据最大数量，默认为1000<br><code>--similarityClassname (classname)</code>: 向量相似度计算类<br><code>outputPathForSimilarityMatrix</code>：SimilarityMatrix输出目录<br><code>--randomSeed</code>：随机种子 –sequencefileOutput：序列文件输出路径<br><code>--tempDir (path)</code>: 存储临时文件的目录，默认为当前用户的home目录下的temp目录<br><code>--startPhase</code><br><code>--endPhase</code><br><code>--threshold (double)</code>: 忽略相似度低于该阀值的item对</p>
<p>一个例子如下，使用<code>SIMILARITY_LOGLIKELIHOOD</code>相似度推荐物品：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop jar /usr/lib/mahout/mahout-examples-0.9-cdh5.4.0-job.jar org.apache.mahout.cf.taste.hadoop.item.RecommenderJob --input /tmp/mahout/part-00000 --output /tmp/mahout-out  -s SIMILARITY_LOGLIKELIHOOD</span><br></pre></td></tr></table></figure></p>
<p>自己运行的例子如下：</p>
<p>部分实验数据：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">1	25	0.0136316222</span><br><span class="line">1	116	0.0090877481</span><br><span class="line">1	5	0.0045438741</span><br><span class="line">1	23	0.1862988368</span><br><span class="line">1	17	0.0272632444</span><br><span class="line">1	3	1.4122360602</span><br><span class="line">1	11	0.0363509925</span><br><span class="line">1	12	0.4543874068</span><br><span class="line">1	120	0.0027263244</span><br><span class="line">1	93	0.0136316222</span><br><span class="line">1	21	0.0036350993</span><br><span class="line">1	6	0.7688234922</span><br><span class="line">1	47	0.0018175496</span><br><span class="line">1	66	0.0454387407</span><br><span class="line">1	27	0.0254456948</span><br><span class="line">1	44	0.0245369200</span><br><span class="line">1	315	0.0045438741</span><br><span class="line">1	28	0.0545264888</span><br><span class="line">1	138	0.0636142369</span><br><span class="line">1	108	0.0045438741</span><br><span class="line">1	1	7.2695320732</span><br><span class="line">1	85	12.5188577359</span><br><span class="line">1	168	0.0545264888</span><br><span class="line">10	4	0.6772009029</span><br><span class="line">10	6	0.6772009029</span><br><span class="line">10	217	0.0112866817</span><br><span class="line">10	2	1.7607223476</span><br><span class="line">10	1	1.8735891648</span><br><span class="line">100	25	0.4788867023</span><br><span class="line">100	5	0.0047793084</span><br><span class="line">100	17	0.2915378128</span><br><span class="line">100	26	0.0047793084</span><br><span class="line">100	3	0.1194827101</span><br><span class="line">100	11	0.6987348890</span><br><span class="line">100	4	0.6652797301</span><br><span class="line">100	12	0.0047793084</span><br><span class="line">100	30	0.0736013495</span><br><span class="line">100	32	0.5257239247</span><br><span class="line">100	31	0.0076468934</span><br><span class="line">100	37	0.0430137757</span><br><span class="line">100	29	0.0592634242</span><br><span class="line">100	44	0.0009558617</span><br><span class="line">100	13	4.4313747540</span><br><span class="line">100	1	9.5461906101</span><br><span class="line">100	10	0.0439696373</span><br><span class="line">1000	8	0.2902055623</span><br><span class="line">1000	14	0.0483675937</span><br><span class="line">1000	5	0.0725513906</span><br><span class="line">1000	9	0.0725513906</span><br><span class="line">1000	26	0.3869407497</span><br><span class="line">1000	3	0.1451027811</span><br><span class="line">1000	436	0.0120918984</span><br><span class="line">1000	2	3.9177750907</span><br><span class="line">1000	15	2.4304715840</span><br></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ubuntu@Master:~/data$ mahout org.apache.mahout.cf.taste.hadoop.item.RecommenderJob -i /test/item/ckm_pre_result1000000.txt -o /test/item/outputPersonCorr --similarityClassname org.apache.mahout.math.hadoop.similarity.cooccurrence.measures.PearsonCorrelationSimilarity</span><br></pre></td></tr></table></figure>
<p>部分结果：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1	[196:10.482155,14:9.271373,145:8.875873,177:7.779633,360:7.537198,114:7.1917353,635:7.085346,75:6.8879533,235:6.796164,210:6.586777]</span><br><span class="line">2	[386:4.339762,631:4.2806735,194:4.274664,153:4.1018524,362:3.7975848,934:3.422003,195:3.0110214,188:2.7676048,30:2.6990044,746:2.693153]</span><br><span class="line">3	[45:4.2422075,212:4.1731844,270:3.9618893,309:3.960001,204:3.933118,275:3.6498196,321:3.6286862,179:3.487534,240:3.3450491,170:3.28568]</span><br><span class="line">4	[293:4.3900704,746:3.9469879,51:3.3795352,52:3.3444872,312:2.818981,24:2.719058,649:2.2690945,28:2.1947412,196:2.170363,145:2.008545]</span><br><span class="line">5	[590:1.1531421,332:1.1508745,336:1.134177,852:1.1335075,561:1.121143,36:1.1099223,535:1.0878772,129:1.0850264,236:1.0413511,83:1.0349866]</span><br></pre></td></tr></table></figure></p>
<p>默认情况下，mahout使用的reduce数目为1，这样造成大数据处理时效率较低，可以通过参数mahout执行脚本中的<code>MAHOUT_OPTS</code>中的<code>-Dmapred.reduce.tasks</code>参数指定reduce数目。</p>
<p>上面命令运行完成之后，会在当前用户的hdfs主目录生成temp目录，该目录可由–tempDir (path)参数设置：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop fs -ls temp</span><br><span class="line">Found 10 items</span><br><span class="line">-rw-r--r--   3 root hadoop          7 2015-06-10 14:42 temp/maxValues.bin</span><br><span class="line">-rw-r--r--   3 root hadoop    5522717 2015-06-10 14:42 temp/norms.bin</span><br><span class="line">drwxr-xr-x   - root hadoop          0 2015-06-10 14:41 temp/notUsed</span><br><span class="line">-rw-r--r--   3 root hadoop          7 2015-06-10 14:42 temp/numNonZeroEntries.bin</span><br><span class="line">-rw-r--r--   3 root hadoop    3452222 2015-06-10 14:41 temp/observationsPerColumn.bin</span><br><span class="line">drwxr-xr-x   - root hadoop          0 2015-06-10 14:47 temp/pairwiseSimilarity</span><br><span class="line">drwxr-xr-x   - root hadoop          0 2015-06-10 14:52 temp/partialMultiply</span><br><span class="line">drwxr-xr-x   - root hadoop          0 2015-06-10 14:39 temp/preparePreferenceMatrix</span><br><span class="line">drwxr-xr-x   - root hadoop          0 2015-06-10 14:50 temp/similarityMatrix</span><br><span class="line">drwxr-xr-x   - root hadoop          0 2015-06-10 14:42 temp/weights</span><br></pre></td></tr></table></figure>
<p>观察yarn的管理界面，该命令会生成9个任务，任务名称依次是：</p>
<ul>
<li>PreparePreferenceMatrixJob-ItemIDIndexMapper-Reducer</li>
<li>PreparePreferenceMatrixJob-ToItemPrefsMapper-Reducer</li>
<li>PreparePreferenceMatrixJob-ToItemVectorsMapper-Reducer</li>
<li>RowSimilarityJob-CountObservationsMapper-Reducer</li>
<li>RowSimilarityJob-VectorNormMapper-Reducer</li>
<li>RowSimilarityJob-CooccurrencesMapper-Reducer</li>
<li>RowSimilarityJob-UnsymmetrifyMapper-Reducer</li>
<li>partialMultiply</li>
<li>RecommenderJob-PartialMultiplyMapper-Reducer</li>
</ul>
<p>从任务名称，大概可以知道每个任务在做什么，如果你的输入参数不一样，生成的任务数可能不一样，这个需要测试一下才能确认。</p>
<p>在hdfs上查看输出的结果，用户和推荐结果用\t分隔，推荐结果中物品之间用逗号分隔，物品后面通过冒号连接评分：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">843 [10709679:4.8334665,8389878:4.833426,9133835:4.7503786,10366169:4.7503185,9007487:4.750272,8149253:4.7501993,10366165:4.750115,9780049:4.750108,8581254:4.750071,10456307:4.7500467]</span><br><span class="line">6253    [10117445:3.0375953,10340299:3.0340924,8321090:3.0340924,10086615:3.032164,10436801:3.0187714,9668385:3.0141575,8502110:3.013954,10476325:3.0074399,10318667:3.0004222,8320987:3.0003839]</span><br></pre></td></tr></table></figure></p>
<p>使用Java API方式执行，请参考<a href="http://blog.fens.me/hadoop-mahout-mapreduce-itemcf/" target="_blank" rel="external">Mahout分步式程序开发 基于物品的协同过滤ItemCF</a>。</p>
<p>在Scala或者Spark中，可以以Java API或者命令方式运行，最后还可以通过Spark来处理推荐的结果，例如：过滤、去重、补足数据，这部分内容不做介绍。</p>
<p>本文基本转载自：转载自<a href="http://blog.javachen.com/" target="_blank" rel="external">JavaChen Blog</a>，作者：JavaChen，文章地址<a href="http://blog.javachen.com/2015/06/10/collaborative-filtering-using-mahout.html" target="_blank" rel="external">http://blog.javachen.com/2015/06/10/collaborative-filtering-using-mahout.html</a></p>
<p>其他参考资料:</p>
<ul>
<li><a href="http://blog.fens.me/hadoop-mapreduce-recommend/" target="_blank" rel="external">用Hadoop构建电影推荐系统</a></li>
<li><a href="http://blog.fens.me/hadoop-mahout-mapreduce-itemcf/" target="_blank" rel="external">Mahout分步式程序开发 基于物品的协同过滤ItemCF</a></li>
<li><a href="http://www.chinahadoop.cn/group/5/thread/499" target="_blank" rel="external">mahout分布式：Item-based推荐</a></li>
<li><a href="https://mahout.apache.org/users/recommender/intro-itembased-hadoop.html" target="_blank" rel="external">Introduction to Item-Based Recommendations with Hadoop</a></li>
<li><a href="http://my.oschina.net/Cfreedom/blog/201828" target="_blank" rel="external">使用Mahout搭建推荐系统之入门篇4-Mahout实战</a></li>
<li><a href="http://zengzhaozheng.blog.51cto.com/8219051/1557054" target="_blank" rel="external">基于MapReduce的ItemBase推荐算法的共现矩阵实现</a></li>
</ul>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/07/19/VMWare虚拟机：“锁定文件失败”的解决方法/" itemprop="url">
                  VMWare虚拟机：“锁定文件失败”的解决方法
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-07-19T14:38:06+08:00" content="2016-07-19">
              2016-07-19
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/计算机技巧/" itemprop="url" rel="index">
                    <span itemprop="name">计算机技巧</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/07/19/VMWare虚拟机：“锁定文件失败”的解决方法/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2016/07/19/VMWare虚拟机：“锁定文件失败”的解决方法/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h3><p>如果使用VMWare虚拟机的时候突然系统崩溃蓝屏或者强行关机，有一定几率会导致无法启动，会提示：锁定文件失败，打不开磁盘或快照所依赖的磁盘。</p>
<blockquote>
<p>虚拟磁盘(.vmdk)本身有一个磁盘保护机制，为了防止多台虚拟机同时访问同一个虚拟磁盘(.vmdk)带来的数据丢失和性能削减方面的隐患，每次启动虚拟机的时候虚拟机会使用扩展名为.lck（磁盘锁）文件对虚拟磁盘(.vmdk)进行锁定保护。当虚拟机关闭时.lck（磁盘锁）文件自动删除。但是可能由于您非正常关闭虚拟机，这时虚拟机还没来得及删除您系统上的.lck（磁盘锁）文件，所以当下次您启动虚拟机的时候出现了上述错误。</p>
</blockquote>
<h3 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h3><p>打开你存放虚拟机系统文件的文件夹，注意，是系统文件，不是虚拟机的安装目录。搜索关键字<code>*.lck</code>，删除搜索到的文件即可。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/07/19/亚马逊EC2搭建Shadowsocks/" itemprop="url">
                  亚马逊EC2搭建Shadowsocks
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-07-19T11:26:26+08:00" content="2016-07-19">
              2016-07-19
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/计算机技巧/" itemprop="url" rel="index">
                    <span itemprop="name">计算机技巧</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/07/19/亚马逊EC2搭建Shadowsocks/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2016/07/19/亚马逊EC2搭建Shadowsocks/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>在网上寻找优秀的科学上网方式中发现，自己搭建Shadowscoks是非常不错的方式，并且AWS有一年的免费使用期限，到期后再买别的VPS就行了。<br><a href="https://segmentfault.com/a/1190000003101075" target="_blank" rel="external">https://segmentfault.com/a/1190000003101075</a><br>上面连接的教程从注册申请AWS到搭建服务已经非常详细，自己再做一些补充。</p>
<h2 id="服务器端配置"><a href="#服务器端配置" class="headerlink" title="服务器端配置"></a>服务器端配置</h2><p>在EC2上创建好Linux服务器后（本人为Ubuntu）需要对其安装环境<br>参考官方文档<a href="http://docs.aws.amazon.com/zh_cn/AWSEC2/latest/UserGuide/putty.html" target="_blank" rel="external">使用 PuTTY 从 Windows 连接到 Linux 实例</a>连接服务器</p>
<h3 id="安装shadowsocks依赖"><a href="#安装shadowsocks依赖" class="headerlink" title="安装shadowsocks依赖"></a>安装shadowsocks依赖</h3><ol>
<li><code>sudo -s // 获取超级管理员权限</code></li>
<li><code>apt-get update // 更新apt-get</code></li>
<li><code>apt-get install python-pip // 安装python包管理工具pip</code></li>
<li><code>pip install shadowsocks // 安装shadowsocks</code></li>
</ol>
<h3 id="配置shadowsocks"><a href="#配置shadowsocks" class="headerlink" title="配置shadowsocks"></a>配置shadowsocks</h3><p><code>vim /etc/shadowsocks.json</code></p>
<p>单一端口配置<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">&quot;server&quot;:&quot;server_ip&quot;, #EC2实例的IP，注意这里我们不能填写公有IP，需要填写私有IP或者0.0.0.0  填0.0.0.0即可</span><br><span class="line">&quot;server_port&quot;:8388, #server端监听的端口，需要在EC2实例中开放此端口</span><br><span class="line">&quot;local_address&quot;: &quot;127.0.0.1&quot;,</span><br><span class="line">&quot;local_port&quot;:1080,</span><br><span class="line">&quot;password&quot;:&quot;password&quot;, #密码</span><br><span class="line">&quot;timeout&quot;:300,</span><br><span class="line">&quot;method&quot;:&quot;aes-256-cfb&quot;, #加密方式</span><br><span class="line">&quot;fast_open&quot;: false #是否开启fast open</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>如果想要把VPN分享给其它人而不泄露自己的密码，也可以在配置文件中设置多端口+多密码的模式，如：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">&quot;server&quot;:&quot;server_ip&quot;, #EC2实例的IP，注意这里我们不能填写公有IP，需要填写私有IP或者0.0.0.0</span><br><span class="line">&quot;local_address&quot;: &quot;127.0.0.1&quot;,</span><br><span class="line">&quot;local_port&quot;:1080,</span><br><span class="line">&quot;port_password&quot;:</span><br><span class="line">&#123;</span><br><span class="line">&quot;8088”: “password8088”,</span><br><span class="line">&quot;8089”: &quot;password8089”</span><br><span class="line">&#125;</span><br><span class="line">&quot;timeout&quot;:300,</span><br><span class="line">&quot;method&quot;:&quot;aes-256-cfb&quot;, #加密方式</span><br><span class="line">&quot;fast_open&quot;: false #是否开启fast open</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="配置完成后启动Shawdowsocks"><a href="#配置完成后启动Shawdowsocks" class="headerlink" title="配置完成后启动Shawdowsocks"></a>配置完成后启动Shawdowsocks</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">启动：ssserver -c /etc/shadowsocks.json -d start</span><br><span class="line">停止：ssserver -c /etc/shadowsocks.json -d stop</span><br><span class="line">重启：ssserver -c /etc/shadowsocks.json -d restart</span><br><span class="line">查看状态：ssserver -c /etc/shadowsocks.json -d status</span><br></pre></td></tr></table></figure>
<h3 id="关闭服务器防火墙"><a href="#关闭服务器防火墙" class="headerlink" title="关闭服务器防火墙"></a>关闭服务器防火墙</h3><p><code>sudo ufw disable</code></p>
<h3 id="开启AWS入站端口"><a href="#开启AWS入站端口" class="headerlink" title="开启AWS入站端口"></a>开启AWS入站端口</h3><p>配置好shaodowsocks后，还需要将配置中的端口打开,这样客户端的服务才能链接得上EC2中的shadowsocks服务。<br>在EC2网页中编辑入站规则将配置文件中的端口号（如8388）加入入站规则。<br>服务器端配置完毕。</p>
<h2 id="客户端下载"><a href="#客户端下载" class="headerlink" title="客户端下载"></a>客户端下载</h2><p><a href="https://www.gaotizi.com/knowledgebase/2/shadowsocks.html" target="_blank" rel="external">shadowsocks下载地址1</a><br><a href="https://shadowsocks.com/client.html" target="_blank" rel="external">shadowsocks下载地址2</a><br>windows10尽量使用2.3版本，否则可能出现500或者502错误<br><a href="http://pan.baidu.com/s/1hqIk4mS" target="_blank" rel="external">http://pan.baidu.com/s/1hqIk4mS</a></p>
<h3 id="500或者502错误"><a href="#500或者502错误" class="headerlink" title="500或者502错误"></a>500或者502错误</h3><ul>
<li>使用2.3版本</li>
<li>或者尝试以下命令<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">netsh interface ipv4  reset</span><br><span class="line">netsh interface ipv6  reset</span><br><span class="line">netsh winsock reset</span><br></pre></td></tr></table></figure></li>
</ul>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/07/13/window下搭建eclipse运行MapReduce环境/" itemprop="url">
                  window下搭建eclipse运行MapReduce环境
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-07-13T09:47:21+08:00" content="2016-07-13">
              2016-07-13
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/大数据/" itemprop="url" rel="index">
                    <span itemprop="name">大数据</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/07/13/window下搭建eclipse运行MapReduce环境/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2016/07/13/window下搭建eclipse运行MapReduce环境/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="系统环境及所需文件"><a href="#系统环境及所需文件" class="headerlink" title="系统环境及所需文件"></a>系统环境及所需文件</h2><ul>
<li>eclipse-jee-mars-2</li>
<li>hadoop2.7.2</li>
<li><a href="https://github.com/winghc/hadoop2x-eclipse-plugin" target="_blank" rel="external">hadoop-eclipse-plugin</a></li>
<li><a href="http://download.csdn.net/download/chenxf10/9564165" target="_blank" rel="external">hadoop.dll &amp; winutils.exe</a></li>
</ul>
<h2 id="修改Master节点的hdfs-site-xml"><a href="#修改Master节点的hdfs-site-xml" class="headerlink" title="修改Master节点的hdfs-site.xml"></a>修改Master节点的hdfs-site.xml</h2><pre><code>&lt;property&gt;      
    &lt;name&gt;dfs.permissions&lt;/name&gt;      
    &lt;value&gt;false&lt;/value&gt;  
&lt;/property&gt; 
</code></pre><p>旨在取消权限检查</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt; </span><br><span class="line">	&lt;name&gt;dfs.web.ugi&lt;/name&gt; </span><br><span class="line">	&lt;value&gt;Skye,supergroup&lt;/value&gt; </span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>
<h2 id="配置Hadoop插件"><a href="#配置Hadoop插件" class="headerlink" title="配置Hadoop插件"></a>配置Hadoop插件</h2><ol>
<li>windows下载hadoop-2.7.2解压到某目录下，如：E:\hadoop\hadoop-2.7.2</li>
<li>下载hadoop-eclipse-plugin插件hadoop-eclipse-plugin，将release目录下的hadoop-eclipse-plugin-2.6.0.jar拷贝到eclipse/plugins，重启eclipse。</li>
<li>插件配置windows-&gt;show view-&gt;other 显示mapreduce视图</li>
<li>window-&gt;preferences-&gt;hadoop map/reduce 指定windows上的hadoop根目录（即：E:\hadoop\hadoop-2.7.2）</li>
<li>在Map/Reduce Locations 面板中，点击小象图标定义hadoop<br><img src="http://i.imgur.com/61yzGm9.png" alt=""><br>解释：<br>MapReduce Master<br>Host：虚拟机hadoop master对应ip<br>Port：hdfs-site.xml中dfs.datanode.ipc.address指定的的端口号。此处填9001<br>DFS Master中Port：core-site.xml中fs.defaultFS指定的端口。应填9000<br>User name：linux中运行hadoop的用户。 </li>
</ol>
<h2 id="配置完毕查看结果"><a href="#配置完毕查看结果" class="headerlink" title="配置完毕查看结果"></a>配置完毕查看结果</h2><p><img src="http://i.imgur.com/zVOXsbw.png" alt=""></p>
<h2 id="windows下运行环境配置"><a href="#windows下运行环境配置" class="headerlink" title="windows下运行环境配置"></a>windows下运行环境配置</h2><ol>
<li>在系统环境变量中增加HADOOP_HOME，并在Path中加入%HADOOP_HOME%\bin</li>
<li>将下载下来的hadoop.dll,winutils.exe拷贝到HADOOP_HOME/bin目录下</li>
</ol>
<h2 id="创建-MapReduce工程并运行"><a href="#创建-MapReduce工程并运行" class="headerlink" title="创建 MapReduce工程并运行"></a>创建 MapReduce工程并运行</h2><p>需要拷贝 服务器hadoop中的log4j.properties文件到工程的src目录</p>
<p><code>run on hadoop</code></p>
<p>运行时报如下错误，弄了好长一段时间，发现原因是服务器通过内网ip访问，外网无法解析。用虚拟机连接成功.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line">16/07/13 10:42:38 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.</span><br><span class="line">16/07/13 10:42:39 INFO mapreduce.Job: Job job_local510776960_0001 running in uber mode : false</span><br><span class="line">16/07/13 10:42:39 INFO mapreduce.Job:  map 0% reduce 0%</span><br><span class="line">16/07/13 10:42:39 INFO mapred.Task:  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@3bfe5dd7</span><br><span class="line">16/07/13 10:42:39 INFO mapred.MapTask: Processing split: hdfs://Master:9000/test/test3.txt:0+259</span><br><span class="line">16/07/13 10:42:39 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)</span><br><span class="line">16/07/13 10:42:39 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100</span><br><span class="line">16/07/13 10:42:39 INFO mapred.MapTask: soft limit at 83886080</span><br><span class="line">16/07/13 10:42:39 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600</span><br><span class="line">16/07/13 10:42:39 INFO mapred.MapTask: kvstart = 26214396; length = 6553600</span><br><span class="line">16/07/13 10:42:39 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer</span><br><span class="line">16/07/13 10:43:00 WARN hdfs.BlockReaderFactory: I/O error constructing remote block reader.</span><br><span class="line">java.net.ConnectException: Connection timed out: no further information</span><br><span class="line">	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)</span><br><span class="line">	at sun.nio.ch.SocketChannelImpl.finishConnect(Unknown Source)</span><br><span class="line">	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)</span><br><span class="line">	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)</span><br><span class="line">	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)</span><br><span class="line">	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)</span><br><span class="line">	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)</span><br><span class="line">	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)</span><br><span class="line">	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:656)</span><br><span class="line">	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:882)</span><br><span class="line">	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)</span><br><span class="line">	at java.io.DataInputStream.read(Unknown Source)</span><br><span class="line">	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:59)</span><br><span class="line">	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)</span><br><span class="line">	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)</span><br><span class="line">	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:91)</span><br><span class="line">	at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:144)</span><br><span class="line">	at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:184)</span><br><span class="line">	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:556)</span><br><span class="line">	at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)</span><br><span class="line">	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)</span><br><span class="line">	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145)</span><br><span class="line">	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)</span><br><span class="line">	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)</span><br><span class="line">	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)</span><br><span class="line">	at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)</span><br><span class="line">	at java.util.concurrent.FutureTask.run(Unknown Source)</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)</span><br><span class="line">	at java.lang.Thread.run(Unknown Source)</span><br><span class="line">16/07/13 10:43:00 WARN hdfs.DFSClient: Failed to connect to /10.0.0.14:50010 for block, add to deadNodes and continue. java.net.ConnectException: Connection timed out: no further information</span><br><span class="line">java.net.ConnectException: Connection timed out: no further information</span><br><span class="line">	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)</span><br><span class="line">	at sun.nio.ch.SocketChannelImpl.finishConnect(Unknown Source)</span><br><span class="line">	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)</span><br><span class="line">	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)</span><br><span class="line">	at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436)</span><br><span class="line">	at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)</span><br><span class="line">	at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)</span><br><span class="line">	at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)</span><br><span class="line">	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:656)</span><br><span class="line">	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:882)</span><br><span class="line">	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)</span><br><span class="line">	at java.io.DataInputStream.read(Unknown Source)</span><br><span class="line">	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:59)</span><br><span class="line">	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)</span><br><span class="line">	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)</span><br><span class="line">	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:91)</span><br><span class="line">	at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:144)</span><br><span class="line">	at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:184)</span><br><span class="line">	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:556)</span><br><span class="line">	at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)</span><br><span class="line">	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)</span><br><span class="line">	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145)</span><br><span class="line">	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)</span><br><span class="line">	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)</span><br><span class="line">	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)</span><br><span class="line">	at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)</span><br><span class="line">	at java.util.concurrent.FutureTask.run(Unknown Source)</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)</span><br><span class="line">	at java.lang.Thread.run(Unknown Source)</span><br></pre></td></tr></table></figure></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/07/11/mapreduce任务运行时shuffle Error/" itemprop="url">
                  mapreduce任务运行时shuffle Error
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-07-11T12:36:36+08:00" content="2016-07-11">
              2016-07-11
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/大数据/" itemprop="url" rel="index">
                    <span itemprop="name">大数据</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/07/11/mapreduce任务运行时shuffle Error/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2016/07/11/mapreduce任务运行时shuffle Error/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><em>本文引用参考</em>：<a href="http://blog.csdn.net/dslztx/article/details/46445725" target="_blank" rel="external">MapReduce任务Shuffle Error错误</a><br><em>相关参考连接</em>：<a href="http://blog.csdn.net/stark_summer/article/details/48494391" target="_blank" rel="external"> yarn &amp; mapreduce 配置参数总结</a></p>
<h2 id="错误描述"><a href="#错误描述" class="headerlink" title="错误描述"></a>错误描述</h2><p>在运行MapReduce任务的时候，出现如下错误：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">Error: org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#1</span><br><span class="line">        at org.apache.hadoop.mapreduce.task.reduce.Shuffle.run(Shuffle.java:134)</span><br><span class="line">        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:376)</span><br><span class="line">        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)</span><br><span class="line">        at java.security.AccessController.doPrivileged(Native Method)</span><br><span class="line">        at javax.security.auth.Subject.doAs(Subject.java:396)</span><br><span class="line">        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)</span><br><span class="line">        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)</span><br><span class="line">Caused by: java.lang.OutOfMemoryError: Java heap space</span><br><span class="line">        at org.apache.hadoop.io.BoundedByteArrayOutputStream.&lt;init&gt;(BoundedByteArrayOutputStream.java:56)</span><br><span class="line">        at org.apache.hadoop.io.BoundedByteArrayOutputStream.&lt;init&gt;(BoundedByteArrayOutputStream.java:46)</span><br><span class="line">        at org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput.&lt;init&gt;(InMemoryMapOutput.java:63)</span><br><span class="line">        at org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl.unconditionalReserve(MergeManagerImpl.java:297)</span><br><span class="line">        at org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl.reserve(MergeManagerImpl.java:287)</span><br><span class="line">        at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyMapOutput(Fetcher.java:411)</span><br><span class="line">        at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyFromHost(Fetcher.java:341)</span><br><span class="line">        at org.apache.hadoop.mapreduce.task.reduce.Fetcher.run(Fetcher.java:165)</span><br></pre></td></tr></table></figure></p>
<h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>  根据《Hadoop:The Definitive Guide 4th　Edition》所述(P203-219)，map任务和reduce任务之间要经过一个shuffle过程，该过程复制map任务的输出作为reduce任务的输入。<br>  具体的来说，shuffle过程的输入是：map任务的输出文件，它的输出接收者是：运行reduce任务的机子上的内存buffer，并且shuffle过程以并行方式运行。<br>  参数mapreduce.reduce.shuffle.input.buffer.percent控制运行reduce任务的机子上多少比例的内存用作上述buffer(默认值为0.70)，参数mapreduce.reduce.shuffle.parallelcopies控制shuffle过程的并行度(默认值为5)。那么”mapreduce.reduce.shuffle.input.buffer.percent” * “mapreduce.reduce.shuffle.parallelcopies” 必须小于等于1，否则就会出现如上错误<br>因此，我将mapreduce.reduce.shuffle.input.buffer.percent设置成值为0.1，就可以正常运行了（设置成0.2，还是会抛同样的错）</p>
<p><code>job.getConfiguration().setStrings(&quot;mapreduce.reduce.shuffle.input.buffer.percent&quot;, &quot;0.1&quot;);</code><br>或者在<code>maperd-site.xml</code>中修改<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">   &lt;name&gt;mapreduce.reduce.input.buffer.percent&lt;/name&gt;</span><br><span class="line">   &lt;value&gt;0.0&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></p>
<p>  另外，可以发现如果使用两个参数的默认值，那么两者乘积为3.5，大大大于1了，为什么没有经常抛出以上的错误呢？<br>1)首先，把默认值设为比较大，主要是基于性能考虑，将它们设为比较大，可以大大加快从map复制数据的速度<br>2)其次，要抛出如上异常，还需满足另外一个条件，就是map任务的数据一下子准备好了等待shuffle去复制，在这种情况下，就会导致shuffle过程的“线程数量”和“内存buffer使用量”都是满负荷的值，自然就造成了内存不足的错误；而如果map任务的数据是断断续续完成的，那么没有一个时刻shuffle过程的“线程数量”和“内存buffer使用量”是满负荷值的，自然也就不会抛出如上错误</p>
<p>另外，如果在设置以上参数后，还是出现错误，那么有可能是运行Reduce任务的进程的内存总量不足，可以通过mapred.child.java.opts参数来调节，比如设置mapred.child.java.opts=-Xmx2024m</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/06/16/MapReduce 多文件输入/" itemprop="url">
                  MapReduce 多文件输入
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-06-16T16:26:16+08:00" content="2016-06-16">
              2016-06-16
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/大数据/" itemprop="url" rel="index">
                    <span itemprop="name">大数据</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/06/16/MapReduce 多文件输入/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2016/06/16/MapReduce 多文件输入/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="多路径输入"><a href="#多路径输入" class="headerlink" title="多路径输入"></a>多路径输入</h2><ol>
<li><p>FileInputFormat.addInputPath 多次调用加载不同路径</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">FileInputFormat.addInputPath(job, new Path(args[0]));</span><br><span class="line">FileInputFormat.addInputPath(job, new Path(args[1]));</span><br></pre></td></tr></table></figure>
</li>
<li><p>FileInputFormat.addInputPaths一次调用加载 多路径字符串用逗号隔开</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">FileInputFormat.addInputPaths(job, &quot;hdfs://master:9000/cs/path1,hdfs://RS5-112:9000/cs/path2&quot;);</span><br></pre></td></tr></table></figure>
</li>
<li><p>多种输入**MultipleInputs可以加载不同路径的输入文件，并且每个路径可用不同的</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">maperMultipleInputs.addInputPath(job, new Path(&quot;hdfs://master:9000/cs/path1&quot;), TextInputFormat.class,MultiTypeFileInput1Mapper.class);</span><br><span class="line">MultipleInputs.addInputPath(job, new Path(&quot;hdfs://master:9000/cs/path3&quot;), TextInputFormat.class,MultiTypeFileInput3Mapper.class);</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h2 id="网上例子："><a href="#网上例子：" class="headerlink" title="网上例子："></a>网上例子：</h2><pre><code>package example;

import java.io.IOException;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.MultipleInputs;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
/**
* 多类型文件输入
* @author lijl
*
*/

public class MultiTypeFileInputMR {
static class MultiTypeFileInput1Mapper extends Mapper&lt;LongWritable, Text, Text, Text&gt;{
public void map(LongWritable key,Text value,Context context){
try {
String[] str = value.toString().split(&quot;\\|&quot;);
context.write(new Text(str[0]), new Text(str[1]));
} catch (IOException e) {
e.printStackTrace();
} catch (InterruptedException e) {
e.printStackTrace();
}
}
}
static class MultiTypeFileInput3Mapper extends Mapper&lt;LongWritable, Text, Text, Text&gt;{
public void map(LongWritable key,Text value,Context context){
try {
String[] str = value.toString().split(&quot;&quot;);
context.write(new Text(str[0]), new Text(str[1]));
} catch (IOException e) {
e.printStackTrace();
} catch (InterruptedException e) {
e.printStackTrace();
}
}
}
static class MultiTypeFileInputReducer extends Reducer&lt;Text, Text, Text, Text&gt;{
public void reduce(Text key,Iterable&lt;Text&gt; values,Context context){
try {
for(Text value:values){
context.write(key,value);
}

} catch (IOException e) {
e.printStackTrace();
} catch (InterruptedException e) {
e.printStackTrace();
}
}
}

public static void main(String[] args) throws IOException, InterruptedException, ClassNotFoundException {
Configuration conf = new Configuration();
conf.set(&quot;mapred.textoutputformat.separator&quot;, &quot;,&quot;);
Job job = new Job(conf,&quot;MultiPathFileInput&quot;);
job.setJarByClass(MultiTypeFileInputMR.class);
FileOutputFormat.setOutputPath(job, new Path(&quot;hdfs://RS5-112:9000/cs/path6&quot;));

job.setMapOutputKeyClass(Text.class);
job.setMapOutputValueClass(Text.class);
job.setOutputKeyClass(Text.class);
job.setOutputValueClass(Text.class);

job.setReducerClass(MultiTypeFileInputReducer.class);
job.setNumReduceTasks(1);
MultipleInputs.addInputPath(job, new Path(&quot;hdfs://RS5-112:9000/cs/path1&quot;), TextInputFormat.class,MultiTypeFileInput1Mapper.class);
MultipleInputs.addInputPath(job, new Path(&quot;hdfs://RS5-112:9000/cs/path3&quot;), TextInputFormat.class,MultiTypeFileInput3Mapper.class);
System.exit(job.waitForCompletion(true)?0:1);
}

}
</code></pre><h2 id="自己例子"><a href="#自己例子" class="headerlink" title="自己例子"></a>自己例子</h2><p>QLMapper.java<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">package com.hdu.mr;</span><br><span class="line"></span><br><span class="line">import java.io.IOException;</span><br><span class="line"></span><br><span class="line">import org.apache.hadoop.io.LongWritable;</span><br><span class="line">import org.apache.hadoop.io.Text;</span><br><span class="line">import org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line">public class QLMapper extends Mapper&lt;LongWritable, Text, Text, LongWritable&gt; &#123;</span><br><span class="line">	String[] mbUnlike = &#123; &quot;盒子&quot;, &quot;助手&quot;, &quot;输入法&quot;, &quot;平台&quot; &#125;;</span><br><span class="line">	String mbdylxLike = &quot;游戏&quot;;</span><br><span class="line">	String mbdylxUnlike = &quot;网页游戏&quot;;</span><br><span class="line">	String delWeb = &quot;访问网站&quot;;</span><br><span class="line">	Text outputValue = new Text();</span><br><span class="line"></span><br><span class="line">	@Override</span><br><span class="line">	protected void map(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, LongWritable&gt;.Context context)</span><br><span class="line">			throws IOException, InterruptedException &#123;</span><br><span class="line">		// 接收数据v1</span><br><span class="line">		String line = value.toString();</span><br><span class="line">		// 切分数据</span><br><span class="line">		String[] words = line.split(&quot;&quot;);</span><br><span class="line">		// String[] words = line.split(&quot;\t&quot;);</span><br><span class="line"></span><br><span class="line">		boolean flag = true;</span><br><span class="line"></span><br><span class="line">		for (int i = 0; i &lt; 4; i++) &#123;</span><br><span class="line">			if (words.length &lt; 5) &#123; // 过滤 长度小于4的信息 即访问网站等</span><br><span class="line">				flag = false;</span><br><span class="line">				break;</span><br><span class="line">			&#125;</span><br><span class="line">			if (words[3].indexOf(mbUnlike[i]) != -1) &#123; // 有其中一个则为false</span><br><span class="line">				flag = false;</span><br><span class="line">				break;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		if (flag == true) &#123;</span><br><span class="line">			if (words[4].indexOf(mbdylxUnlike) != -1) &#123; // 有网页游戏则为false</span><br><span class="line">				flag = false;</span><br><span class="line">			&#125; else if (words[4].indexOf(mbdylxLike) == -1) &#123; // 没有游戏则为false</span><br><span class="line">				flag = false;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">		if (flag == true) &#123;</span><br><span class="line">			outputValue.set(line);</span><br><span class="line">			context.write(outputValue, new LongWritable(1L));</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>QLReducer.java<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">package com.hdu.mr;</span><br><span class="line"></span><br><span class="line">import java.io.IOException;</span><br><span class="line"></span><br><span class="line">import org.apache.hadoop.io.LongWritable;</span><br><span class="line">import org.apache.hadoop.io.NullWritable;</span><br><span class="line">import org.apache.hadoop.io.Text;</span><br><span class="line">import org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line">public class QLReducer extends Reducer&lt;Text, LongWritable, Text, NullWritable&gt; &#123;</span><br><span class="line"></span><br><span class="line">	@Override</span><br><span class="line">	protected void reduce(Text key, Iterable&lt;LongWritable&gt; values,</span><br><span class="line">			Reducer&lt;Text, LongWritable, Text, NullWritable&gt;.Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">		// 接收数据</span><br><span class="line"></span><br><span class="line">		// 输出</span><br><span class="line">		context.write(key, NullWritable.get());</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>DataClean.java<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">package com.hdu.mr;</span><br><span class="line"></span><br><span class="line">import java.io.IOException;</span><br><span class="line"></span><br><span class="line">import org.apache.hadoop.io.LongWritable;</span><br><span class="line">import org.apache.hadoop.io.NullWritable;</span><br><span class="line">import org.apache.hadoop.io.Text;</span><br><span class="line">import org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line">public class QLReducer extends Reducer&lt;Text, LongWritable, Text, NullWritable&gt; &#123;</span><br><span class="line"></span><br><span class="line">	@Override</span><br><span class="line">	protected void reduce(Text key, Iterable&lt;LongWritable&gt; values,</span><br><span class="line">			Reducer&lt;Text, LongWritable, Text, NullWritable&gt;.Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">		// 接收数据</span><br><span class="line"></span><br><span class="line">		// 输出</span><br><span class="line">		context.write(key, NullWritable.get());</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/06/16/HBase集群搭建/" itemprop="url">
                  HBase集群搭建
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-06-16T16:26:16+08:00" content="2016-06-16">
              2016-06-16
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/大数据/" itemprop="url" rel="index">
                    <span itemprop="name">大数据</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/06/16/HBase集群搭建/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2016/06/16/HBase集群搭建/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="1-上传hbase安装包"><a href="#1-上传hbase安装包" class="headerlink" title="1. 上传hbase安装包"></a>1. 上传hbase安装包</h2><h2 id="2-解压"><a href="#2-解压" class="headerlink" title="2. 解压"></a>2. 解压</h2><h2 id="3-配置hbase集群，要修改3个文件（首先zk集群已经安装好了）"><a href="#3-配置hbase集群，要修改3个文件（首先zk集群已经安装好了）" class="headerlink" title="3. 配置hbase集群，要修改3个文件（首先zk集群已经安装好了）"></a>3. 配置hbase集群，要修改3个文件（首先zk集群已经安装好了）</h2><p><em>注意：要把hadoop的hdfs-site.xml和core-site.xml 放到hbase/conf下</em></p>
<h4 id="vim-hbase-env-sh"><a href="#vim-hbase-env-sh" class="headerlink" title="vim hbase-env.sh"></a>vim hbase-env.sh</h4><pre><code>export JAVA_HOME=/usr/java/jdk1.7.0_55
//告诉hbase使用外部的zk 
export HBASE_MANAGES_ZK=false
</code></pre><h4 id="vim-hbase-site-xml"><a href="#vim-hbase-site-xml" class="headerlink" title="vim hbase-site.xml"></a>vim hbase-site.xml</h4><pre><code> &lt;configuration&gt;
    &lt;!-- 指定hbase在HDFS上存储的路径 --&gt;
    &lt;property&gt;
            &lt;name&gt;hbase.rootdir&lt;/name&gt;
            &lt;value&gt;hdfs://ns1/hbase&lt;/value&gt;
    &lt;/property&gt;
    &lt;!-- 指定hbase是分布式的 --&gt;
    &lt;property&gt;
            &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;
            &lt;value&gt;true&lt;/value&gt;
    &lt;/property&gt;
    &lt;!-- 指定zk的地址，多个用“,”分割 --&gt;
    &lt;property&gt;
            &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;
            &lt;value&gt;itcast04:2181,itcast05:2181,itcast06:2181&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;
</code></pre><h4 id="vim-regionservers"><a href="#vim-regionservers" class="headerlink" title="vim regionservers"></a>vim regionservers</h4><pre><code>itcast03
itcast04
itcast05
itcast06
</code></pre><h2 id="4-将配置好的HBase拷贝到每一个节点并同步时间"><a href="#4-将配置好的HBase拷贝到每一个节点并同步时间" class="headerlink" title="4. 将配置好的HBase拷贝到每一个节点并同步时间"></a>4. 将配置好的HBase拷贝到每一个节点并同步时间</h2><pre><code>scp -r /itcast/hbase-0.96.2-hadoop2/ itcast02:/itcast/
scp -r /itcast/hbase-0.96.2-hadoop2/ itcast03:/itcast/
scp -r /itcast/hbase-0.96.2-hadoop2/ itcast04:/itcast/
scp -r /itcast/hbase-0.96.2-hadoop2/ itcast05:/itcast/
scp -r /itcast/hbase-0.96.2-hadoop2/ itcast06:/itcast/
</code></pre><h2 id="5-启动所有的hbase"><a href="#5-启动所有的hbase" class="headerlink" title="5. 启动所有的hbase"></a>5. 启动所有的hbase</h2><pre><code>分别启动zk
    ./zkServer.sh start
启动hbase集群
    start-dfs.sh
启动hbase，在主节点上运行：
    start-hbase.sh
</code></pre><h2 id="6-通过浏览器访问hbase管理页面"><a href="#6-通过浏览器访问hbase管理页面" class="headerlink" title="6.通过浏览器访问hbase管理页面"></a>6.通过浏览器访问hbase管理页面</h2><pre><code>192.168.1.201:60010
</code></pre><h2 id="7-为保证集群的可靠性，要启动多个HMaster-在itcast02上启动"><a href="#7-为保证集群的可靠性，要启动多个HMaster-在itcast02上启动" class="headerlink" title="7.为保证集群的可靠性，要启动多个HMaster,在itcast02上启动"></a>7.为保证集群的可靠性，要启动多个HMaster,在itcast02上启动</h2><pre><code>hbase-daemon.sh start master
</code></pre>
          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  


          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel  sidebar-panel-active ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.png"
               alt="Skye" />
          <p class="site-author-name" itemprop="name">Skye</p>
          <p class="site-description motion-element" itemprop="description">学习总结 思想感悟</p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">7</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          
            <div class="site-state-item site-state-categories">
              <a href="/categories">
                <span class="site-state-item-count">2</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">5</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="http://weibo.com/1811544100" target="_blank" title="Weibo">
                  
                    <i class="fa fa-fw fa-weibo"></i>
                  
                  Weibo
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://github.com/Skyexu" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
          
        </div>

        
        

        
        

      </section>

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2016</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Skye</span>
</div>

<div class="powered-by">
  由 <a class="theme-link" href="http://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Pisces
  </a>
</div>

        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.0.1"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.0.1"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.0.1"></script>



  

  
    
  

  <script type="text/javascript">
    var duoshuoQuery = {short_name:"skyexu"};
    (function() {
      var ds = document.createElement('script');
      ds.type = 'text/javascript';ds.async = true;
      ds.id = 'duoshuo-script';
      ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
      ds.charset = 'UTF-8';
      (document.getElementsByTagName('head')[0]
      || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
  </script>

  
    
  






  
  
  

  

  

</body>
</html>
