---
title: 决策树(Decision Tree) && ID3 Algorithm 
date: 2017/1/13 10:01:02  
tags: Algorithm
categories: Machine learning

---
# 分类与预测
餐饮企业经常会碰到下面的问题：
1. 如何预测未来一段时间内，哪些顾客会流失，哪些顾客最有可能成为VIP客户？
2. 如何预测一种心产品的销售量，以及在哪种类型的客户中会较受欢迎？

除此之外，餐厅经理需要通过数据分析来了解具有某些特征的顾客的消费习惯/这些都是分类与预测的例子。
<!-- more -->
## 常见的分类预测算法

### 贝叶斯
贝叶斯（Bayes）分类算法是一类利用概率统计知识进行分类的算法，如朴素贝叶斯（Naive Bayes）算法。这些算法主要利用Bayes定理来预测一个未知类别的样本属于各个类别的可能性，选择其中可能性最大的一个类别作为该样本的最终类别。

### 决策树
决策树是用于分类和预测的主要技术之一，决策树学习是以实例为基础的归纳学习算法，它着眼于从一组无次序、无规则的实例中推理出以决策树表示的分类规则。

### 人工神经网络
人工神经网络（Artificial Neural Networks，ANN）是一种应用类似于大脑神经突触联接的结构进行信息处理的数学模型。在这种模型中，大量的节点（或称”神经元”，或”单元”）之间相互联接构成网络，即”神经网络”，以达到处理信息的目的。

### 支持向量机
支持向量机（SVM，Support Vector Machine）是Vapnik根据统计学习理论提出的一种新的学习方法，它的最大特点是根据结构风险最小化准则，以最大化分类间隔构造最优分类超平面来提高学习机的泛化能力，较好地解决了非线性、高维数、局部极小点等问题。
# 决策树简介

决策树(Decision Tree)是一个预测模型，他代表的是对象属性与对象值之间的一种映射关系。树中每个节点表示某个对象，而每个分叉路径则代表的某个可能的属性值，而每个叶结点则对应从根节点到该叶节点所经历的路径所表示的对象的值。决策树仅有单一输出，若欲有复数输出，可以建立独立的决策树以处理不同输出。 数据挖掘中决策树是一种经常要用到的技术，可以用于分析数据，同样也可以用来作预测。

## 示例

记录
![](http://ojpgmz933.bkt.clouddn.com/17-1-15/51314537-file_1484471322544_ab94.png)

根据上述数据构造出如下决策树
![](http://ojpgmz933.bkt.clouddn.com/17-1-15/54649081-file_1484471333776_17eac.png)

# 信息熵、条件熵和信息增益

## 信息熵

信息熵也称为香农熵，是随机变量的期望。度量信息的不确定程度。信息的熵越大，信息就越不容易搞清楚。处理信息就是为了把信息搞清楚，就是熵减少的过程。决定信息的不确定性或者说复杂程度主要因素是概率。

 我们要获得随机变量 D 的取值结果至少要进行1次试验，试验次数与随机变量 D 可能的取值数量(2种)的对数函数Log有联系。Log2=1(以2为底)。因此熵的计算公式是：

![](http://images0.cnblogs.com/blog/46139/201507/241728079433989.gif)

## 条件熵

条件熵是通过获得更多的信息来消除一元模型中的不确定性。也就是通过二元或多元模型来降低一元模型的熵。我们知道的信息越多，信息的不确定性越小。例如，只使用一元模型时我们无法根据用户历史数据中的购买频率来判断这个用户本次是否也会购买。因为不确定性太大。在加入了促销活动，商品价格等信息后，在二元模型中我们可以发现用户购买与促销活动，或者商品价格变化之间的联系。并通过购买与促销活动一起出现的概率，和不同促销活动时购买出现的概率来降低不确定性。以下公式为属性A的信息条件熵。

![](http://images0.cnblogs.com/blog/46139/201507/241728092875245.gif)

用属性A出现的概率乘以属性A确定的情况下，相应分类的信息熵。

## 信息增益

信息增益用来衡量信息之间相关性的指标。用于度量属性A降低样本集合X熵的贡献大小。信息增益越大，不确定性越小，越适于对X分类。具体的计算方法就熵与条件熵之间的差。公式如下：

![](http://images0.cnblogs.com/blog/46139/201507/241728131317071.gif)

# ID3 算法原理

> 奥卡姆剃刀（Occam's Razor, Ockham's Razor），又称“奥坎的剃刀”，是由14世纪逻辑学家、圣方济各会修士奥卡姆的威廉（William of Occam，约1285年至1349年）提出，他在《箴言书注》2卷15题说“切勿浪费较多东西，去做‘用较少的东西，同样可以做好的事情’。简单点说，便是：be simple。

ID3算法（Iterative Dichotomiser 3 迭代二叉树3代）是一个由Ross Quinlan发明的用于决策树的算法。这个算法便是建立在上述所介绍的奥卡姆剃刀的基础上：越是小型的决策树越优于大的决策树（be simple 简单理论）。尽管如此，该算法也不是总是生成最小的树形结构，而是一个启发式算法。

OK，从信息论知识中我们知道，期望信息越小，信息增益越大，从而纯度越高。ID3算法的核心思想就是以信息增益度量属性选择，选择分裂后信息增益(很快，由下文你就会知道信息增益又是怎么一回事)最大的属性进行分裂。该算法采用自顶向下的贪婪搜索遍历可能的决策树空间。

算法流程：

1. 对当前样本集合计算所有属性的信息增益；
2. 选择信息增益最大的属性作为测试属性，把测试属性取值相同的样本划为同一个子样本集；
3. 若子样本集的类别属性只含有单个属性，则分支为叶子节点，判断其属性之并标上相应的符号，然后返回调用处；否则对子样本集递归调用本算法。

## 缺点

由于ID3决策树算法采用信息增益作为选择测试属性的标准，会偏向于选择取值较多的，即所谓的高度分支属性，而这类属性并不一定是最优属性。并且其只能处理离散属性，对于连续类型属性需要对其进行离散化。为了解决倾向于选择高度分支属性的问题，采用信息增益率作为选择测试属性的标准，这样便有了C4.5决策树算法。常用的还有 CART,SLIQ,SPRINT,PUBLIC等。

# 决策树实例

这是一家高尔夫球俱乐部的历史数据，里面记录了不同天气状况用户来打高尔夫球的历史记录。我们要做的是通过构建决策树来预测用户是否会来打高尔夫球。这里用户是否来打球是一个一元模型，具有不确定性，熵值很高。我们无法仅通过Yes和No的频率来判断用户明天是否会来。因此，需要借助天气的信息来减少不确定性。下面分别记录到了4种天气情况，我们通过计算条件熵和互信息来开始构建决策树的第一步：构建根决策点。
![](http://ojpgmz933.bkt.clouddn.com/17-1-14/12057450-file_1484368445392_1304.png)

## 构建根决策节点

构建根决策点的方法就是寻找4种天气情况中与打高尔夫球相关性最高的一个。首先我们来看Play Golf这个一元模型的熵，来看看这件事的不确定性有多高.

### 一元模型的熵即信息熵

在一元模型中，仅通过历史数据的概率来看预测Play Golf是一件非常不确定的事情，在14条历史数据中，打球的概率为64%，不打球的概率为36%。熵值达到了0.940。这与之前抛硬币的例子很像。在无法改变历史数据的概率时，我们需要借助更多的信息来降低不确定性。也就是计算条件熵。
![](http://ojpgmz933.bkt.clouddn.com/17-1-14/88660281-file_1484368589678_1418d.png)

### 二元模型条件熵

计算二元模型的条件熵需要知道Play Golf与4种天气情况一起出现的概率，以及在不同天气情况下Play Golf出现的条件概率。下面我们分别来计算这两类概率。

出现概率
![](http://ojpgmz933.bkt.clouddn.com/17-1-14/16481236-file_1484368665443_9e5a.png)

条件概率
![](http://ojpgmz933.bkt.clouddn.com/17-1-14/10038807-file_1484368678860_b99e.png)

条件熵
![](http://ojpgmz933.bkt.clouddn.com/17-1-14/30903876-file_1484368712279_a685.png)

### 信息增益

在已知Play Golf的一元模型熵和不同天气条件下的二元模型熵后。我们就可以通过信息增益来度量哪种天气与Play Golf的相关性最高了。
![](http://ojpgmz933.bkt.clouddn.com/17-1-14/7604661-file_1484368761620_fe7b.png)

### 构建根节点

在整个决策树中，Outlook因为与Play Golf的相关性最高，所以作为决策树的根节点。以Outlook作为根节点后，决策树出现了三个分支，分别是Outlook的三个不同的取值Sunny，Overcast和Rainy。其中Overcast所对应的Play Golf都是Yes，因此这个分支的叶子节点为Yes。
![](http://ojpgmz933.bkt.clouddn.com/17-1-14/32657150-file_1484368828842_d411.png)

## 构建分支节点
另外两个分支我们将使用和前面一样的方法，通过计算熵，条件熵和信息增益来挑选下一个分支的决策节点。
通过将决策树中每个决策点还原为原始数据表可以发现，每一个决策点都对应了一张数据表。从根决策节点开始，我们通过计算熵寻找与Play Golf最相关的天气信息，来建立决策点及分支，并反复迭代这一过程。直到最终构建完整的决策树。
![](http://ojpgmz933.bkt.clouddn.com/17-1-14/20779378-file_1484369378883_2419.png)
![](http://ojpgmz933.bkt.clouddn.com/17-1-14/99229625-file_1484368967065_909c.png)

## 使用决策树进行预测

文章开始的时候我们说过，决策树是用来进行分类和预测的。具体过程如下。当我们构建好决策树后，当有新的信息发送时，我们利用已有的决策树逻辑对新的信息结构进行判断。当信息的内容与决策树一致时，就进入下一分支进行判断，并通过叶子节点获得分类的结果。例如，当新的一天开始时，我们就可以通过4个天气特征来判断用户是否会来打高尔夫球。以下是具体预测流程的示意图，首先寻找新信息中的根决策节点Outlook，根据Outlook的取值进入到Sunny分支，在Sunny分支中继续判断下一决策点Windy的取值，新的信息中Windy的取值为FALSE，根据决策树中的逻辑返回Yes。因此在新信息中通过对天气情况的判断预测用户会来打高尔夫球。
![](http://ojpgmz933.bkt.clouddn.com/17-1-14/56003626-file_1484369065520_9d04.png)

# 随机森林

决策树是建立在已知的历史数据及概率上的，一课决策树的预测可能会不太准确，提高准确率最好的方法是构建随机森林(Random Forest)。所谓随机森林就是通过随机抽样的方式从历史数据表中生成多张抽样的历史表，对每个抽样的历史表生成一棵决策树。由于每次生成抽样表后数据都会放回到总表中，因此每一棵决策树之间都是独立的没有关联。将多颗决策树组成一个随机森林。当有一条新的数据产生时，让森林里的每一颗决策树分别进行判断，以投票最多的结果作为最终的判断结果。以此来提高正确的概率。

## 使用 Mahout 进行 Random Forests 实现
数据集示例：
```
1,1.52101,13.64,4.49,1.10,71.78,0.06,8.75,0.00,0.00,1
2,1.51761,13.89,3.60,1.36,72.73,0.48,7.83,0.00,0.00,1
3,1.51618,13.53,3.55,1.54,72.99,0.39,7.78,0.00,0.00,1
4,1.51766,13.21,3.69,1.29,72.61,0.57,8.22,0.00,0.00,1
5,1.51742,13.27,3.62,1.24,73.08,0.55,8.07,0.00,0.00,1
6,1.51596,12.79,3.61,1.62,72.97,0.64,8.07,0.00,0.26,1
7,1.51743,13.30,3.60,1.14,73.09,0.58,8.17,0.00,0.00,1
8,1.51756,13.15,3.61,1.05,73.24,0.57,8.24,0.00,0.00,1
9,1.51918,14.04,3.58,1.37,72.08,0.56,8.30,0.00,0.00,1
10,1.51755,13.00,3.60,1.36,72.99,0.57,8.40,0.00,0.11,1
...
```
```
//上传数据集
$ hadoop fs -put glass.txt /user/ubuntu/glass.txt

//生成描述文件
ubuntu@master:~/algorithm$ mahout describe -p /user/ubuntu/glass.txt -f glass.info -d I 9 N L
MAHOUT_LOCAL is not set; adding HADOOP_CONF_DIR to classpath.
Running on hadoop, using /home/ubuntu/cloud/hadoop-2.7.2/bin/hadoop and HADOOP_CONF_DIR=/home/ubuntu/cloud/hadoop-2.7.2/etc/hadoop
MAHOUT-JOB: /home/ubuntu/cloud/mahout-0.10.1/mahout-examples-0.10.1-job.jar
17/01/14 02:53:49 WARN MahoutDriver: No describe.props found on classpath, will use command-line arguments only
17/01/14 02:53:49 INFO Describe: Generating the descriptor...
17/01/14 02:53:50 INFO Describe: generating the dataset...
17/01/14 02:53:51 INFO Describe: storing the dataset description
17/01/14 02:53:51 INFO MahoutDriver: Program took 1584 ms (Minutes: 0.0264)

// 生成随机森林
$ mahout buildforest -d /user/ubuntu/glass.txt -ds glass.info -sl 3 -ms 3 -p -t 5 -o output

....

17/01/14 03:05:50 INFO HadoopUtil: Deleting hdfs://master:9000/user/ubuntu/output
17/01/14 03:05:50 INFO BuildForest: Build Time: 0h 0m 15s 86
17/01/14 03:05:50 INFO BuildForest: Forest num Nodes: 203
17/01/14 03:05:50 INFO BuildForest: Forest mean num Nodes: 40
17/01/14 03:05:50 INFO BuildForest: Forest mean max Depth: 9
17/01/14 03:05:50 INFO BuildForest: Storing the forest in: output/forest.seq
17/01/14 03:05:50 INFO MahoutDriver: Program took 16124 ms (Minutes: 0.2687333333333333)

// 对随机森林进行评估
$ mahout testforest -i /user/ubuntu/glass.txt -ds glass.info -m output -mr -a -o test
...

17/01/14 03:09:48 INFO TestForest: 
=======================================================
Summary
-------------------------------------------------------
Correctly Classified Instances          :        191	   89.2523%
Incorrectly Classified Instances        :         23	   10.7477%
Total Classified Instances              :        214

=======================================================
Confusion Matrix
-------------------------------------------------------
a    	b    	c    	d    	e    	f    	<--Classified as
68   	1    	1    	0    	0    	0    	 |  70    	a     = 1
6    	68   	1    	0    	0    	1    	 |  76    	b     = 2
5    	1    	11   	0    	0    	0    	 |  17    	c     = 3
0    	0    	0    	10   	0    	3    	 |  13    	d     = 5
0    	0    	0    	0    	8    	1    	 |  9     	e     = 6
1    	1    	1    	0    	0    	26   	 |  29    	f     = 7

=======================================================
Statistics
-------------------------------------------------------
Kappa                                       0.7707
Accuracy                                   89.2523%
Reliability                                72.3985%
Reliability (standard deviation)            0.3365
Weighted precision                           0.897
Weighted recall                             0.8925
Weighted F1 score                           0.8914

17/01/14 03:09:48 INFO MahoutDriver: Program took 18829 ms (Minutes: 0.3138166666666667)
```

参考文章
> 张良均等.Hadoop大数据分析与挖掘实战.机械工业出版社.2016.10
> [决策树分类和预测算法的原理及实现](http://blog.csdn.net/mikefei007/article/details/53895015)
> [数据挖掘之决策树](http://www.cnblogs.com/hantan2008/archive/2015/07/27/4674097.html)
> [算法杂货铺——分类算法之决策树](http://www.cnblogs.com/leoo2sk/archive/2010/09/19/decision-tree.html "算法杂货铺——分类算法之决策树(Decision tree)")
> [了解信息增益和决策树](http://www.cnblogs.com/wentingtu/archive/2012/03/24/2416235.html)