<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Datax 自定义函数 dx_groovy]]></title>
    <url>%2F2017%2F09%2F16%2FDatx%20%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0%E7%BC%96%E5%86%99%20groovy%2F</url>
    <content type="text"><![CDATA[DataX 是阿里云开源离线同步工具，致力于实现包括关系型数据库(MySQL、Oracle等)、HDFS、Hive、MaxCompute(原ODPS)、HBase、FTP等各种异构数据源之间稳定高效的数据同步功能。 Datax 的数据转换支持 UserDefined Function 官方的使用说明如下： dx_groovy 参数。 第一个参数： groovy code 第二个参数（列表或者为空）：extraPackage 备注： dx_groovy只能调用一次。不能多次调用。 groovy code中支持java.lang, java.util的包，可直接引用的对象有record，以及element下的各种column（BoolColumn.class,BytesColumn.class,DateColumn.class,DoubleColumn.class,LongColumn.class,StringColumn.class）。不支持其他包，如果用户有需要用到其他包，可设置extraPackage，注意extraPackage不支持第三方jar包。 groovy code中，返回更新过的Record（比如record.setColumn(columnIndex, new StringColumn(newValue));），或者null。返回null表示过滤此行。 用户可以直接调用静态的Util方式（GroovyTransformerStaticUtil），目前GroovyTransformerStaticUtil的方法列表 (按需补充)： 举例:1234567groovy 实现的subStr: String code = "Column column = record.getColumn(1);\n" + " String oriValue = column.asString();\n" + " String newValue = oriValue.substring(0, 3);\n" + " record.setColumn(1, new StringColumn(newValue));\n" + " return record;"; dx_groovy(record); 123456groovy 实现的ReplaceString code2 = "Column column = record.getColumn(1);\n" + " String oriValue = column.asString();\n" + " String newValue = \"****\" + oriValue.substring(3, oriValue.length());\n" + " record.setColumn(1, new StringColumn(newValue));\n" + " return record;"; 12345678910111213141516171819groovy 实现的PadString code3 = "Column column = record.getColumn(1);\n" + " String oriValue = column.asString();\n" + " String padString = \"12345\";\n" + " String finalPad = \"\";\n" + " int NeedLength = 8 - oriValue.length();\n" + " while (NeedLength &gt; 0) &#123;\n" + "\n" + " if (NeedLength &gt;= padString.length()) &#123;\n" + " finalPad += padString;\n" + " NeedLength -= padString.length();\n" + " &#125; else &#123;\n" + " finalPad += padString.substring(0, NeedLength);\n" + " NeedLength = 0;\n" + " &#125;\n" + " &#125;\n" + " String newValue= finalPad + oriValue;\n" + " record.setColumn(1, new StringColumn(newValue));\n" + " return record;"; 以下是我自己实现的字符串替换 dx_groovy 函数{ “job”: { &quot;content&quot;: [ { &quot;reader&quot;: { &quot;name&quot;: &quot;mysqlreader&quot;, &quot;parameter&quot;: { &quot;column&quot;: [&quot;user_role_id&quot;,&quot;username&quot;,&quot;ROLE&quot;], &quot;connection&quot;: [ { &quot;jdbcUrl&quot;: [&quot;jdbc:mysql://10.1.18.155:3306/saiku_test&quot;], &quot;table&quot;: [&quot;user_roles_copy&quot;] } ], &quot;password&quot;: &quot;123456&quot;, &quot;username&quot;: &quot;root&quot;, &quot;where&quot;: &quot;&quot; } }, &quot;writer&quot;: { &quot;name&quot;: &quot;mysqlwriter&quot;, &quot;parameter&quot;: { &quot;column&quot;: [&quot;user_role_id&quot;,&quot;username&quot;,&quot;ROLE&quot;], &quot;connection&quot;: [ { &quot;jdbcUrl&quot;: &quot;jdbc:mysql://10.1.18.155:3306/saiku_test&quot;, &quot;table&quot;: [&quot;user_roles_trans&quot;] } ], &quot;password&quot;: &quot;123456&quot;, &quot;preSql&quot;: [], &quot;session&quot;: [], &quot;username&quot;: &quot;root&quot;, &quot;writeMode&quot;: &quot;insert&quot; } }, &quot;transformer&quot;: [ { &quot;name&quot;: &quot;dx_filter&quot;, &quot;parameter&quot;: { &quot;columnIndex&quot;:1, &quot;paras&quot;:[&quot;=&quot;,&quot;null&quot;] } },{ &quot;name&quot;: &quot;dx_groovy&quot;, &quot;parameter&quot;: { &quot;code&quot;: &quot;Column column = record.getColumn(2);\nString oriValue = column.asString();\nString sourceString = \&quot;ROLE_ADMIN\&quot;;\nString changeString = \&quot;replaceTest\&quot;;\nif (oriValue.equals(sourceString)){record.setColumn(2, new StringColumn(changeString));\nreturn record;}\nreturn record;&quot;, &quot;extraPackage&quot;:[] } } ] } ], &quot;setting&quot;: { &quot;speed&quot;: { &quot;channel&quot;: &quot;1&quot; } } }}]]></content>
      <categories>
        <category>Database</category>
      </categories>
      <tags>
        <tag>Database</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Oracle Mysql 日期格式化]]></title>
    <url>%2F2017%2F09%2F16%2FOracle%20Mysql%20%E6%97%A5%E6%9C%9F%E6%A0%BC%E5%BC%8F%E5%8C%96%2F</url>
    <content type="text"><![CDATA[Mysqlmysql查询记录如果有时间戳字段时，查看结果不方便，不能即时看到时间戳代表的含义，现提供mysql格式换时间函数，可以方便的看到格式化后的时间。 1. DATE_FORMAT() 函数用于以不同的格式显示日期/时间数据。DATE_FORMAT(date,format) 12345678910111213141516171819202122232425262728293031%a 缩写星期名%b 缩写月名%c 月，数值%D 带有英文前缀的月中的天%d 月的天，数值(00-31)%e 月的天，数值(0-31)%f 微秒%H 小时 (00-23)%h 小时 (01-12)%I 小时 (01-12)%i 分钟，数值(00-59)%j 年的天 (001-366)%k 小时 (0-23)%l 小时 (1-12)%M 月名%m 月，数值(00-12)%p AM 或 PM%r 时间，12-小时（hh:mm:ss AM 或 PM）%S 秒(00-59)%s 秒(00-59)%T 时间, 24-小时 (hh:mm:ss)%U 周 (00-53) 星期日是一周的第一天%u 周 (00-53) 星期一是一周的第一天%V 周 (01-53) 星期日是一周的第一天，与 %X 使用%v 周 (01-53) 星期一是一周的第一天，与 %x 使用%W 星期名%w 周的天 （0=星期日, 6=星期六）%X 年，其中的星期日是周的第一天，4 位，与 %V 使用%x 年，其中的星期一是周的第一天，4 位，与 %v 使用%Y 年，4 位%y 年，2 位 例子1234DATE_FORMAT(NOW(),'%b %d %Y %h:%i %p') DATE_FORMAT(NOW(),'%m-%d-%Y') DATE_FORMAT(NOW(),'%d %b %y') DATE_FORMAT(NOW(),'%d %b %Y %T:%f') 输出结果1234Dec 29 2008 11:45 PM 12-29-2008 29 Dec 08 29 Dec 2008 16:25:46 2. MySQL 格式化函数 FROM_UNIXTIME()123SELECT FROM_UNIXTIME(date, '%Y-%c-%d %h:%i:%s' ) as post_date , date_format(NOW(), '%Y-%c-%d %h:%i:%s' ) as post_date_gmt FROM `article` where outkey = 'Y' FROM_UNIXTIME( unix_timestamp )参数：一般为10位的时间戳，如:1417363200 返回值：有两种，可能是类似 ‘YYYY-MM-DD HH:MM:SS’ 这样的字符串，也有可能是类似于 YYYYMMDDHHMMSS.uuuuuu 这样的数字，具体返回什么取决于该函数被调用的形式。 1234567mysql&gt; select FROM_UNIXTIME(1344887103); +---------------------------+ | FROM_UNIXTIME(1344887103) | +---------------------------+ | 2012-08-14 03:45:03 | +---------------------------+ row in set (0.00 sec) FROM_UNIXTIME( unix_timestamp ，format )参数 unix_timestamp ：与方法 FROM_UNIXTIME( unix_timestamp ) 中的参数含义一样；参数 format : 转换之后的时间字符串显示的格式; 返回值：按照指定的时间格式显示的字符串； 1234567891011121314mysql&gt; select FROM_UNIXTIME(1344887103,'%Y-%M-%D %h:%i:%s'); +-----------------------------------------------+ | FROM_UNIXTIME(1344887103,'%Y-%M-%D %h:%i:%s') | +-----------------------------------------------+ | 2012-August-14th 03:45:03 | +-----------------------------------------------+ row in set (0.00 sec) mysql&gt; select FROM_UNIXTIME(1344887103,'%Y-%m-%D %h:%i:%s'); +-----------------------------------------------+ | FROM_UNIXTIME(1344887103,'%Y-%m-%D %h:%i:%s') | +-----------------------------------------------+ | 2012-08-14th 03:45:03 | +-----------------------------------------------+ row in set (0.00 sec) Oracle格式化函数1234TO_CHAR(datetime, 'format') TO_DATE(character, 'format') TO_TIMESTAMP(character, 'format') TO_TIMESTAMP_TZ(character, 'format') 不同格式化产生不同结果SELECT TO_CHAR(current_timestamp, &#39;format&#39;) FROM DUAL; Format Result YYYY-MM-DD 2015-06-15 YYYY-MON-DD 2015-JUN-15 YYYY-MM-DD HH24:MI:SS FF3 2015-06-15 13:18:10 700 DL Monday, June 15, 2015 TS 1:18:10 PM Oracle 支持的格式123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869Format DescriptionY 年的最后一位数字，如：5YY 年的最后两位数字，如：15YYY 年的最后三位数字，如：015YYYY 年，如：2015Y,YYY 年用逗号分割SYYYY 年YEAR 年拼写，如：TWENTY FIFTEENSYEAR 年拼写，如：TWENTY FIFTEENI ISO年的最后一位数字，如：5IY ISO年的最后两位数字，如：15IYY ISO年的最后三位数字，如：015IYYY ISO年，如：2015RR 两位数字年，如：15RRRR 四位数字年，如：2015 MM Month (01-12)MON 月份简称，如：JUNMONTH 月份全称，如：JUNERM 罗马数字月份 D Day of week (1-7)DD Day of month (1-31)DDD Day of year (1-366) HH Hour of day (1-12)HH12 Hour of day (1-12)HH24 Hour of day (0-23)MI Minute (0-59)SS Second (0-59)SSSSS Seconds past midnightFF [1..9] 毫秒 DS 日期简称，如：6/12/2015DL 日期全称，如：Friday, June 12, 2015TS 时间简称，如：5:18:03 PM CC 世纪，如：21SCC 世纪，如：21Q Quarter of year (1, 2, 3, 4)W Week of month (1-5)WW Week of year (1-53)IW ISO Week of year (1-52 or 1-53)DY 星期简称，如：FriDAY 星期全称，如：Friday AM A.M. PM P.M. AD A.D. BC B.C. TZD 夏令时TZR 时区TZH 时区之时差TZM 时区之分钟差EE era 全称E era 简称J The number of days since January 1, 4712 BCFM 去掉首尾空格FX 精确匹配X 秒和毫秒分隔符TH DDTH --&gt; 4thSP DDSP --&gt;FOURSPTH DDSPTH --&gt; FOURTHTHSP DDTHSP --&gt; FOURTH 默认情况下，Oracle 格式化日期时，有一定的容错性，如下面的 SQL 返回正确的结果。 12select to_date('20150612', 'YYYY/MM/DD') from dual select to_date('2015#06#12', 'YYYY/MM/DD') from dual 如果你想精确匹配，你可以加上 FX修饰符，如:select to_date(&#39;2015/06/12&#39;, &#39;FXYYYY/MM/DD&#39;) from dual 参考整理自： http://blog.csdn.net/shangboerds/article/details/46502711http://www.cnblogs.com/duhuo/p/5650876.html]]></content>
      <categories>
        <category>Database</category>
      </categories>
      <tags>
        <tag>Database</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mapreduce 错误 The required MAP capability is more than the supported max container capability in the cluster]]></title>
    <url>%2F2017%2F08%2F05%2Fmapreduce%20%E9%94%99%E8%AF%AF%20The%20required%20MAP%20capability%20is%20more%20than%20the%20supported%20max%20container%20capability%20in%20the%20cluster%2F</url>
    <content type="text"><![CDATA[具体错误12The required MAP capability is more than the supported max container capability in the cluster. Killing the Job. mapResourceRequest: &lt;memory:3072, vCores:1&gt; maxContainerCapability:&lt;memory:1460, vCores:1&gt;Job received Kill while in RUNNING state. 此错导致 job 被 kill 解决方法解答1 https://stackoverflow.com/questions/25878458/rhadoop-reduce-capability-required-is-more-than-the-supported-max-container-cap I have not used RHadoop. However I’ve had a very similar problem on my cluster, and this problem seems to be linked only to MapReduce. The maxContainerCapability in this log refers to the yarn.scheduler.maximum-allocation-mb property of your yarn-site.xml configuration. It is the maximum amount of memory that can be used in any container. The mapResourceReqt and reduceResourceReqt in your log refer to the mapreduce.map.memory.mb and mapreduce.reduce.memory.mb properties of your mapred-site.xml configuration. It is the memory size of the containers that will be created for a Mapper or a Reducer in mapreduce. If the size of your Reducer’s container is set to be greater than yarn.scheduler.maximum-allocation-mb, which seems to be the case here, your job will be killed because it is not allowed to allocate so much memory to a container. Check your configuration at http://[your-resource-manager]:8088/conf and you should normally find these values and see that this is the case. Maybe your new environment has these values set to 4096 Mb (which is quite big, the default in Hadoop 2.7.1 being 1024). Solution You should either lower the mapreduce.[map|reduce].memory.mb values down to 1024, or if you have lots of memory and want huge containers, raise the yarn.scheduler.maximum-allocation-mb value to 4096. Only then MapReduce be able to create containers. I hope this helps. 解答2 https://stackoverflow.com/questions/25753983/how-do-you-change-the-max-container-capability-in-hadoop-cluster To do this on Hortonworks 2.1, I had to increase VirtualBox memory from 4096 to 8192 (don’t know if that was strictly necessary)Enabled Ambari from http://my.local.host:8000Log into Ambari from http://my.local.host:8080change the values of yarn.nodemanager.resource.memory-mb and yarn.scheduler.maximum-allocation-mb from the defaults to 4096Save and restart everything (via Ambari)This got me past the “capability required” errors, but the actual wordcount.R doesn’t seem to want to complete. Things like hdfs.ls(“/data”) do work, however 简而言之：yarn-site.xml 中的yarn.scheduler.maximum-allocation-mb yarn.nodemanager.resource.memory-mb 配置的值 &gt;= mapred-site.xml 中mapreduce.map.memory.mb、mapreduce.reduce.memory.mb 的值 参考： Yarn最佳实践 http://blog.csdn.net/jiangshouzhuang/article/details/52595781]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kylin api 整理（部分官网未给出）]]></title>
    <url>%2F2017%2F07%2F31%2Fkylin%20api%2F</url>
    <content type="text"><![CDATA[kylin 的官网没有列出保存 cube 信息，model 信息，project等 rest api，这里通过查看源码对实际项目使用中有用到的 api 进行列举 官方文档http://kylin.apache.org/docs16/howto/howto_use_restapi.html#build-cube 保存项目POST /kylin/api/projects ProjectController.java Request Body name - required String 项目名 description - optional String 项目描述 导入 hive tablePOST /kylin/api/tables/{tables}/{project} Request Parameters tables - required string 需要导入的表名，用 , 分隔 project - required String 导入的目标项目 Response Sample1234&#123; "result.loaded": ["DEFAULT.SAMPLE_07"], "result.unloaded": ["sapmle_08"]&#125; 保存 modelPOST /kylin/api/models ModelController.java Request Body project - required String 项目名 modelName - required String Model 名 modelDescData - required String model 描述字符串，json 中必须用字符串，即 “ 用 \” 表示，详见下面例子 post json 请求示例12345&#123; "project": "game_inner", "modelName": "test_model2", "modelDescData": "&#123;\"name\": \"test_model2\", \"owner\": \"ADMIN\", \"description\": \"\", \"fact_table\": \"KYLIN_FLAT_DB.FACT_GAME\", \"lookups\": [], \"dimensions\": [ &#123; \"table\": \"KYLIN_FLAT_DB.FACT_GAME\", \"columns\": [ \"STARTTIME\", \"USERID\", \"GAME\", \"TYPEID\", \"DURATION\", \"LOCATIONID\" ] &#125; ], \"metrics\": [ \"STARTTIME\", \"USERID\", \"GAME\", \"TYPEID\", \"DURATION\", \"LOCATIONID\" ], \"filter_condition\": \"\", \"partition_desc\": &#123; \"partition_date_column\": null, \"partition_time_column\": null, \"partition_date_start\": 0, \"partition_date_format\": \"yyyy-MM-dd\", \"partition_time_format\": \"HH:mm:ss\", \"partition_type\": \"APPEND\", \"partition_condition_builder\": \"org.apache.kylin.metadata.model.PartitionDesc$DefaultPartitionConditionBuilder\" &#125;, \"capacity\": \"MEDIUM\"&#125;"&#125; Response Sample12345678&#123; "uuid": "536582f6-ffa8-415e-b9c8-58f864994ac5", "modelName": "test_model2", "modelDescData": "&#123;\"name\": \"test_model2\", \"owner\": \"ADMIN\", \"description\": \"\", \"fact_table\": \"KYLIN_FLAT_DB.FACT_GAME\", \"lookups\": [], \"dimensions\": [ &#123; \"table\": \"KYLIN_FLAT_DB.FACT_GAME\", \"columns\": [ \"STARTTIME\", \"USERID\", \"GAME\", \"TYPEID\", \"DURATION\", \"LOCATIONID\" ] &#125; ], \"metrics\": [ \"STARTTIME\", \"USERID\", \"GAME\", \"TYPEID\", \"DURATION\", \"LOCATIONID\" ], \"filter_condition\": \"\", \"partition_desc\": &#123; \"partition_date_column\": null, \"partition_time_column\": null, \"partition_date_start\": 0, \"partition_date_format\": \"yyyy-MM-dd\", \"partition_time_format\": \"HH:mm:ss\", \"partition_type\": \"APPEND\", \"partition_condition_builder\": \"org.apache.kylin.metadata.model.PartitionDesc$DefaultPartitionConditionBuilder\" &#125;, \"capacity\": \"MEDIUM\"&#125;", "successful": true, "message": null, "project": "game_inner"&#125; 保存 cube 详细配置POST /kylin/api/cubes CubeController.java Request Body project - required String 项目名 cubeName - required String Cube 名 cubeDescData - required String cube 描述字符串，json 中必须用字符串，即 “ 用 \” 表示，详见下面例子 Curl Example123# “QURNSU46S1lMSU4=”是 “ADMIN:KYLIN”的base64编码curl -X POST -H "Authorization: Basic QURNSU46S1lMSU4=" -H "Content-Type: application/json" -d '&#123; "project":"test_project","cubeName":"test_cube5","cubeDescData":”cube描述的字符串”&#125;' http://localhost:7070/kylin/api/cubes post json 示例12345&#123; "project":"game_inner", "cubeName":"test_cube4", "cubeDescData":"&#123; \"name\": \"test_cube4\", \"model_name\": \"test_game_model\", \"description\": \"\", \"null_string\": null, \"dimensions\": [ &#123; \"name\": \"STARTTIME\", \"table\": \"KYLIN_FLAT_DB.FACT_GAME\", \"column\": \"STARTTIME\", \"derived\": null &#125;, &#123; \"name\": \"USERID\", \"table\": \"KYLIN_FLAT_DB.FACT_GAME\", \"column\": \"USERID\", \"derived\": null &#125;, &#123; \"name\": \"GAME\", \"table\": \"KYLIN_FLAT_DB.FACT_GAME\", \"column\": \"GAME\", \"derived\": null &#125;, &#123; \"name\": \"TYPEID\", \"table\": \"KYLIN_FLAT_DB.FACT_GAME\", \"column\": \"TYPEID\", \"derived\": null &#125;, &#123; \"name\": \"DURATION\", \"table\": \"KYLIN_FLAT_DB.FACT_GAME\", \"column\": \"DURATION\", \"derived\": null &#125;, &#123; \"name\": \"LOCATIONID\", \"table\": \"KYLIN_FLAT_DB.DIM_LOCATION\", \"column\": null, \"derived\": [ \"LOCATIONID\" ] &#125; ], \"measures\": [ &#123; \"name\": \"_COUNT_\", \"function\": &#123; \"expression\": \"COUNT\", \"parameter\": &#123; \"type\": \"constant\", \"value\": \"1\", \"next_parameter\": null &#125;, \"returntype\": \"bigint\" &#125;, \"dependent_measure_ref\": null &#125; ], \"dictionaries\": [], \"rowkey\": &#123; \"rowkey_columns\": [ &#123; \"column\": \"STARTTIME\", \"encoding\": \"dict\", \"isShardBy\": false &#125;, &#123; \"column\": \"USERID\", \"encoding\": \"dict\", \"isShardBy\": false &#125;, &#123; \"column\": \"GAME\", \"encoding\": \"dict\", \"isShardBy\": false &#125;, &#123; \"column\": \"TYPEID\", \"encoding\": \"dict\", \"isShardBy\": false &#125;, &#123; \"column\": \"DURATION\", \"encoding\": \"dict\", \"isShardBy\": false &#125;, &#123; \"column\": \"LOCATIONID\", \"encoding\": \"dict\", \"isShardBy\": false &#125; ] &#125;, \"hbase_mapping\": &#123; \"column_family\": [ &#123; \"name\": \"F1\", \"columns\": [ &#123; \"qualifier\": \"M\", \"measure_refs\": [ \"_COUNT_\" ] &#125; ] &#125; ] &#125;, \"aggregation_groups\": [ &#123; \"includes\": [ \"STARTTIME\", \"USERID\", \"GAME\", \"TYPEID\", \"DURATION\", \"LOCATIONID\" ], \"select_rule\": &#123; \"hierarchy_dims\": [], \"mandatory_dims\": [], \"joint_dims\": [] &#125; &#125; ], \"signature\": \"YH/rFI7MAllwLXuyD3tBlw==\", \"notify_list\": [], \"status_need_notify\": [ \"ERROR\", \"DISCARDED\", \"SUCCEED\" ], \"partition_date_start\": 0, \"partition_date_end\": 3153600000000, \"auto_merge_time_ranges\": [ 604800000, 2419200000 ], \"retention_range\": 0, \"engine_type\": 2, \"storage_type\": 2, \"override_kylin_properties\": &#123;&#125;&#125;"&#125; Response Sample1234567891011&#123; "uuid": "2225fb4c-aafa-470a-8708-ddfbc44d9e78", "cubeName": "test_cube4", "cubeDescData": "&#123; \"name\": \"test_cube4\", \"model_name\": \"test_game_model\", \"description\": \"\", \"null_string\": null, \"dimensions\": [ &#123; \"name\": \"STARTTIME\", \"table\": \"KYLIN_FLAT_DB.FACT_GAME\", \"column\": \"STARTTIME\", \"derived\": null &#125;, &#123; \"name\": \"USERID\", \"table\": \"KYLIN_FLAT_DB.FACT_GAME\", \"column\": \"USERID\", \"derived\": null &#125;, &#123; \"name\": \"GAME\", \"table\": \"KYLIN_FLAT_DB.FACT_GAME\", \"column\": \"GAME\", \"derived\": null &#125;, &#123; \"name\": \"TYPEID\", \"table\": \"KYLIN_FLAT_DB.FACT_GAME\", \"column\": \"TYPEID\", \"derived\": null &#125;, &#123; \"name\": \"DURATION\", \"table\": \"KYLIN_FLAT_DB.FACT_GAME\", \"column\": \"DURATION\", \"derived\": null &#125;, &#123; \"name\": \"LOCATIONID\", \"table\": \"KYLIN_FLAT_DB.DIM_LOCATION\", \"column\": null, \"derived\": [ \"LOCATIONID\" ] &#125; ], \"measures\": [ &#123; \"name\": \"_COUNT_\", \"function\": &#123; \"expression\": \"COUNT\", \"parameter\": &#123; \"type\": \"constant\", \"value\": \"1\", \"next_parameter\": null &#125;, \"returntype\": \"bigint\" &#125;, \"dependent_measure_ref\": null &#125; ], \"dictionaries\": [], \"rowkey\": &#123; \"rowkey_columns\": [ &#123; \"column\": \"STARTTIME\", \"encoding\": \"dict\", \"isShardBy\": false &#125;, &#123; \"column\": \"USERID\", \"encoding\": \"dict\", \"isShardBy\": false &#125;, &#123; \"column\": \"GAME\", \"encoding\": \"dict\", \"isShardBy\": false &#125;, &#123; \"column\": \"TYPEID\", \"encoding\": \"dict\", \"isShardBy\": false &#125;, &#123; \"column\": \"DURATION\", \"encoding\": \"dict\", \"isShardBy\": false &#125;, &#123; \"column\": \"LOCATIONID\", \"encoding\": \"dict\", \"isShardBy\": false &#125; ] &#125;, \"hbase_mapping\": &#123; \"column_family\": [ &#123; \"name\": \"F1\", \"columns\": [ &#123; \"qualifier\": \"M\", \"measure_refs\": [ \"_COUNT_\" ] &#125; ] &#125; ] &#125;, \"aggregation_groups\": [ &#123; \"includes\": [ \"STARTTIME\", \"USERID\", \"GAME\", \"TYPEID\", \"DURATION\", \"LOCATIONID\" ], \"select_rule\": &#123; \"hierarchy_dims\": [], \"mandatory_dims\": [], \"joint_dims\": [] &#125; &#125; ], \"signature\": \"YH/rFI7MAllwLXuyD3tBlw==\", \"notify_list\": [], \"status_need_notify\": [ \"ERROR\", \"DISCARDED\", \"SUCCEED\" ], \"partition_date_start\": 0, \"partition_date_end\": 3153600000000, \"auto_merge_time_ranges\": [ 604800000, 2419200000 ], \"retention_range\": 0, \"engine_type\": 2, \"storage_type\": 2, \"override_kylin_properties\": &#123;&#125;&#125;", "streamingData": null, "kafkaData": null, "successful": true, "message": null, "project": "game_inner", "streamingCube": null&#125; 构建 CubePUT /kylin/api/cubes/{cubeName}/rebuild Path Variable cubeName - required string Cube name.Request Body startTime - required long Start timestamp of data to build, e.g. 1388563200000 for 2014-1-1 endTime - required long End timestamp of data to build- buildType - required string Supported build type: ‘BUILD’, ‘MERGE’, ‘REFRESH’ 获取 Cube （查看信息）GET /kylin/api/cubes/{cubeName} ###Path Variable cubeName - required string Cube name to find. GET /kylin/api/cube_desc/{cubeName} Get descriptor for specified cube instance. Path Variable cubeName - required string Cube name. Response Sample123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250[ &#123; "uuid": "a24ca905-1fc6-4f67-985c-38fa5aeafd92", "name": "test_kylin_cube_with_slr_desc", "description": null, "dimensions": [ &#123; "id": 0, "name": "CAL_DT", "table": "EDW.TEST_CAL_DT", "column": null, "derived": [ "WEEK_BEG_DT" ], "hierarchy": false &#125;, &#123; "id": 1, "name": "CATEGORY", "table": "DEFAULT.TEST_CATEGORY_GROUPINGS", "column": null, "derived": [ "USER_DEFINED_FIELD1", "USER_DEFINED_FIELD3", "UPD_DATE", "UPD_USER" ], "hierarchy": false &#125;, &#123; "id": 2, "name": "CATEGORY_HIERARCHY", "table": "DEFAULT.TEST_CATEGORY_GROUPINGS", "column": [ "META_CATEG_NAME", "CATEG_LVL2_NAME", "CATEG_LVL3_NAME" ], "derived": null, "hierarchy": true &#125;, &#123; "id": 3, "name": "LSTG_FORMAT_NAME", "table": "DEFAULT.TEST_KYLIN_FACT", "column": [ "LSTG_FORMAT_NAME" ], "derived": null, "hierarchy": false &#125;, &#123; "id": 4, "name": "SITE_ID", "table": "EDW.TEST_SITES", "column": null, "derived": [ "SITE_NAME", "CRE_USER" ], "hierarchy": false &#125;, &#123; "id": 5, "name": "SELLER_TYPE_CD", "table": "EDW.TEST_SELLER_TYPE_DIM", "column": null, "derived": [ "SELLER_TYPE_DESC" ], "hierarchy": false &#125;, &#123; "id": 6, "name": "SELLER_ID", "table": "DEFAULT.TEST_KYLIN_FACT", "column": [ "SELLER_ID" ], "derived": null, "hierarchy": false &#125; ], "measures": [ &#123; "id": 1, "name": "GMV_SUM", "function": &#123; "expression": "SUM", "parameter": &#123; "type": "column", "value": "PRICE", "next_parameter": null &#125;, "returntype": "decimal(19,4)" &#125;, "dependent_measure_ref": null &#125;, &#123; "id": 2, "name": "GMV_MIN", "function": &#123; "expression": "MIN", "parameter": &#123; "type": "column", "value": "PRICE", "next_parameter": null &#125;, "returntype": "decimal(19,4)" &#125;, "dependent_measure_ref": null &#125;, &#123; "id": 3, "name": "GMV_MAX", "function": &#123; "expression": "MAX", "parameter": &#123; "type": "column", "value": "PRICE", "next_parameter": null &#125;, "returntype": "decimal(19,4)" &#125;, "dependent_measure_ref": null &#125;, &#123; "id": 4, "name": "TRANS_CNT", "function": &#123; "expression": "COUNT", "parameter": &#123; "type": "constant", "value": "1", "next_parameter": null &#125;, "returntype": "bigint" &#125;, "dependent_measure_ref": null &#125;, &#123; "id": 5, "name": "ITEM_COUNT_SUM", "function": &#123; "expression": "SUM", "parameter": &#123; "type": "column", "value": "ITEM_COUNT", "next_parameter": null &#125;, "returntype": "bigint" &#125;, "dependent_measure_ref": null &#125; ], "rowkey": &#123; "rowkey_columns": [ &#123; "column": "SELLER_ID", "length": 18, "dictionary": null, "mandatory": true &#125;, &#123; "column": "CAL_DT", "length": 0, "dictionary": "true", "mandatory": false &#125;, &#123; "column": "LEAF_CATEG_ID", "length": 0, "dictionary": "true", "mandatory": false &#125;, &#123; "column": "META_CATEG_NAME", "length": 0, "dictionary": "true", "mandatory": false &#125;, &#123; "column": "CATEG_LVL2_NAME", "length": 0, "dictionary": "true", "mandatory": false &#125;, &#123; "column": "CATEG_LVL3_NAME", "length": 0, "dictionary": "true", "mandatory": false &#125;, &#123; "column": "LSTG_FORMAT_NAME", "length": 12, "dictionary": null, "mandatory": false &#125;, &#123; "column": "LSTG_SITE_ID", "length": 0, "dictionary": "true", "mandatory": false &#125;, &#123; "column": "SLR_SEGMENT_CD", "length": 0, "dictionary": "true", "mandatory": false &#125; ], "aggregation_groups": [ [ "LEAF_CATEG_ID", "META_CATEG_NAME", "CATEG_LVL2_NAME", "CATEG_LVL3_NAME", "CAL_DT" ] ] &#125;, "signature": "lsLAl2jL62ZApmOLZqWU3g==", "last_modified": 1445850327000, "model_name": "test_kylin_with_slr_model_desc", "null_string": null, "hbase_mapping": &#123; "column_family": [ &#123; "name": "F1", "columns": [ &#123; "qualifier": "M", "measure_refs": [ "GMV_SUM", "GMV_MIN", "GMV_MAX", "TRANS_CNT", "ITEM_COUNT_SUM" ] &#125; ] &#125; ] &#125;, "notify_list": null, "auto_merge_time_ranges": null, "retention_range": 0 &#125;] 获取构建任务状态GET /kylin/api/jobs/{jobId} Path variablejobId - required string Job id. 此处构建 cube 时返回的 uuid 即为任务 id Response Sample12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758&#123; "uuid":"c143e0e4-ac5f-434d-acf3-46b0d15e3dc6", "last_modified":1407908916705, "name":"test_kylin_cube_with_slr_empty - 19700101000000_20140731160000 - BUILD - PDT 2014-08-12 22:48:36", "type":"BUILD", "duration":0, "related_cube":"test_kylin_cube_with_slr_empty", "related_segment":"19700101000000_20140731160000", "exec_start_time":0, "exec_end_time":0, "mr_waiting":0, "steps":[ &#123; "interruptCmd":null, "name":"Create Intermediate Flat Hive Table", "sequence_id":0, "exec_cmd":"hive -e \"DROP TABLE IF EXISTS kylin_intermediate_test_kylin_cube_with_slr_desc_19700101000000_20140731160000_c143e0e4_ac5f_434d_acf3_46b0d15e3dc6;\nCREATE EXTERNAL TABLE IF NOT EXISTS kylin_intermediate_test_kylin_cube_with_slr_desc_19700101000000_20140731160000_c143e0e4_ac5f_434d_acf3_46b0d15e3dc6\n(\nCAL_DT date\n,LEAF_CATEG_ID int\n,LSTG_SITE_ID int\n,META_CATEG_NAME string\n,CATEG_LVL2_NAME string\n,CATEG_LVL3_NAME string\n,LSTG_FORMAT_NAME string\n,SLR_SEGMENT_CD smallint\n,SELLER_ID bigint\n,PRICE decimal\n)\nROW FORMAT DELIMITED FIELDS TERMINATED BY '\\177'\nSTORED AS SEQUENCEFILE\nLOCATION '/tmp/kylin-c143e0e4-ac5f-434d-acf3-46b0d15e3dc6/kylin_intermediate_test_kylin_cube_with_slr_desc_19700101000000_20140731160000_c143e0e4_ac5f_434d_acf3_46b0d15e3dc6';\nSET mapreduce.job.split.metainfo.maxsize=-1;\nSET mapred.compress.map.output=true;\nSET mapred.map.output.compression.codec=com.hadoop.compression.lzo.LzoCodec;\nSET mapred.output.compress=true;\nSET mapred.output.compression.codec=com.hadoop.compression.lzo.LzoCodec;\nSET mapred.output.compression.type=BLOCK;\nSET mapreduce.job.max.split.locations=2000;\nSET hive.exec.compress.output=true;\nSET hive.auto.convert.join.noconditionaltask = true;\nSET hive.auto.convert.join.noconditionaltask.size = 300000000;\nINSERT OVERWRITE TABLE kylin_intermediate_test_kylin_cube_with_slr_desc_19700101000000_20140731160000_c143e0e4_ac5f_434d_acf3_46b0d15e3dc6\nSELECT\nTEST_KYLIN_FACT.CAL_DT\n,TEST_KYLIN_FACT.LEAF_CATEG_ID\n,TEST_KYLIN_FACT.LSTG_SITE_ID\n,TEST_CATEGORY_GROUPINGS.META_CATEG_NAME\n,TEST_CATEGORY_GROUPINGS.CATEG_LVL2_NAME\n,TEST_CATEGORY_GROUPINGS.CATEG_LVL3_NAME\n,TEST_KYLIN_FACT.LSTG_FORMAT_NAME\n,TEST_KYLIN_FACT.SLR_SEGMENT_CD\n,TEST_KYLIN_FACT.SELLER_ID\n,TEST_KYLIN_FACT.PRICE\nFROM TEST_KYLIN_FACT\nINNER JOIN TEST_CAL_DT\nON TEST_KYLIN_FACT.CAL_DT = TEST_CAL_DT.CAL_DT\nINNER JOIN TEST_CATEGORY_GROUPINGS\nON TEST_KYLIN_FACT.LEAF_CATEG_ID = TEST_CATEGORY_GROUPINGS.LEAF_CATEG_ID AND TEST_KYLIN_FACT.LSTG_SITE_ID = TEST_CATEGORY_GROUPINGS.SITE_ID\nINNER JOIN TEST_SITES\nON TEST_KYLIN_FACT.LSTG_SITE_ID = TEST_SITES.SITE_ID\nINNER JOIN TEST_SELLER_TYPE_DIM\nON TEST_KYLIN_FACT.SLR_SEGMENT_CD = TEST_SELLER_TYPE_DIM.SELLER_TYPE_CD\nWHERE (test_kylin_fact.cal_dt &lt; '2014-07-31 16:00:00')\n;\n\"", "interrupt_cmd":null, "exec_start_time":0, "exec_end_time":0, "exec_wait_time":0, "step_status":"PENDING", "cmd_type":"SHELL_CMD_HADOOP", "info":null, "run_async":false &#125;, &#123; "interruptCmd":null, "name":"Extract Fact Table Distinct Columns", "sequence_id":1, "exec_cmd":" -conf C:/kylin/Kylin/server/src/main/resources/hadoop_job_conf_medium.xml -cubename test_kylin_cube_with_slr_empty -input /tmp/kylin-c143e0e4-ac5f-434d-acf3-46b0d15e3dc6/kylin_intermediate_test_kylin_cube_with_slr_desc_19700101000000_20140731160000_c143e0e4_ac5f_434d_acf3_46b0d15e3dc6 -output /tmp/kylin-c143e0e4-ac5f-434d-acf3-46b0d15e3dc6/test_kylin_cube_with_slr_empty/fact_distinct_columns -jobname Kylin_Fact_Distinct_Columns_test_kylin_cube_with_slr_empty_Step_1", "interrupt_cmd":null, "exec_start_time":0, "exec_end_time":0, "exec_wait_time":0, "step_status":"PENDING", "cmd_type":"JAVA_CMD_HADOOP_FACTDISTINCT", "info":null, "run_async":true &#125;, &#123; "interruptCmd":null, "name":"Load HFile to HBase Table", "sequence_id":12, "exec_cmd":" -input /tmp/kylin-c143e0e4-ac5f-434d-acf3-46b0d15e3dc6/test_kylin_cube_with_slr_empty/hfile/ -htablename KYLIN-CUBE-TEST_KYLIN_CUBE_WITH_SLR_EMPTY-19700101000000_20140731160000_11BB4326-5975-4358-804C-70D53642E03A -cubename test_kylin_cube_with_slr_empty", "interrupt_cmd":null, "exec_start_time":0, "exec_end_time":0, "exec_wait_time":0, "step_status":"PENDING", "cmd_type":"JAVA_CMD_HADOOP_NO_MR_BULKLOAD", "info":null, "run_async":false &#125; ], "job_status":"PENDING", "progress":0.0&#125;]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Kylin</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kylin 在 CDH 中的安装、错误解决]]></title>
    <url>%2F2017%2F06%2F21%2FKylin%20%E5%9C%A8%20CDH%20%E4%B8%AD%E7%9A%84%E5%AE%89%E8%A3%85%E3%80%81%E9%94%99%E8%AF%AF%E8%A7%A3%E5%86%B3%2F</url>
    <content type="text"><![CDATA[Apache Kylin中文名麒（shen）麟（shou） 是Hadoop动物园的重要成员。Apache Kylin是一个开源的分布式分析引擎，最初由eBay开发贡献至开源社区。它提供Hadoop之上的SQL查询接口及多维分析（OLAP）能力以支持大规模数据，能够处理TB乃至PB级别的分析任务，能够在亚秒级查询巨大的Hive表，并支持高并发。 安装环境 CDH 5.10.0 apache-kylin-1.6.0-cdh5.7-bin 官网建议 CDH 5.10 安装 Kylin 2.0 ，尝试后发现部分查询有问题，后又换成 1.6.0 版本 下载并解压到 /opt 目录123tar zvxf apache-kylin-1.5.2.1-cdh5.7-bin.tar -C /optmv /opt/apache-kylin-1.6.0-cdh5.7-bin /opt/kylin-1.6.0 赋予权限 kylin 运行用户必须有，hdfs,hive,hbase 操作权限 /opt/kylin-1.6.0 以下目录，当前用户必须有读写修改权限 环境变量 /etc/profile添加以下变量12export KYLIN_HOME=/opt/kylin-1.6.0export HCAT_HOME=/opt/cloudera/parcels/CDH/lib/hive-hcatalog 由于使用的是 CDH 集群，其它组件环境已经配置好，只需配以上变量即可 配置 kylin.properties123456789# 修改kylin.hive.client=beelinekylin.hive.beeline.params=-n hive --hiveconf hive.security.authorization.sqlstd.confwhitelist.append='mapreduce.job.*|dfs.*' -u 'jdbc:hive2://master:10000'# 增加kylin.job.jar=/opt/kylin-1.6.0/lib/kylin-job-1.6.0.jarkylin.coprocessor.local.jar=/opt/kylin-1.6.0/lib/kylin-coprocessor-1.6.0.jarkylin.job.yarn.app.rest.check.status.url=http://master:8088/ws/v1/cluster/apps/$&#123;job_id&#125;?anonymous=true# kylin.job.mr.lib.dir=/opt/cloudera/parcels/CDH/lib/sentry/lib 环境检查/opt/kylin-1.6.0/bin 目录下12345./find-hive-dependency.sh./find-hbase-dependency.sh./check-env.sh 导入测试数据./sample.sh 启动 Kylin./kylin.sh start 访问 web 界面登录后台：http://xxxxx:7070/kylin账号密码：ADMIN/KYLIN 问题解决HDFS 权限 dfs.permissions 设置为 false 目录权限可使用，hdfs dfs -chmod -R 777 / kylin 2.0 版本中查询问题./kylin.sh org.apache.kylin.storage.hbase.util.DeployCoprocessorCLI /opt/kylin-1.6.0/lib/kylin-coprocessor-1.6.0.jar all 构建 Cube 出错 如图，一直卡在构建 cube 的第三步，相应 mapreduce 任务在 map 阶段出错1Error: java.lang.ClassNotFoundException: org.apache.hadoop.hive.serde2.typeinfo.TypeInfo at java.net.URLClassLoader$1.run(URLClassLoader.java:366) at java.net.URLClassLoader$1.run(URLClassLoader.java:355) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(URLClassLoader.java:354) at java.lang.ClassLoader.loadClass(ClassLoader.java:425) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308) at java.lang.ClassLoader.loadClass(ClassLoader.java:358) at java.lang.Class.forName0(Native Method) at java.lang.Class.forName(Class.java:270) at org.apache.hadoop.conf.Configuration.getClassByNameOrNull(Configuration.java:2138) at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2103) at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2197) at org.apache.hadoop.mapreduce.task.JobContextImpl.getInputFormatClass(JobContextImpl.java:184) at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:749) at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341) at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:415) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1796) at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158) google 一番后,发现如下解决方案 123456789101112I can still confirm this issue with kylin version 2.0 on cloudera CDH 5.11. I tried the exports (setting them to the parcel directory), but the error still appeared.Adding kylin.job.mr.lib.dir=/opt/cloudera/parcels/CDH/lib/sentry/lib to the kylin.properties helped though.Thanks for the workaround and helpSince 2.0, another workaround other than the "kylin.engine.mr.lib-dir" is set the below env vars.export HADOOP_CONF_DIR=/etc/hadoop/confexport HIVE_LIB=/usr/lib/hiveexport HIVE_CONF=/etc/hive/confexport HCAT_HOME=/usr/lib/hive-hcatalogThen "bin/find-hive-dependencies.sh" should pick up the right hive jars. 最后在 kylin.properties 中添加 kylin.job.mr.lib.dir=/opt/cloudera/parcels/CDH/lib/sentry/lib 解决]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Kylin</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Centos虚拟机克隆后的ip、mac、uuid 修改]]></title>
    <url>%2F2017%2F05%2F19%2FCentos%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%85%8B%E9%9A%86%E5%90%8E%E7%9A%84%20ip%E3%80%81mac%20%E4%BF%AE%E6%94%B9%2F</url>
    <content type="text"><![CDATA[方法一：修改ip,mac地址（HWADDR），删除UUID项1234567891011vi /etc/sysconfig/network-scripts/ifcfg-eth0DEVICE=eth0 TYPE=EthernetONBOOT=yesNM_CONTROLLED=yesBOOTPROTO=staticIPADDR=10.1.18.212DNS1=your dnsDNS2=8.8.8.8GATEWAY=10.1.18.254 删除旧网卡vi /etc/udev/rules.d/70-persistent-net.rules可以看到会有2张PCI device网卡，删除eth0那行，再把eth1的那行里的”eh1”改成”eth0”。 方法二造成这样的原因，是因为在虚拟机（Vmware）中移动了Centos系统对应的文件，导致重新配置时，网卡的MAC地址变了，输入ifconfig -a,找不到eth0 安装完一个centos虚拟机，又拷贝一份，开机后网卡无法正常启动，报错：Device eth0 does not seem to be present,delaying initialization 解决：12mv /etc/sysconfig/network-scripts/ifcfg-eth0 sysconfig/network-scripts/ifcfg-eth1 1vim sysconfig/network-scripts/ifcfg-eth1 修改DEVICE=”eth0”为DEVICE=”eth1”修改IP地址，删除mac地址（HWADDR），删除UUID项。然后重启启动网卡尝试下 方法三###获取HWaddr 步骤1.：vi /etc/udev/rules.d/70-persistent-net.rules 显示为： # PCI device 0x8086:0x100f (e1000) SUBSYSTEM==&quot;net&quot;, ACTION==&quot;add&quot;, DRIVERS==&quot;?*&quot;, ATTR{address}==&quot;00:0c:29:27:a7:ab&quot;, ATTR{type}==&quot;1&quot;, KERNEL==&quot;eth*&quot;, NAME=&quot;eth0&quot; # PCI device 0x8086:0x100f (e1000) SUBSYSTEM==&quot;net&quot;, ACTION==&quot;add&quot;, DRIVERS==&quot;?*&quot;, ATTR{address}==&quot;00:0c:29:73:84:7f&quot;, ATTR{type}==&quot;1&quot;, KERNEL==&quot;eth*&quot;, NAME=&quot;eth1&quot; 步骤2 ：vi /etc/sysconfig/network-scripts/ 如果里面是ifcfg-eth0: 那么将步骤1的NAME=&quot;eth1&quot;对应的ATTR{address}==&quot;00:0c:29:73:84:7f&quot;赋值给HWaddr； 反之，则是将ATTR{address}==&quot;00:0c:29:27:a7:ab&quot;赋值给HWaddr。 这样我们就得到了HWaddr的值 获取UUID在克隆时，会设置 在该位置设置的文件夹下会找到564d9b3a-a4b0-08bd-046f-af70789026cc.vmem.lck类似这样的文件夹，那么564d9b3a-a4b0-08bd-046f-af70789026cc就是所谓的UUID。 最后就是将ifcfg-eth0改成ifcfg-eth1或者相反。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo 更换电脑重新部署]]></title>
    <url>%2F2017%2F05%2F17%2FHexo%20%E6%9B%B4%E6%8D%A2%E7%94%B5%E8%84%91%E9%87%8D%E6%96%B0%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[拷贝原博客文件必须拷贝文件：├──_config.yml├── theme├── scaffolds #文章模板├── package.json #说明使用哪些包├── .gitignore #限定在提交的时候哪些文件可以忽略└── source （1）讨论下哪些文件是必须拷贝的：首先是之前自己修改的文件，像站点配置_config.yml，theme文件夹里面的主题，以及source里面自己写的博客文件，这些肯定要拷贝的。除此之外，还有三个文件需要有，就是scaffolds文件夹（文章的模板）、package.json（说明使用哪些包）和.gitignore（限定在提交的时候哪些文件可以忽略）。其实，这三个文件不是我们修改的，所以即使丢失了，也没有关系，我们可以建立一个新的文件夹，然后在里面执行hexo init，就会生成这三个文件，我们只需要将它们拷贝过来使用即可。总结：_config.yml，theme/，source/，scaffolds/，package.json，.gitignore，是需要拷贝的。 （2）再讨论下哪些文件是不必拷贝的，或者说可以删除的：首先是.git文件，无论是在站点根目录下，还是主题目录下的.git文件，都可以删掉。然后是文件夹node_modules（在用npm install会重新生成），public（这个在用hexo g时会重新生成），.deploy_git文件夹（在使用hexo d时也会重新生成），db.json文件。其实上面这些文件也就是是.gitignore文件里面记载的可以忽略的内容。总结：.git/，node_modules/，public/，.deploy_git/，db.json文件需要删除。 环境部署安装 Git从官网Git下载git，在新电脑上安装，因为https速度慢，而且每次都要输入口令，常用的是使用ssh。使用下面方法创建： 打开git bash，设置用户名称和邮件地址 12$ git config --global user.name "username"$ git config --global user.email "username@example.com" 在用户主目录下运行：ssh-keygen -t rsa -C “youremail@example.com” 把其中的邮件地址换成自己的邮件地址，然后一路回车 最后完成后，会在用户主目录下生成.ssh目录，里面有id_rsa和id_rsa.pub两个文件，这两个就是SSH key密钥对，id_rsa是私钥，千万不能泄露出去，id_rsa.pub是公钥，可以放心地告诉任何人。 登陆GitHub，打开「Settings」-&gt;「SSH and GPG keys」，然后点击「new SSH key」，填上任意Title，在Key文本框里粘贴公钥id_rsa.pub文件的内容（千万不要粘贴成私钥了！），最后点击「Add SSH Key」，你就应该看到已经添加的Key。 安装Node.JS安装 Hexo打开git bash客户端，输入 npm install hexo-cli -g，开始安装hexo由于之前我是用 npm install hexo -g 安装的，此处我使用这个命令。因为我用 npm install hexo-cli -g命令安装后出现无法提交的情况，可能版本问题。 安装模块在git bash中切换目录到新拷贝的文件夹里，使用 npm install 命令，进行模块安装。 不要用hexo init初始化，部分文件已经拷贝生成，如果不慎使用，则站点配置文件_config.yml会被初始化为默认值 同步至Github12hexo ghexo d 其它组件123$ npm install npm install hexo-deployer-git --save #同步内容至github，若前面同步不成功就安装这个$ npm install hexo-generator-feed --save #RSS订阅$ npm install hexo-generator-sitemap --save #站点地图 参考 https://www.zhihu.com/question/21193762/answer/103097754http://puhemo.xyz/2016/06/03/hexo-change-pc/Hexo 官方文档]]></content>
      <categories>
        <category>Geek</category>
      </categories>
      <tags>
        <tag>博客</tag>
        <tag>Geek</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[「The Outline」It's for you]]></title>
    <url>%2F2017%2F05%2F17%2FThe%20Outline%2F</url>
    <content type="text"><![CDATA[https://theoutline.com/ 每日早上来实验室习惯性的会打开一些咨询类网站看看进来世界都发生了什么。今天无意间发现了这样一个网站，让我幸喜若狂。 以上是其官网的介绍。 The Outline 是一种给新人类的新型出版物。我们做这件事情是因为我们相信正确的故事以正确的方式告诉别人，能改变他的人生。但是以有意义和现代化的方式阐述正确的故事，这件事情本身不会发生。我们必须让其发生。 我们成立 The Outline 的初衷是把一些没有出现过的东西放到这个世界上，这让我们的非常激动。我们的报道集中在三个主题，这些主题越来越趋于以奇怪和重要的方式报道：权力（谁拥有它，谁想得到它，当他面得到时做了什么？），文化（我们生活和交流的方式），未来（我们下一步该往哪儿走？） 我们没有说任何这些故事，只因为这里有空间填补你的一天。我们正在诉说这些事情，是因为我们认为这个世界上还有一个值得叙述的事情是一个看不见的，没有报道的，或完全被错过的。 如果我们有一个主要的目标，就是尽可能地用尊敬和真实去喂饱你的好奇心与智慧。没有游戏，只有 TMD 有趣的每一天。 如果你真的想看看我们的事情，去探索我们的故事。并记得：迷路也是好的。 看完这些，且不管内容Ruhr，我已经被这个网站，深深吸引了。 WikiThe Outline (website)The Outline is a New York based digital news company focused on power (as it relates to politics and business), culture and the future. It was founded by Joshua Topolsky in 2016 who raised $5 million from several venture capitalists to start the company. The company does not want to be too reliant on social media distribution, but instead aims to reach a “smart, influential” readership who visit its website directly. The articles are visually interactive, and highly optimised for mobile.The interface contains articles represented as a stack of cards that users can swipe through. The company earns income by virtue of its partnerships with 10 to 12 companies a year, as opposed to reliance on a format employing traditional banner ads. HistoryThe Outline was founded in 2016 under a holding company named Independent Media with funding from RRE Ventures, Advancit Capital, Boat Rocker Ventures and Nextview Ventures. The company initially hired 10 employees and launched its website on December 5th, 2016. It currently has 26 employees, having recruited people from Vox Media, Vice and Buzzfeed.[7] In April 2017, The Outline introduced The Outline World Dispatch, a short daily podcast with news roundups.]]></content>
      <categories>
        <category>好奇心英语</category>
      </categories>
      <tags>
        <tag>猎奇</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[经典推荐算法之 Slope one]]></title>
    <url>%2F2017%2F05%2F16%2F%E7%BB%8F%E5%85%B8%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95%E4%B9%8B%20Slope%20one%20%2F</url>
    <content type="text"><![CDATA[Slope One 是一系列应用于协同过滤的算法的统称。由 Daniel Lemire和Anna Maclachlan于2005年发表的论文中提出。 有争议的是，该算法堪称基于项目评价的non-trivial 协同过滤算法最简洁的形式。该系列算法的简洁特性使它们的实现简单而高效，而且其精确度与其它复杂费时的算法相比也不相上下。 该系列算法也被用来改进其它算法。 协同过滤简介及其主要优缺点协同过滤推荐（Collaborative Filtering recommendation）在信息过滤和信息系统中正迅速成为一项很受欢迎的技术。与传统的基于内容过滤直接分析内容进行推荐不同，协同过滤分析用户兴趣，在用户群中找到指定用户的相似（兴趣）用户，综合这些相似用户对某一信息的评价，形成系统对该指定用户对此信息的喜好程度预测。 与传统文本过滤相比，协同过滤有下列优点: 能够过滤难以进行机器自动基于内容分析的信息。如艺术品、音乐。 能够基于一些复杂的，难以表达的概念（信息质量、品位)进行过滤。 推荐的新颖性。尽管协同过滤技术在个性化推荐系统中获得了极大的成功，但随着站点结构、内容的复杂度和用户人数的不断增加，协同过滤技术的一些缺点逐渐暴露出来。 主要有以下三点: 稀疏性(sparsity)：在许多推荐系统中，每个用户涉及的信息量相当有限，在一些大的系统如亚马逊网站中，用户最多不过就评估了上百万本书的1%~2%。造成评估矩阵数据相当稀疏，难以找到相似用户集，导致推荐效果大大降低。 扩展性(scalability)：“最近邻居”算法的计算量随着用户和项的增加而大大增加，对于上百万之巨的数目，通常的算法将遭遇到严重的扩展性问题。 精确性(accuracy)：通过寻找相近用户来产生推荐集，在数量较大的情况下，推荐的可信度随之降低。 Item-based协同过滤 和 过拟合当可以对一些项目评分的时候，比如人们可以对一些东西给出1到5星的评价的时候，协同过滤意图基于一个个体过去对某些项目的评分和（庞大的）由其他用户的评价构成的数据库，来预测该用户对未评价项目的评分。 例如: 如果一个人给披头士的评分为5（总分5）的话，我们能否预测他对席琳狄翁新专辑的评分呢？ 这种情形下, item-based 协同过滤系统根据其它项目的评分来预测某项目的分值，一般方法为 线性回归 (). 于是，需要列出x^2个线性回归方程和2x^2个回归量，例如：当有1000个项目时，需要列多达1,000,000个线性回归方程， 以及多达2,000,000个回归量。除非我们只选择某些用户共同评价过的项目对，否则协同过滤会遇到过拟合问题。 另外一种更好的方法是使用更简单一些的式子，比如 实验证明当使用一半的回归量的时候，该式子（称为Slope One）的表现有时优于线性回归方程。该简化方法也不需要那么多存储空间和延迟。 Item-based 协同过滤只是协同过滤的一种形式.其它还有像 user-based 协同过滤一样研究用户间的联系的过滤系统。但是，考虑到其他用户数量庞大，item-based协同过滤更可行一些。 电子商务中的Item-based协同过滤人们并不总是能给出评分，当用户只提供二进制数据（购买与否）的时候，就无法应用Slope One 和其它基于评分的算法。 二进制 item-based协同过滤应用的例子之一就是Amazon的 item-to-item 专利算法，该算法中用二进制向量表示用户-项目购买关系的矩阵，并计算二进制向量间的cosine相关系数。 有人认为Item-to-Item 算法甚至比Slope One 还简单，例如： 在本例当中，项目1和项目2间的cosine相关系数为： 项目1和项目3间的cosine相关系数为： 而项目2和项目3的cosine相关系数为： 于是，浏览项目1的顾客会被推荐买项目3(两者相关系数最大),而浏览项目2的顾客会被推荐买项目3,浏览了项目3的会首先被推荐买项目1（再然后是项目2,因为2和3的相关系数小于1和3）。该模型只使用了每对项目间的一个参数（cosine相关系数）来产生推荐。因此，如果有n个项目，则需要计算和存储 n（n-1）/2 个cosine相关系数。 Slope One 协同过滤为了大大减少过适(过拟合)的发生，提升算法简化实现， Slope One 系列易实现的Item-based协同过滤算法被提了出来。本质上，该方法运用更简单形式的回归表达式 和单一的自由参数，而不是一个项目评分和另一个项目评分间的线性回归 。 该自由参数只不过就是两个项目评分间的平均差值。甚至在某些实例当中，它比线性回归的方法更准确[2]，而且该算法只需要一半（甚至更少）的存储量。 例: User A 对 Item I 评分为1 对Item J.评分为1.5 User B 对 Item I 评分为2. 你认为 User B 会给 Item J 打几分? Slope One 的答案是：2.5 (1.5-1+2=2.5). 举个更实际的例子，考虑下表： 在本例中，项目2和1之间的平均评分差值为 (2+(-1))/2=0.5. 因此，item1的评分平均比item2高0.5。同样的，项目3和1之间的平均评分差值为3。因此，如果我们试图根据Lucy 对项目2的评分来预测她对项目1的评分的时候，我们可以得到 2+0.5 = 2.5。同样，如果我们想要根据她对项目3的评分来预测她对项目1的评分的话，我们得到 5+3=8. 如果一个用户已经评价了一些项目，可以这样做出预测：简单地把各个项目的预测通过加权平均值结合起来。当用户两个项目都评价过的时候，权值就高。在上面的例子中，项目1和项目2都评价了的用户数为2,项目1和项目3 都评价了的用户数为1,因此权重分别为2和1. 我们可以这样预测Lucy对项目1的评价：于是，对“n”个项目，想要实现 Slope One，只需要计算并存储“n”对评分间的平均差值和评价数目即可。 步骤计算物品之间的评分差的均值，记为物品间的评分偏差(两物品同时被评分) 根据物品间的评分偏差和用户的历史评分，预测用户对未评分的物品的评分。 将预测评分排序，取topN对应的物品推荐给用户。举例 假设有100个人对物品A和物品B打分了，R(AB)表示这100个人对A和B打分的平均偏差;有1000个人对物品B和物品C打分了， R(CB)表示这1000个人对C和B打分的平均偏差； 应用Slope One的推荐系统 ● hitflip DVD推荐系统 ● How Happy ● inDiscover MP3推荐系统 ● RACOFI Composer ● Value Investing News 股票新闻网站 ● AllTheBests 购物推荐引擎 Python 实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667def loadData(): items=&#123;'A':&#123;1:5,2:3&#125;, 'B':&#123;1:3,2:4,3:2&#125;, 'C':&#123;1:2,3:5&#125;&#125; users=&#123;1:&#123;'A':5,'B':3,'C':2&#125;, 2:&#123;'A':3,'B':4&#125;, 3:&#123;'B':2,'C':5&#125;&#125; return items,users#***计算物品之间的评分差#items:从物品角度，考虑评分#users:从用户角度，考虑评分def buildAverageDiffs(items,users,averages): #遍历每条物品-用户评分数据 for itemId in items: for otherItemId in items: average=0.0 #物品间的评分偏差均值 userRatingPairCount=0 #两件物品均评过分的用户数 if itemId!=otherItemId: #若无不同的物品项 for userId in users: #遍历用户-物品评分数 userRatings=users[userId] #每条数据为用户对物品的评分 #当前物品项在用户的评分数据中，且用户也对其他物品由评分 if itemId in userRatings and otherItemId in userRatings: #两件物品均评过分的用户数加1 userRatingPairCount+=1 #评分偏差为每项当前物品评分-其他物品评分求和 average+=(userRatings[otherItemId]-userRatings[itemId]) averages[(itemId,otherItemId)]=average/userRatingPairCount#***预测评分#users:用户对物品的评分数据#items：物品由哪些用户评分的数据#averages：计算的评分偏差#targetUserId：被推荐的用户#targetItemId：被推荐的物品def suggestedRating(users,items,averages,targetUserId,targetItemId): runningRatingCount=0 #预测评分的分母 weightedRatingTotal=0.0 #分子 for i in users[targetUserId]: #物品i和物品targetItemId共同评分的用户数 ratingCount=userWhoRatedBoth(users,i,targetItemId) #分子 weightedRatingTotal+=(users[targetUserId][i]-averages[(targetItemId,i)])\ *ratingCount #分母 runningRatingCount+=ratingCount #返回预测评分 return weightedRatingTotal/runningRatingCount# 物品itemId1与itemId2共同有多少用户评分def userWhoRatedBoth(users,itemId1,itemId2): count=0 #用户-物品评分数据 for userId in users: #用户对物品itemId1与itemId2都评过分则计数加1 if itemId1 in users[userId] and itemId2 in users[userId]: count+=1 return countif __name__=='__main__': items,users=loadData() averages=&#123;&#125; #计算物品之间的评分差 buildAverageDiffs(items,users,averages) #预测评分:用户2对物品C的评分 predictRating=suggestedRating(users,items,averages,2,'C') print 'Guess the user will rate the score :',predictRating 12结果：用户2对物品C的预测分值为 Guess the user will rate the score : 3.33333333333 Slop one 增量更新在于根据新的评分项，更新偏差表与共同评分项个数 维基百科推荐算法之 slope one 算法黄明波. 基于Slope One算法的增量音乐推荐系统的设计与实现[D].重庆大学,2016]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>推荐系统</tag>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu16.04下设置静态IP]]></title>
    <url>%2F2017%2F05%2F09%2FUbuntu16.04%E4%B8%8B%E8%AE%BE%E7%BD%AE%E9%9D%99%E6%80%81IP%2F</url>
    <content type="text"><![CDATA[vi /etc/network/interfaces123456789101112131415161718# This file describes the network interfaces available on your system# and how to activate them. For more information, see interfaces(5).source /etc/network/interfaces.d/*# The loopback network interfaceauto loiface lo inet loopback# The primary network interfaceauto ens160 #设置自动启动ens160接口#iface ens160 inet dhcp iface ens160 inet static #配置静态IPaddress 10.1.18.200 #IP地址netmask 255.255.255.0 #子网掩码gateway 10.1.18.254 #默认网关dns-nameserver 210.32.32.10 dns-nameserver 210.32.32.10 这句一定需要有，因为以前是DHCP解析，所以会自动分配DNS 服务器地址。而一旦设置为静态IP后就没有自动获取到DNS服务器了 设置完重启电脑/etc/resolv.conf 文件中会自动添加1nameserver 210.32.32.10 (或者nameserver 8.8.8.8)可以根据访问速度，选择合适的公共DNS]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[KNN (k-nearest neighbor classification)]]></title>
    <url>%2F2017%2F03%2F16%2FKNN%2F</url>
    <content type="text"><![CDATA[K-近邻算法（KNN）概述最简单最初级的分类器是将全部的训练数据所对应的类别都记录下来，当测试对象的属性和某个训练对象的属性完全匹配时，便可以对其进行分类。但是怎么可能所有测试对象都会找到与之完全匹配的训练对象呢，其次就是存在一个测试对象同时与多个训练对象匹配，导致一个训练对象被分到了多个类的问题，基于这些问题呢，就产生了KNN。 KNN是通过测量不同特征值之间的距离进行分类。它的的思路是：如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别。K通常是不大于20的整数。KNN算法中，所选择的邻居都是已经正确分类的对象。该方法在定类决策上只依据最邻近的一个或者几个样本的类别来决定待分样本所属的类别。 下面通过一个简单的例子说明一下：如下图，绿色圆要被决定赋予哪个类，是红色三角形还是蓝色四方形？如果K=3，由于红色三角形所占比例为2/3，绿色圆将被赋予红色三角形那个类，如果K=5，由于蓝色四方形比例为3/5，因此绿色圆被赋予蓝色四方形类。 由此也说明了KNN算法的结果很大程度取决于K的选择。 在KNN中，通过计算对象间距离来作为各个对象之间的非相似性指标，避免了对象之间的匹配问题，在这里距离一般使用欧氏距离或曼哈顿距离： 同时，KNN通过依据k个对象中占优的类别进行决策，而不是单一的对象类别决策。这两点就是KNN算法的优势。 接下来对KNN算法的思想总结一下：就是在训练集中数据和标签已知的情况下，输入测试数据，将测试数据的特征与训练集中对应的特征进行相互比较，找到训练集中与之最为相似的前K个数据，则该测试数据对应的类别就是K个数据中出现次数最多的那个分类，其算法的描述为： 1）计算测试数据与各个训练数据之间的距离； 2）按照距离的递增关系进行排序； 3）选取距离最小的K个点； 4）确定前K个点所在类别的出现频率； 5）返回前K个点中出现频率最高的类别作为测试数据的预测分类。 优缺点1、优点 简单，易于理解，易于实现，无需估计参数，无需训练 适合对稀有事件进行分类（例如当流失率很低时，比如低于0.5%，构造流失预测模型） 特别适合于多分类问题(multi-modal,对象具有多个类别标签)，例如根据基因特征来判断其功能分类，kNN比SVM的表现要好 2、缺点 懒惰算法，对测试样本分类时的计算量大，内存开销大，评分慢 可解释性较差，无法给出决策树那样的规则。 常见问题1、k值设定为多大？ k太小，分类结果易受噪声点影响；k太大，近邻中又可能包含太多的其它类别的点。（对距离加权，可以降低k值设定的影响） k值通常是采用交叉检验来确定（以k=1为基准） 经验规则：k一般低于训练样本数的平方根 2、类别如何判定最合适？ 投票法没有考虑近邻的距离的远近，距离更近的近邻也许更应该决定最终的分类，所以加权投票法更恰当一些。 3、如何选择合适的距离衡量？ 高维度对距离衡量的影响：众所周知当变量数越多，欧式距离的区分能力就越差。 变量值域对距离的影响：值域越大的变量常常会在距离计算中占据主导作用，因此应先对变量进行标准化。 4、训练样本是否要一视同仁？ 在训练集中，有些样本可能是更值得依赖的。 可以给不同的样本施加不同的权重，加强依赖样本的权重，降低不可信赖样本的影响。 5、性能问题？ kNN是一种懒惰算法，平时不好好学习，考试（对测试样本分类）时才临阵磨枪（临时去找k个近邻）。 懒惰的后果：构造模型很简单，但在对测试样本分类地的系统开销大，因为要扫描全部训练样本并计算距离。 已经有一些方法提高计算的效率，例如压缩训练样本量等。 6、能否大幅减少训练样本量，同时又保持分类精度？ 浓缩技术(condensing) 编辑技术(editing) 使用 sklearn 进行算法使用对地形进行分类 knn.py 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061#!/usr/bin/pythonimport matplotlib.pyplot as pltfrom prep_terrain_data import makeTerrainDatafrom class_vis import prettyPicturefrom time import timefeatures_train, labels_train, features_test, labels_test = makeTerrainData()### the training data (features_train, labels_train) have both "fast" and "slow"### points mixed together--separate them so we can give them different colors### in the scatterplot and identify them visuallygrade_fast = [features_train[ii][0] for ii in range(0, len(features_train)) if labels_train[ii]==0]bumpy_fast = [features_train[ii][1] for ii in range(0, len(features_train)) if labels_train[ii]==0]grade_slow = [features_train[ii][0] for ii in range(0, len(features_train)) if labels_train[ii]==1]bumpy_slow = [features_train[ii][1] for ii in range(0, len(features_train)) if labels_train[ii]==1]#### initial visualizationplt.xlim(0.0, 1.0)plt.ylim(0.0, 1.0)plt.scatter(bumpy_fast, grade_fast, color = "b", label="fast")plt.scatter(grade_slow, bumpy_slow, color = "r", label="slow")plt.legend()plt.xlabel("bumpiness")plt.ylabel("grade")plt.show()################################################################################### your code here! name your classifier object clf if you want the ### visualization code (prettyPicture) to show you the decision boundaryfrom sklearn.neighbors import KNeighborsClassifierclf = KNeighborsClassifier(n_neighbors=10,weights='distance')t0 = time()clf.fit(features_train,labels_train)print "training time : " ,round(time()-t0,3),"s"t1 = time()pred = clf.predict(features_test)print "predicting time : " ,round(time()-t1,3),"s"from sklearn.metrics import accuracy_scoreacc = accuracy_score(pred,labels_test)print 'accuracy_score :',acctry: prettyPicture(clf, features_test, labels_test)except NameError: pass prep_terrain_data.py 123456789101112131415161718192021222324252627282930313233343536373839404142#!/usr/bin/pythonimport randomdef makeTerrainData(n_points=1000):################################################################################## make the toy dataset random.seed(42) grade = [random.random() for ii in range(0,n_points)] bumpy = [random.random() for ii in range(0,n_points)] error = [random.random() for ii in range(0,n_points)] y = [round(grade[ii]*bumpy[ii]+0.3+0.1*error[ii]) for ii in range(0,n_points)] for ii in range(0, len(y)): if grade[ii]&gt;0.8 or bumpy[ii]&gt;0.8: y[ii] = 1.0### split into train/test sets X = [[gg, ss] for gg, ss in zip(grade, bumpy)] split = int(0.75*n_points) X_train = X[0:split] X_test = X[split:] y_train = y[0:split] y_test = y[split:] grade_sig = [X_train[ii][0] for ii in range(0, len(X_train)) if y_train[ii]==0] bumpy_sig = [X_train[ii][1] for ii in range(0, len(X_train)) if y_train[ii]==0] grade_bkg = [X_train[ii][0] for ii in range(0, len(X_train)) if y_train[ii]==1] bumpy_bkg = [X_train[ii][1] for ii in range(0, len(X_train)) if y_train[ii]==1] training_data = &#123;"fast":&#123;"grade":grade_sig, "bumpiness":bumpy_sig&#125; , "slow":&#123;"grade":grade_bkg, "bumpiness":bumpy_bkg&#125;&#125; grade_sig = [X_test[ii][0] for ii in range(0, len(X_test)) if y_test[ii]==0] bumpy_sig = [X_test[ii][1] for ii in range(0, len(X_test)) if y_test[ii]==0] grade_bkg = [X_test[ii][0] for ii in range(0, len(X_test)) if y_test[ii]==1] bumpy_bkg = [X_test[ii][1] for ii in range(0, len(X_test)) if y_test[ii]==1] test_data = &#123;"fast":&#123;"grade":grade_sig, "bumpiness":bumpy_sig&#125; , "slow":&#123;"grade":grade_bkg, "bumpiness":bumpy_bkg&#125;&#125; return X_train, y_train, X_test, y_test 训练数据分布图 测试数据分类结果及决策面 计算结果 123training time : 0.002 spredicting time : 0.005 saccuracy_score : 0.94 参考转载 http://www.cnblogs.com/ybjourney/p/4702562.htmlhttp://blog.csdn.net/lx85416281/article/details/40656877]]></content>
      <categories>
        <category>Machine learning</category>
      </categories>
      <tags>
        <tag>Machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[支持向量机 Support Vector Machine]]></title>
    <url>%2F2017%2F03%2F13%2FSupport%20Vector%20Machine%2F</url>
    <content type="text"><![CDATA[支持向量机（SVM）是90年代中期发展起来的基于统计学习理论的一种机器学习方法，通过寻求结构化风险最小来提高学习机泛化能力，实现经验风险和置信范围的最小化，从而达到在统计样本量较少的情况下，亦能获得良好统计规律的目的。 通俗来讲，它是一种二类分类模型，其基本模型定义为特征空间上的间隔最大的线性分类器，即支持向量机的学习策略便是间隔最大化，最终可转化为一个凸二次规划问题的求解。 线性分割 kernel trick 核技巧，多特征，将x,y映射到多维空间进行特征分割，之后再返回二维空间形成非线性分割线 使用 sklearn 实战 ###调参 在 rbf 核下，分别使用10、100、1000、10000的C参数进行调整输出准确率。使用1%的数据集。 C参数： 低C使决策表面平滑，而高C旨在通过给予模型自由选择更多样本作为支持向量来正确地分类所有训练样本。 12features_train = features_train[:len(features_train)/100]labels_train = labels_train[:len(labels_train)/100] 结果如下1234C=10.0 accuracy=0.616040955631C=100.0 accuracy=0.616040955631C=1000. accuracy=0.821387940842C=10000. accuracy=0.892491467577 结果计算在全数据集下，进行准确度计算，并计算预测为Chris邮件的个数 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859#!/usr/bin/python""" This is the code to accompany the Lesson 2 (SVM) mini-project. Use a SVM to identify emails from the Enron corpus by their authors: Sara has label 0 Chris has label 1""" import sysfrom time import timesys.path.append("../tools/")from email_preprocess import preprocess### features_train and features_test are the features for the training### and testing datasets, respectively### labels_train and labels_test are the corresponding item labelsfeatures_train, features_test, labels_train, labels_test = preprocess()# reduce training data#features_train = features_train[:len(features_train)/100]#labels_train = labels_train[:len(labels_train)/100]############################################################ your code goes here ###from sklearn.svm import SVCclf = SVC(kernel='rbf',C=10000.)#### now your job is to fit the classifier#### using the training features/labels, and to#### make a set of predictions on the test datat0 = time()clf.fit(features_train,labels_train)print "training time : " ,round(time()-t0,3),"s"#### store your predictions in a list named predt1 = time()pred = clf.predict(features_test)print "predicting time : " ,round(time()-t1,3),"s"from sklearn.metrics import accuracy_scoreacc = accuracy_score(pred, labels_test)print "accuracy: ", acc# answer1=pred[10]# answer2=pred[26]# answer3=pred[50]num = 0for n in pred: if n == 1: num = num + 1print "total Chris(1): ", num######################################################### 结果如下123456no. of Chris training emails: 7936no. of Sara training emails: 7884training time : 159.096 spredicting time : 19.393 saccuracy: 0.990898748578total Chris(1): 877]]></content>
      <categories>
        <category>Machine learning</category>
      </categories>
      <tags>
        <tag>Machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[朴素贝叶斯 naive bayes]]></title>
    <url>%2F2017%2F03%2F09%2F%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%20naive%20bayes%2F</url>
    <content type="text"><![CDATA[背景案例几年前，J.K. 罗琳（凭借《哈利波特》出名）试着做了件有趣的事。她以 Robert Galbraith 的化名写了本名叫《The Cuckoo’s Calling》的书。尽管该书得到一些不错的评论，但是大家都不太重视它，直到 Twitter 上一个匿名的知情人士说那是 J.K. Rowling 写的。《伦敦周日泰晤士报》找来两名专家对《杜鹃在呼唤》和 Rowling 的《偶发空缺》以及其他几名作者的书进行了比较。分析结果强有力地指出罗琳就是作者，《泰晤士报》直接询问出版商情况是否属实，而出版商也证实了这一说法，该书在此后一夜成名。 这就是一个文本分类预测的例子，接下来我们看看朴素贝叶斯是怎么做的。 贝叶斯定理贝叶斯分类是一类分类算法的总称，这类算法均以贝叶斯定理为基础，故统称为贝叶斯分类。这个定理解决了现实生活里经常遇到的问题：已知某条件概率，如何得到两个事件交换后的概率，也就是在已知P(A|B)的情况下如何求得P(B|A)。 条件概率： 贝叶斯定理之所以有用，是因为我们在生活中经常遇到这种情况：我们可以很容易直接得出P(A|B)，P(B|A)则很难直接得出，但我们更关心P(B|A)，贝叶斯定理就为我们打通从P(A|B)获得P(B|A)的道路。 贝叶斯定理： 朴素贝叶斯朴素贝叶斯之所以朴素，是因为其思想很朴素，英文 naive …..（不禁想到某位长者）。在文本分类上，其忽略了词序，记录词频。朴素贝叶斯的思想基础是这样的：对于给出的待分类项，求解在此项出现的条件下各个类别出现的概率，哪个最大，就认为此待分类项属于哪个类别。 优点： 易执行 特征空间大 有效 缺点： 无语义分析 朴素贝叶斯分类的正式定义如下： 设 为一个待分类项，而每个a为x的一个特征属性。 有类别集合 。 计算 。 如果 ，则 。 那么现在的关键就是如何计算第3步中的各个条件概率。我们可以这么做： 找到一个已知分类的待分类项集合，这个集合叫做训练样本集。 统计得到在各类别下各个特征属性的条件概率估计。即 如果各个特征属性是条件独立的，则根据贝叶斯定理有如下推导： 因为分母对于所有类别为常数，因为我们只要将分子最大化皆可。又因为各特征属性是条件独立的，所以有： 流程 第一阶段——准备工作阶段，这个阶段的任务是为朴素贝叶斯分类做必要的准备，主要工作是根据具体情况确定特征属性，并对每个特征属性进行适当划分，然后由人工对一部分待分类项进行分类，形成训练样本集合。这一阶段的输入是所有待分类数据，输出是特征属性和训练样本。这一阶段是整个朴素贝叶斯分类中唯一需要人工完成的阶段，其质量对整个过程将有重要影响，分类器的质量很大程度上由特征属性、特征属性划分及训练样本质量决定。 第二阶段——分类器训练阶段，这个阶段的任务就是生成分类器，主要工作是计算每个类别在训练样本中的出现频率及每个特征属性划分对每个类别的条件概率估计，并将结果记录。其输入是特征属性和训练样本，输出是分类器。这一阶段是机械性阶段，根据前面讨论的公式可以由程序自动计算完成。 第三阶段——应用阶段。这个阶段的任务是使用分类器对待分类项进行分类，其输入是分类器和待分类项，输出是待分类项与类别的映射关系。这一阶段也是机械性阶段，由程序完成。 Example病人分类的例子让我从一个例子开始讲起，你会看到贝叶斯分类器很好懂，一点都不难。某个医院早上收了六个门诊病人，如下表。 1234567症状 职业 疾病 打喷嚏 护士 感冒 打喷嚏 农夫 过敏 头痛 建筑工人 脑震荡 头痛 建筑工人 感冒 打喷嚏 教师 感冒 头痛 教师 脑震荡 现在又来了第七个病人，是一个打喷嚏的建筑工人。请问他患上感冒的概率有多大？ 根据贝叶斯定理：P(A|B) = P(B|A) P(A) / P(B) 可得123P(感冒|打喷嚏x建筑工人) = P(打喷嚏x建筑工人|感冒) x P(感冒) / P(打喷嚏x建筑工人) 假定”打喷嚏”和”建筑工人”这两个特征是独立的，因此，上面的等式就变成了 123P(感冒|打喷嚏x建筑工人) = P(打喷嚏|感冒) x P(建筑工人|感冒) x P(感冒) / P(打喷嚏) x P(建筑工人) 这是可以计算的 123P(感冒|打喷嚏x建筑工人) = 0.66 x 0.33 x 0.5 / 0.5 x 0.33 = 0.66 因此，这个打喷嚏的建筑工人，有66%的概率是得了感冒。同理，可以计算这个病人患上过敏或脑震荡的概率。比较这几个概率，就可以知道他最可能得什么病。这就是贝叶斯分类器的基本方法：在统计资料的基础上，依据某些特征，计算各个类别的概率，从而实现分类。 实战我们有一组邮件，分别由同一家公司的两个人撰写其中半数的邮件。我们的目标是仅根据邮件正文区分每个人写的邮件。 先给你一个字符串列表。每个字符串代表一封经过预处理的邮件的正文；提供代码，用来将数据集分解为训练集和测试集。 朴素贝叶斯特殊的一点在于，这种算法非常适合文本分类。在处理文本时，常见的做法是将每个单词看作一个特征，这样就会有大量的特征。此算法的相对简单性和朴素贝叶斯独立特征的这一假设，使其能够出色完成文本的分类。此项目使用 python 的 sklearn包，然后使用朴素贝叶斯根据作者对邮件进行分类。 123456789101112131415161718192021222324252627282930313233#!/usr/bin/python import sysfrom time import timesys.path.append("../tools/")from email_preprocess import preprocessfrom sklearn.metrics import accuracy_score### features_train and features_test are the features for the training### and testing datasets, respectively### labels_train and labels_test are the corresponding item labelsfeatures_train, features_test, labels_train, labels_test = preprocess()############################################################ main code ###from sklearn.naive_bayes import GaussianNB# 创建分类器clf = GaussianNB()t0 = time()clf.fit(features_train, labels_train)print "training time:", round(time()-t0, 3), "s"t1 = time()# 进行预测pred = clf.predict(features_test)print "predicting time:", round(time()-t1, 3), "s"# 输出预测准确率print accuracy_score(pred,labels_test)######################################################### 结果123456no. of Chris training emails: 7936no. of Sara training emails: 7884training time: 2.396 spredicting time: 0.464 saccuracy: 0.973265073948 参考文章 算法杂货铺——分类算法之朴素贝叶斯分类(Naive Bayesian classification))朴素贝叶斯分类器的应用Intro to machine learning]]></content>
      <categories>
        <category>Machine learning</category>
      </categories>
      <tags>
        <tag>Machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[有监督学习和无监督学习]]></title>
    <url>%2F2017%2F03%2F06%2F%E6%9C%89%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E5%92%8C%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[有监督学习 supervised learning对具有概念标记（分类）的训练样本进行学习，以尽可能对训练样本集外的数据进行标记（分类）预测。这里，所有的标记（分类）是已知的。因此，训练样本的岐义性低。监督学习的典型例子就是决策树、神经网络以及疾病监测. 监督学习是训练神经网络和决策树的最常见技术。这两种技术（神经网络和决策树）高度依赖于事先确定的分类系统给出的信息。对于神经网络来说，分类系统用于判断网络的错误，然后调整网络适应它；对于决策树，分类系统用来判断哪些属性提供了最多的信息，如此一来可以用它解决分类系统的问题。我们将会看到这两者（神经网络和决策树）更多的细节，但在目前，它们用预先确定分类方法的形式来“监督”就足够了。 无监督学习 unsupervised learning对没有概念标记（分类）的训练样本进行学习，以发现训练样本集中的结构性知识。这里，所有的标记（分类）是未知的。因此，训练样本的岐义性高。聚类就是典型的无监督学习. 在这方面一个突出的例子是Backgammon（西洋双陆棋）游戏，有一系列计算机程序（例如neuro-gammon和TD-gammon）通过非监督学习自己一遍又一遍的玩这个游戏，变得比最强的人类棋手还要出色。这些程序发现的一些原则甚至令双陆棋专家都感到惊讶，并且它们比那些使用预分类样本训练的双陆棋程序工作得更出色。 一种次要的非监督学习类型称之为聚类（原文为clustering）。这类学习类型的目标不是让效用函数最大化，而是找到训练数据中的近似点。聚类常常能发现那些与假设匹配的相当好的直观分类。例如，基于人口统计的聚合个体可能会在一个群体中形成一个富有的聚类，以及其他的贫穷的聚合。 举例本例来自，Udacity 的 Intro to Machine Learning 课程 从一个加了标签的相册中找出某个人。 分析银行诈骗交易。 其中并没有给出异常交易的明确定义，没有例子来说明，所以属于无监督学习。 通过某人的音乐选择及标签特征，推荐音乐。 通过学习风格将优达学城的学生分类。 学生类型不是已知的，不属于监督学习。]]></content>
      <categories>
        <category>Machine learning</category>
      </categories>
      <tags>
        <tag>Machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[「China Daily」Online fantasy novel hits the small screen]]></title>
    <url>%2F2017%2F02%2F24%2FOnline%20fantasy%20novel%20hits%20the%20small%20screen%2F</url>
    <content type="text"><![CDATA[http://www.chinadaily.com.cn/culture/2017-02/17/content_28244549.htm#Content A Ten Miles of Peach Blossom poster [Photo provided to chinadaily.com.cn] News textAdapted from a popular online novel of the same name, a TV series called Ten Miles of Peach Blossom has hit the small screen during the Spring Festival. Set in a fantasy world where monsters, gods and humans coexist, the story tells a love story between a 140,000-year-old fox princess and a 50,000-year-old dragon prince. The story has won many fans for its beautiful scenes, poetic dialogues and popular stars. The most important reason for turning the online novel into the TV series is the strong demand from fans. The novel was published by Shenyang Press in 2009 and has sold 1.1 million copies. According to an industry insider, the online popularity and potential for market earnings from the online novels could propel the TV series or film to be the next blockbuster. Here are some beautiful scenes from the TV series. Words: hit the small screen : 在电视上热映， hit the screen 在这里指“搬上荧幕”，big screen为电影 TV series：电视剧 propel: 推进；驱使；激励；驱策 blockbuster：轰动；巨型炸弹；一鸣惊人者 Translation根据同名流行的网络小说改编的电视连续剧十里桃花，在春节期间火热播出。 怪兽，神和人类共存在一个虚幻的世界中，其讲述了一个关于14万岁的狐狸公主和5万岁的龙王子的爱情故事。 美丽的场景，诗意的对话和明星给故事赢得了很多粉丝。 把网络小说拍成电视剧的主要原因是粉丝的强烈要求。 小说于2009年在沈阳出版社出版，并且已经售出110万本。 据业内人士透露，在线小说的在线流行度和市场收益的潜力可能推动电视剧或电影成为下一个大片。 Thought]]></content>
      <categories>
        <category>好奇心英语</category>
      </categories>
      <tags>
        <tag>ChinaDaily</tag>
        <tag>News</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java读取properties配置文件]]></title>
    <url>%2F2017%2F02%2F20%2Fjava%E8%AF%BB%E5%8F%96properties%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[读取.properties配置文件在实际的开发中使用的很多，总结了一下，有以下几种方法（仅仅是我知道的）： 通过jdk提供的java.util.Properties类此类继承自java.util.HashTable，即实现了Map接口，所以，可使用相应的方法来操作属性文件，但不建议使用像put、putAll这两个方法，因为put方法不仅允许存入String类型的value，还可以存入Object类型的。因此java.util.Properties类提供了getProperty()和setProperty()方法来操作属性文件，同时使用store或save(已过时)来保存属性值（把属性值写入.properties配置文件）。在使用之前，还需要加载属性文件，它提供了两个方法：load和loadFromXML。load有两个方法的重载：load(InputStream inStream)、load(Reader reader)，所以，可根据不同的方式来加载属性文件。可根据不同的方式来获取InputStream，如： 通过当前类加载器的getResourceAsStream方法获取 1InputStream inStream = TestProperties.class.getClassLoader().getResourceAsStream("test.properties"); 从文件获取 1InputStream inStream = new FileInputStream(new File("filePath")); 也是通过类加载器来获取，和第一种一样 1InputStream in = ClassLoader.getSystemResourceAsStream("filePath"); 在servlet中，还可以通过context来获取InputStream 1InputStream in = context.getResourceAsStream("filePath"); 通过URL来获取 12URL url = new URL("path"); InputStream inStream = url.openStream(); 读取方法如下：1234Properties prop = new Properties(); prop.load(inStream); String key = prop.getProperty("username"); //String key = (String) prop.get("username"); 通过java.util.ResourceBundle类来读取这种方式比使用Properties要方便一些。 通过ResourceBundle.getBundle()静态方法来获取（ResourceBundle是一个抽象类），这种方式来获取properties属性文件不需要加.properties后缀名，只需要文件名即可。12ResourceBundle resource = ResourceBundle.getBundle("com/mmq/test");// test为属性文件名，放在包com.mmq下，如果是放在src下，直接用test即可 String key = resource.getString(“username”); 从InputStream中读取，获取InputStream的方法和上面一样，不再赘述。1ResourceBundle resource = new PropertyResourceBundle(inStream); 注意：在使用中遇到的最大的问题可能是配置文件的路径问题，如果配置文件入在当前类所在的包下，那么需要使用包名限定，如：test.properties入在com.mmq包下，则要使用com/mmq/test.properties（通过Properties来获取）或com/mmq/test（通过ResourceBundle来获取）；属性文件在src根目录下，则直接使用test.properties或test即可。 本文转载自 http://blog.csdn.net/mhmyqn/article/details/7683909]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[台湾清华大学彭明辉教授的研究生手册]]></title>
    <url>%2F2017%2F01%2F15%2F%E5%8F%B0%E6%B9%BE%E6%B8%85%E5%8D%8E%E5%A4%A7%E5%AD%A6%E5%BD%AD%E6%98%8E%E8%BE%89%E6%95%99%E6%8E%88%E7%9A%84%E7%A0%94%E7%A9%B6%E7%94%9F%E6%89%8B%E5%86%8C%2F</url>
    <content type="text"><![CDATA[一、论文的要求 我对硕士论文的基本要求是： （1）论文的主要内容，是叙述一套方法在一个特定场合中的应用。 （2）这套方法必须要有所创新或突破，并因而对学术界有所贡献。因此，它或者是解决既有问题的新方法，或者是既有方法的新应用，或者是以一个新的方法开启一整片新的应用领域。 （3）在论文中，你必须要有能力提出足够的证据来让读者信服说：针对这个应用场合，你所提出来的方法确实有比文献中一切既有方法更优越之处。 （4）此外，你必须要能清楚指出这个方法在应用上的限制，并且提出充分证据来说服读者：任何应用场合，只要能够满足你所提出来的假设（前提）条件，你的方法就一定适用，而且你所描述的优点就一定会存在。 （5）你还必须要在论文中清楚指出这个方法的限制和可能的缺点（相对于其它文献上的既有方法，或者在其它应用场合里）假如这个方法有任何重大缺点，在口试时才被口试委员指出来，其后果有可能是论文无法通过。 （6）行文风格上，它是一篇论证严谨，逻辑关系清晰，而且结构有条理的专业论述。也就是说，在叙述你的方法的过程，你必须要清清楚楚地交代这个方法的应用程序以及所有仿真或实验结果的过程，使得这个专业领域内的任何读者，都有办法根据你的描述，在他的实验室下复制出你的研究成果，以便确定你的结论确实是可以「在任何时间、任何地点、任何人」都具有可重复性（可重复性是「科学」的根本要求）。 （7）而且，你对这个方法的每一个步骤都必须要提供充分的理由说明「为什么非如此不可」。 （ 8）最后，你的论文必须要在适当位置清楚注明所有和你所研究之题目相关的文献。而且，你必须要记得：只要是和你所研究的问题相关的学术文献（尤其是学术期刊论文），你都有必要全部找出来（如果漏掉就是你的过失），仔细读过。假如你在学位论文口试时，有口试委员指出有一篇既有文献，在你所讨论的问题中处理得比你的方法还好，这就构成你论文无法及格的充分理由。 （9）第（2）款所谓「对学术界的贡献」，指的是：把你的所有研究成果扣除掉学术界已经发表过的所有成果（不管你实际上有没有参考过，没有参考过也算是你的重大过失），剩下的就是你的贡献。假如这个贡献太少，也构成你论文无法及格的充分理由。 上面所叙述的九款要件中，除第（2）款之外，通通都是必须要做到的，因此没有好坏之分。一篇硕士论文的好坏（以及成绩的评定标准），主要是看第（2）款所谓「对学术界的贡献」的多寡与重要性而定。假如你要申请国外的博士班，最重要的也是看你的硕士论文有什么「贡献」而定（这往往比TOFEL、GRE、GPA还重要）。 一个判断硕士论文的好坏有一个粗浅办法：假如你的研究成果可以在国外著名学术期刊（journals，而非 magazines）上发表，通常就比一篇只能在国外学术会议（conferences）上发表的硕士论文贡献多；一篇国外学术会议的论文又通常比无法发表的论文贡献多；在国际顶尖学术期刊上发表的论文通常比一篇二流的学术期刊论文贡献多。SCI有一种叫做 Impact Factor 的指数，统计一个期刊每篇论文被引述的次数。通常这个次数（或指数）愈高，对学术界的影响力就愈大。以机械视觉相关领域的期刊而言，Impact Factor 在 1.0 以上的期刊，都算是顶尖的期刊。这些期刊论文的作者，通常是国外顶尖学府的著名教授指导全球一流的博士生做出来的研究成果。 二、完成硕士论文所需要的能力 从前面的叙述可以归纳出来，完成硕士论文所需要的能力包括以下数项，依它们的培养先后次序逐项讨论。 （1）资料检索的能力：在给定（或自己拟定）的题目范围内，你必须有能力利用文资料索引系统，查出所有相关的论文，而无任何遗漏（否则你可能在论文口试时才发现同一个题目已经有人发表过了）。你到底要用什么样的关键词和查所程序去保证你已经找出所有相关的文献？这是第一个大的挑战。每一组关键词（包含联集与交集）代表一个论文所构成的集合，假如你用的关键词不恰当，你可能找到的集合太小，没有涵盖所有的相关文献；假如你用的关键词太一般化（譬如「image」），通常你找到的集合会太大，除了所有相关文献之外还加上好几十倍的毫不相关的文献。 （2）资料筛选的能力：即使你使用了恰当的搜寻策略，通常找到的文献集合都还是明显地比你所需要的集合大，而且通常文献比数大概在一两百篇或数百篇之间，而其中会和你的的研究子题直接且密切相关的论文，通常只有廿、卅篇左右。你如何可以只读论文的题目、摘要、简介和结论，而还没有完全看懂内文，就准确地判断出这篇论文中是否有值得你进一步参考的内容，以便快速地把需要仔细读完的论文从数百篇降低到廿、卅篇？这考验着你从事资料筛选的能力。 （3）期刊论文的阅读能力：期刊论文和大学部的课本截然不同。大学部的课本是寻次渐进地从最基本的知识背景逐步交代出整套有系统的知识，中间没有任何的跳跃，只要你逐页读下去，就可以整本都读懂，不需要在去别的地方找参考资料。但是期刊论文是没头没尾的十几页文献，只交代最核心的创意，并援引许多其它论文的研究成果（但只注明文献出处，而完全没有交代其内容）。因此，要读懂一篇论文，一定要同时读懂数篇或十数篇被援引的其它论文。偏偏，这十几篇被援引的论文又各自援引十数篇其它论文。因此，相对于大学部的教科书而言，期刊论文是一个极端没有系统的知识，必须要靠读者自己从几十篇论文中撷取出相关的片段，自己组织成一个有系统的知识，然后才有办法开始阅读与吸收。要培养出这种自己组织知识的能力，需要在学校靠着大量而持续的时间去摸索、体会，而不可能只利用业余的零星时间去培养。因此，一个大学毕业后就不再念研究所的学生，不管他在毕业生和大学毕业生最大的差别，就是：学士只学习过吸收系统知识的能力（也就是读别人整理、组织好的知识，典型的就是课本）；但硕士则学习过自己从无组织的知识中检索、筛选、组织知识的能力。 （4）期刊论文的分析能力：为了确定你的学位论文研究成果确实比所有相关的学术期刊论文都更适合处理你所拟定的应用场域，首先你必须要有能力逐篇分析出所有相关期刊论文的优点与缺点，以及自己的研究成果的优点与缺点，然后再拿他们来做比较，总结出你的论文的优点和缺点（限制）。但是，好的期刊论文往往是国外著名学府的名师和一流的博士生共同的研究成果，假如你要在锁定的应用场域上「打败」他们，突出自己的优点，这基本上是一个极端困难的挑战。即使只是要找出他们的缺点，都已经是一个相当困难的工作了。一个大学毕业生，四年下来都是假定「课本是对的」这样地学下来的，从来没有学习如何分析课本知识的优缺点，也就是「只有理解的能力，而没有批判的能力」。硕士生则必须要有「对一切既有进行精确批判」的能力。但是，这个批判并非个人好恶或情绪化的批判，而是真的找得到充分理由去支持的批判。这个批判的能力，让你有能力自己找到自己的优、缺点，因此也有机会自己精益求精。所以，一个大学毕业生在业界做事的时候，需要有人指导他（从事批判性检验），帮他找出缺点和建议改进的可能性。但是，一个严格训练过的合格硕士，他做事的时候应该是不需要有人在背后替他做检证，他自己就应该要有能力分析自己的优、缺点，主动向上级或平行单位要求支持。其实，至少要能够完成这个能力，才勉强可以说你是有「独立自主的判断能力」。 （5）创新的能力：许多大学毕业的工程师也能创新，但是硕士的创新是和全世界同一个学术团体内所有的名师和博士生挑战。因此，两者是站在不同的比较基础上在进行的：前者往往是一个企业内部的「闭门造车」，后者是一个全球的开放性竞争。其次，工程师的创新往往是无法加以明确证明其适用条件，但是学术的创新却必须要能够在创新的同时厘清这个创新的有效条件。因此，大学毕业生的主要能力是吸收既有知识，但硕士毕业生却应该要有能力创造知识。此外，台湾历年来工业产品的价位偏低，这一部分是因为国际大厂的打压以及国际消费者的信任不易建立。但是，另一方面，这是因为台湾的产品在品质上无法控制，因此只好被当作最粗糙的商品来贩卖。台湾的产品之所以无法有稳定的品质，背后的技术原因就是：各种创新都是只凭一时偶然的巧思，却没有办法进一步有系统地厘清这些巧思背后可以成立的条件。但是，创新其实是可以有一套「有迹可寻」的程序的，这是我最得意的心得，也是我最想教的。 三、为什么要坚持培养阅读与分析期刊论文的能力 我所以一直坚持要训练研究生阅读与分析期刊论文的能力，主要是为了学生毕业后中长期的竞争力着想。 台湾从来都只生产国外已经有的产品，而不事创新。假如国外企业界比国外学术的技术落后三年，而台湾的技术比国外技术落后五年，则台湾业界所需要的所有技术都可以在国外学术期刊上找到主要的理论依据和技术核心构想（除了一些技术的细节和 know how 之外）。因此，阅读期刊的能力是台湾想要保持领先大陆技术的必备条件。 此外，只要能够充分掌握阅读与分析期刊论文的技巧，就可以水到渠成地轻松进行「创新」的工作。所以，只要深入掌握到阅读与分析期刊论文的技巧，就可以掌握到大学生不曾研习过的三种能力：（1）自己从无组织的知识中检索、筛选、组织知识的能力、（2）对一切既有进行精确批判的独立自主判断能力、（3）创造新知识的能力。 创新的能力在台湾一直很少被需要（因为台湾只会从国外买整套设备、制程和设计与制造的技术）。但是，大陆已经成为全球廉价品制造中心，而台商为了降低成本也主动带技术到大陆设厂（包括现在的晶元代工），因此整个不具关键性技术的制造业都会持续往大陆移动；甚至 IC 的设计（尤其数字的部分）也无可避免地会迅速朝向「台湾开系统规格，进行系统整合，大陆在前述架构下开发特定数位模块」的设计代工发展。因此，未来台湾将必然会被逼着朝愈来愈创意密集的创意中心走（包括商务创意、经营创意、产品创意、与技术创新）。因此，不能因为今天台湾的业界不需要创新的能力，就误以为自己一辈子都不需要拥有创新的能力。 我在协助民间企业发展技术研发的过程中，碰到过一位三十多岁的厂长。他很聪明，但从小家穷，被环境逼着去念高工，然后上夜校读完工专。和动态性能（ bandwidth、response speed等）无关的技术他都很深入，也因为产品升级的需要而认真向我求教有关动态性能的基本观念。但是，怎么教他都不懂，就只因为他不懂工程数学。偏偏，工程数学不是可以在工厂里靠自修读会的。一个那么聪明的人，只因为不懂工数，就注定从三十岁以后一辈子无法在专业上继续成长！他高工毕业后没几年，廿多岁就当课长，家人与师长都以他为荣；卅岁当厂长，公司还给他技术股，前途无量；谁想得到他会在卅岁以后被逼着「或者升级，或者去大陆，或者失业」？ 每次想起这位厂长，看着迫不急待地要到台积电去「七年赚两千万退休金」的学生，或者只想学现成可用的技术而不想学研究方法的学生，我总忍禁不住地要想：十年后，我教过的学生里，会不会有一堆人就只因为不会读期刊论文而被逼提前退休？ 再者，技术的创新并不是全靠聪明。我熟谙一套技术创新的方法，只要学会分析期刊论文的优缺点，就可拿这套方法分析竞争对手产品的优缺点；而且，只要再稍微加工，就可以从这套优缺点的清单里找到突破瓶颈所需的关键性创意。这套创新程序，可以把「创新」变成不需要太多天分便可以完成的事，从而减轻创意的不定性与风险性。因此，只要会分析论文，几乎就可以轻易地组合出你所需要的绝大部分创意。聪明是不可能教的，但这套技巧却是可以教的；而且只要用心，绝大部分硕士生都可以学会。 就是因为这个原因，我的实验室整个训练的重心只有一个：通过每周一次的 group meeting，培养学生深入掌握阅读与分析期刊论文的技巧，进而培养他们在关键问题上突破与创新的能力。 四、期刊论文的分析技巧与程序 一般来讲，好的期刊论文有较多的创意。虽然读起来较累，但收获较多而深入，因此比较值得花心思去分析。读论文之前，参考SCI Impact Factor 及学长的意见是必要的。 一篇期刊论文，主要分成四个部分。 （1）Abstract： 说明这篇论文的主要贡献、方法特色与主要内容。最慢硕二上学期必须要学会只看 Abstract 和Introduction便可以判断出这篇论文的重点和你的研究有没有直接关连，从而决定要不要把它给读完。假如你有能力每三十篇论文只根据摘要和简介便能筛选出其中最密切相关的五篇论文，你就比别人的效率高五倍以上。以后不管是做事或做学术研究，都比别人有能力从更广泛的文献中挑出最值得参考的资料。 （2）Introduction： Introduction 的功能是介绍问题的背景和起源，交代前人在这个题目上已经有过的主要贡献，说清楚前人留下来的未解问题，以及在这个背景下这篇论文的想解决的问题和它的重要性。对初学的学生而言，从这里可以了解以前研究的概况。通常我会建议初学的学生，对你的题目不熟时，先把跟你题目可能相关的论文收集个 30～40篇，每篇都只读Abstract 和 Introduction，而不要读 Main Body（本文），只在必要时稍微参考一下后面的 Illustrative examples和 Conclusions，直到你能回答下面这三个问题：（2A）在这领域内最常被引述的方法有哪些？（2B）这些方法可以分成哪些主要派别？（2C）每个派别的主要特色（含优点和缺点）是什么？ 问题是，你怎么去找到这最初的30～40篇论文？有一种期刊论文叫做「review paper」，专门在一个题目下面整理出所有相关的论文，并且做简单的回顾。你可以在搜寻 Compendex 时在 keywords 中加一个「review」而筛选出这类论文。然后从相关的数篇review paper 开始，从中根据 title 与 Abstract 找出你认为跟你研究题目较相关的30～40篇论文。 通常只要你反复读过该领域内30～40篇论文的Abstract 和 Introduction，你就应该可以从Introduction的评论中回答（2A）和（2B）这两个问题。尤其要记得，当你阅读的目的是要回答（2A）和（2B）这两个问题时，你一定要先挑那些 Introduction写得比较有观念的论文念（很多论文的Introduction 写得像流水帐，没有观念，这种论文刚开始时不要去读它）。假如你读过假如30～40篇论文的 Abstract 和 Introduction之后，还是回答不了（2C），先做下述的工作。 你先根据（2A）的答案，把这领域内最常被引述的论文找齐，再把他们根据（2B）的答案分成派别，每个派别按日期先后次序排好。然后，你每次只重新读一派的 Abstract 和 Introduction（必要时简略参考内文，但目的只是读懂Introduction内与这派有关的陈述，而不需要真的看懂所有内文），照日期先后读 ，读的时候只企图回答一个问题：这一派的创意与主要诉求是什么？这样，你逐派逐派地把每一派的Abstract 和 Introduction 给读完，总结出这一派主要的诉求 、方法特色和优点（每一篇论文都会说出自己的优点，仔细读就不会漏掉）。 其次，你再把这些论文拿出来，但是只读Introduction，认真回答下述问题：「每篇论文对其它派别有什么批评？」然后你把读到的重点逐一记录到各派别的「缺点」栏内。 通过以上程序，你就应该可以掌握到（2A）、（2B）、和（2C）三个问题的答案。这时你对该领域内主要方法、文献之间的关系算是相当熟捻了，但是你还是只仔细 读完Abstract 和 Introduction而已，内文则只是笼统读过。 这时候，你已经掌握到这领域主要的论文，你可以用这些论文测试看看你用来搜寻这领域论文的 keywords 到底恰不恰当，并且用修正过的 keywords 再搜寻一次论文，把这领域的主要文献补齐，也把原来30～40篇论文中后来发现关系较远的论文给筛选掉，只保留大概20篇左右确定跟你关系较近的文献。如果有把握，可以甚至删除一两个你不想用的派别（要有充分的理由），只保留两、三个派别（也要有充分的理由）继续做完以下工作。 然后你应该利用（2C）的答案，再进一步回答一个问题（2D）：「这个领域内大家认为重要的关键问题有哪些？有哪些特性是大家重视的优点？有哪些特性是大家在意的缺点？这些优点与缺点通常在哪些应用场合时会比较被重视？在哪些应用场合时比较不会被重视？」然后，你就可以整理出这个领域（研究题目）主要的应用场合，以及这些应用场合上该注意的事项。 最后，在你真正开始念论文的 main body 之前，你应该要先根据（2A）和（2C的答案，把各派别内的论文整理在同一个档案夹里，并照时间先后次序排好。然后依照这些派别与你的研究方向的关系远近，一个派别一个派别地逐一把各派一次念完一派的 main bodies。 （3）Main body（含simulation and/or experimental examples）： 在你第一次有系统地念某派别的论文 main bodies 时，你只需要念懂：（3A）这篇论文的主要假设是什么（在什么条件下它是有效的），并且评估一下这些假设在现实条件下有多容易（或多难）成立。愈难成立的假设，愈不好用，参考价值也愈低。（3B）在这些假设下，这篇论文主要有什么好处。（3C）这些好处主要表现在哪些公式的哪些项目的简化上。至于整篇论文详细的推导过程，你不需要懂。除了三、五个关键的公式（最后在应用上要使用的公式，你可以从这里评估出这个方法使用上的方便程度或计算效率，以及在非理想情境下这些公式使用起来的可靠度或稳定性）之外，其它公式都不懂也没关系，公式之间的恒等式推导过程可以完全略过去。假如你要看公式，重点是看公式推导过程中引入的假设条件，而不是恒等式的转换。 但是，在你开始根据前述问题念论文之前，你应该先把这派别所有的论文都拿出来，逐篇粗略地浏览过去（不要勉强自己每篇或每行都弄到懂，而是轻松地读，能懂就懂，不懂就不懂），从中挑出容易念懂的 papers，以及经常被引述的论文。然后把这些论文照时间先后次序依序念下去。记得：你念的时候只要回答（3A）、（ 3B）、（3C）三个问题就好，不要念太细。 这样念完以后，你应该把这一派的主要发展过程，主要假设、主要理论依据、以及主要的成果做一个完整的整理。其次，你还要在根据（2D）的答案以及这一派的主要假设，进一步回答下一个问题：（3D）这一派主要的缺点有哪些。最后，根据（ 3A）、（3B）、（3C）、（3D）的答案综合整理出：这一派最适合什么时候使用，最不适合什么场合使用。 记住：回答完这些问题时，你还是不应该知道恒等式是怎么导出来的！ 当你是生手的时候，你要评估一个方法的优缺点时，往往必须要参考它Examples。但是，要记得：老练的论文写作高手会故意只 present 成功的案例而遮掩失败的案例。所以，simulation examples and/or experiments 很棒不一定表示这方法真的很好。你必须要回到这个方法的基本假设上去，以及他在应用时所使用的主要公式（resultant equations）去，凭自己的思考能力， 并且参考（2C）和（2D）的答案，自己问问看：当某某假设在某些实用场合上无法成立时，这个方法会不会出什么状况？猜一猜，预测一下这个方法应该会在哪些条件下（应用场合）表现优异，又会在哪些条件下（应用场合）出状况？根据这个猜测再检验一次simulation examples and/or experiments，看它的长处与短处是不是确实在这些examples 中充分被检验，且充分表现出来。 那么，你什么时候才需要弄懂一篇论文所有的恒等式推导过程，或者把整篇论文细细读完？NEVER！你只需要把确定会用到的部分给完全搞懂就好，不确定会不会用到的部分，只需要了解它主要的点子就够了。 硕士生和大学生最主要的差别：大学生读什么都必须要从头到尾都懂，硕士生只需要懂他用得着的部分就好了！大学生因为面对的知识是有固定的范围，所以他那样念。硕士生面对的知识是没有范围的，因此他只需要懂他所需要的细腻度就够了。硕士生必须学会选择性的阅读，而且必须锻炼出他选择时的准确度以及选择的速度，不要浪费时间在学用不着的细节知识！多吸收「点子」比较重要，而不是细部的知识。 五、方法与应用场合特性表（有迹可寻的创意程序） 试着想象说你从上图中论文阅读步骤的第（4）与（5）步骤分别获得以下两张表：譬如，当你的题目是「如何标定fiducial mark 之中心位置」，你就必须要仔细搜寻出文献上所有可能可以用来做这一个工作的方法。或许你找到的方法一共有四种，依序如下。譬如（随便乱举例），「方法一」可能表示：「以面积形心标定 fiducial mark 之中心位置」，「方法二」可能表示「以 Hugh transform标定 fiducial mark 之中心位置」，「方法三」可能表示：「以局部弧形 matching 的方法标定fiducial mark 之中心位置」，「方法四」可能表示：「以 ring code标定fiducial mark 之中心位置」。 这些方法各有它的特色（优缺点），譬如（随便乱举例），特性1可能表示「计算速度」（因此，根据上表左边第一个 row，可以发现：方法一的计算速度很快，方法二与方法三的计算速度很慢，而方法四的计算速度普通。其次，特性2可能代表「光源亮度不稳定时计算位置的误差大小」，特性3可能代表「噪声对计算出的位置干扰多大」，特性4可能代表「图形边缘有破损时计算的可靠度」，特性5可能代表「对象有彼此的遮蔽时方法的适用性」等等。所以，以上左图中第五个row为例，可以发现：当对象有彼此的遮蔽时，除方法二之外其它三个方法的适用性都很好。 但是，同样一个方法可能有许多不同的应用场合，而不同应用场合可能会对适用（或最佳）的方法有不同要求。所以，让我们来看右边的「问题特性分析表」。譬如（随便乱举例），应用甲可能是「标定fiducial mark 之中心位置」的方法在「电路插件组装（SMT）」里的应用，应用乙可能是「标定fiducial mark 之中心位置」的方法在「生物检验自动化影像处理」里的应用，而应用丙则可能是「标定 fiducial mark 之中心位置」的方法在「巡乂飞弹目标搜寻」里的应用。这三种应用场合更有其关注的特性。譬如，根据上面右表第二个 row 的资料，三种应用场合对特性2（光源亮度不稳定时计算位置的误差大小）都很在意。再譬如，根据上面右表第四个 row 的资料，三种应用场合中除了应用甲（电路插件组装（SMT））之外，其它两种应用场合对特性4（图形边缘有破损时计算的可靠度）都很在意。 那么，四个方法中哪个方法最好？你可能会回答说：「方法二！因为它的优点最多，缺点最少。」但是，这样的回答是错的！一个方法只有优缺点，而没有好坏。当它被用在一个适合表现其优点而不在乎其缺点的场合里，它就显得很好；但是，当它被用在一个不适合表现其优点而很在乎其缺点的场合里，它就显得很糟。譬如，方法二在应用场合乙，它的表现会非常出色（因为所有的优点刚好那个应用场合都在意，而所有的缺点刚好那个应用场合都不在意）；但是，方法二在应用场合甲里它的表现却会非常糟糕（它所有的缺点刚好那个应用场合都很在意，而它大部分的优点刚好那个应用场合却都不在意）。所以，必须要学会的第一件是就是：方法没有好坏，只有相对优缺点点；只有当方法的特性与应用场合的特性不合时，才能下结论说这方法「不适用」；二当当方法的特性与应用场合的特性吻合时，则下结论说这方法「很适用」。因此，一定要同时有方法特性表与应用场合特性分析表放在一起后，才能判断一个方法的适用性。 更重要的是：上面的方法与问题分析对照表还可以用来把「突破瓶颈所需的创意」简化成一种「有迹可寻」的工作。譬如，假定我们要针对应用甲发展一套适用的方法，首先我们要先从上右表中标定这个应用场合关心哪些问题特性。根据上右表第一个 column，甲应用场合只关心四个特性：特性1、2、3、5（即「计算速度」、「光源亮度不稳定时计算位置的误差大小」、「噪声对计算出的位置的干扰」、「对象有彼此的遮蔽时方法的适用性」）。那么，哪个方法最适用呢？看起来是方法 一，它除了特性2表现普通之外，其它三个特性的表现都很出色。但是，假如我们对方法一的表现仍不够满意，怎么去改善它？最简单的办法就是从上左表找现成的方法和方法一结合，产生出一个更适用的方法。因为方法一只有在特性2上面表现不够令人满意，所以我们就优先针对在特性2上面表现出色的其它方法加以研究。根据上左表，在特性2上面表现出色的方法有方法二和方法四，所以我们就去研究这两个方法和方法一结合的可能性。或许（随便举例）方法四的创意刚好可以被结合进方法一而改善方法一在特性2上面的表现，那么，我们就可以因此轻易地获得一个方法一的改良，从而突破甲应用场合没有适用方法的瓶颈。 有没有可能说单纯常识结合既有方法优点仍无法突破技术瓶颈的状况？可能有。这时候真的需要完全新颖的创意了。但是，这种时候很罕见。多半时候只要应用上一段的分析技巧就可以产生足以解决实用问题的创意了。至少，要产生出一篇学术期刊论文并非那么困难。 六、论文阅读的补充说明 硕士生开始学读期刊论文时，就容易犯的毛病就是戒除不掉大学部的习惯：（1）老是想逐行读懂，有一行读不懂就受不了。（2）不敢发挥自己的想象，读论文像在读教科书，论文没写的就不会，瘫痪在那里；被我逼着去自己猜测或想象时，老怕弄错作者的意思，神经绷紧，脑筋根本动不了。 大学毕业后（不管是念硕、博士或工作），可以参考的资料都没有秩序地交错成一团，而且永远都读不完。用大学生的心态读书，结果一定时间永远不够用。因此，每次读论文都一定要带着问题去读，每次读的时候都只是图回答你要回答的问题。因此，一定是选择性地阅读，一定要逐渐由粗而细地一层一层去了解。上面所规划的读论文的次序，就是由粗而细，每读完一轮，你对这问题的知识就增加一层。根据这一层知识就可以问出下一层更细致的问题，再根据这些更细致的问题去重读，就可以理解到更多的内容。因此，一定是一整批一起读懂到某个层次，而不是逐篇逐篇地整篇一次读懂。 这样读还有一个好处：第一轮读完后，可以根据第一轮所获得的知识判断出哪些论文与你的议题不相关，不相关的就不需要再读下去了。这样才可以从广泛的论文里逐层准确地筛选出你真正非懂不可的部分。不要读不会用到的东西，白费的力气必须被极小化！其实，绝大部分论文都只需要了解它的主要观念（这往往比较容易），而不需要了解它的详细推导过程（这反而比较费时）。 其次，一整批一起读还有一个好处：同一派的观念，有的作者说得较易懂，有的说得不清楚。整批读略过一次之后，就可以规划出一个你以为比较容易懂的阅读次序，而不要硬碰硬地在那里撞墙壁。你可以从甲论文帮你弄懂以论文的一个段落，没人说读懂甲论文只能靠甲论文的信息。所以，整批阅读很像在玩跳棋，你要去规划出你自己阅读时的「最省力路径」。 大学部学生读东西一定要循规蹈矩，你还没修过机械视觉相关课程之前可能也只好循规蹈矩地逐行去念。但是一旦修过机械视觉相关课程，许多论文中没被交代的段落你也已经可以有一些属于你的想象（虽然有可能猜错，尤其刚开始时经常猜错，但没关系，下面详述）。这些想象往往补足论文跳跃处最快速的解决方案。其实，一个大学毕业生所学已经很多了，对许多是都可以有一个不太离谱的想象能力。但是大部分学生却根本不敢去想象。我读论文远比学生快，分析远比学生深入，主要的是我敢想象与猜测，而且多年训练下来想象与猜测的准确度很高。所以，许多论文我根本不是「读懂」的，而是「猜对」了！ 假如猜错了怎么办？不用怕！猜完一后要根据你的猜测在论文里找证据，用以判断你的猜测对不对。猜对了，就用你的猜测（其实是你的推理架构）去吸收作者的资讯与创意（这会比从头硬生生地去迁就作者的思路轻松而容易）；猜错了，论文理会有一些信息告诉你说你错了，而且因为猜错所以你读到对的答案时反而印象更深刻。 七、论文报告的要求与技巧 报告一篇论文，我要求做到以下部分（依报告次序排列）： （1） 投影片第一页必须列出论文的题目、作者、论文出处与年份。 （2） 以下每一页投影片只能讲一个观念，不可以在一张投影片里讲两个观念。 （3） 说明这篇论文所研究的问题的重点，以及这个问题可能和工业界的哪些应用相关。 （4） 清楚交代这篇论文的主要假设，主要公式，与主要应用方式（以及应用上可能的解题流程）。 （5） 说明这篇论文的范例（simulation examples and/or experiments），预测这个方法在不同场合时可能会有的准确度或好用的程度 （6） 你个人的分析、评价与批评，包括：（6A）这篇论文最主要的创意是什么？（6B）这些创意在应用上有什么好处？（6C）这些创意和应用上的好处是在哪些条件下才能成立？（6D）这篇论文最主要的缺点或局限是什么？（6E）这些缺点或局限在应用上有什么坏处？（6F）这些缺点和应用上的坏处是因为哪些因素而引入的？（6G）你建议学长学弟什么时候参考这篇论文的哪些部分（点子）？ 一般来讲，刚开始报告论文（硕一上学期）时只要做到能把前四项要素说清楚就好了，但是硕一结束后（暑假开始）必须要设法做到六项要素都能触及。硕二下学期开始的时候，必须要做到六项都能说清楚。 注意：读论文和报告论文时，最重要的是它的创意和观念架构，而不是数学上恒等式推导过程的细节（顶多只要抓出关键的 equation 去弩懂以及说明清楚即可）。你报告观念与分析创意，别人容易听懂又觉得有趣；你讲恒等式，大家不耐烦又浪费时间。]]></content>
      <categories>
        <category>paper</category>
      </categories>
      <tags>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[决策树(Decision Tree) && ID3 Algorithm]]></title>
    <url>%2F2017%2F01%2F13%2FDecision%20Tree%20%26%26%20ID3%20Algorithm%20%2F</url>
    <content type="text"><![CDATA[分类与预测餐饮企业经常会碰到下面的问题： 如何预测未来一段时间内，哪些顾客会流失，哪些顾客最有可能成为VIP客户？ 如何预测一种心产品的销售量，以及在哪种类型的客户中会较受欢迎？ 除此之外，餐厅经理需要通过数据分析来了解具有某些特征的顾客的消费习惯/这些都是分类与预测的例子。 常见的分类预测算法贝叶斯贝叶斯（Bayes）分类算法是一类利用概率统计知识进行分类的算法，如朴素贝叶斯（Naive Bayes）算法。这些算法主要利用Bayes定理来预测一个未知类别的样本属于各个类别的可能性，选择其中可能性最大的一个类别作为该样本的最终类别。 决策树决策树是用于分类和预测的主要技术之一，决策树学习是以实例为基础的归纳学习算法，它着眼于从一组无次序、无规则的实例中推理出以决策树表示的分类规则。 人工神经网络人工神经网络（Artificial Neural Networks，ANN）是一种应用类似于大脑神经突触联接的结构进行信息处理的数学模型。在这种模型中，大量的节点（或称”神经元”，或”单元”）之间相互联接构成网络，即”神经网络”，以达到处理信息的目的。 支持向量机支持向量机（SVM，Support Vector Machine）是Vapnik根据统计学习理论提出的一种新的学习方法，它的最大特点是根据结构风险最小化准则，以最大化分类间隔构造最优分类超平面来提高学习机的泛化能力，较好地解决了非线性、高维数、局部极小点等问题。 决策树简介决策树(Decision Tree)是一个预测模型，他代表的是对象属性与对象值之间的一种映射关系。树中每个节点表示某个对象，而每个分叉路径则代表的某个可能的属性值，而每个叶结点则对应从根节点到该叶节点所经历的路径所表示的对象的值。决策树仅有单一输出，若欲有复数输出，可以建立独立的决策树以处理不同输出。 数据挖掘中决策树是一种经常要用到的技术，可以用于分析数据，同样也可以用来作预测。 示例记录 根据上述数据构造出如下决策树 信息熵、条件熵和信息增益信息熵信息熵也称为香农熵，是随机变量的期望。度量信息的不确定程度。信息的熵越大，信息就越不容易搞清楚。处理信息就是为了把信息搞清楚，就是熵减少的过程。决定信息的不确定性或者说复杂程度主要因素是概率。 我们要获得随机变量 D 的取值结果至少要进行1次试验，试验次数与随机变量 D 可能的取值数量(2种)的对数函数Log有联系。Log2=1(以2为底)。因此熵的计算公式是： 条件熵条件熵是通过获得更多的信息来消除一元模型中的不确定性。也就是通过二元或多元模型来降低一元模型的熵。我们知道的信息越多，信息的不确定性越小。例如，只使用一元模型时我们无法根据用户历史数据中的购买频率来判断这个用户本次是否也会购买。因为不确定性太大。在加入了促销活动，商品价格等信息后，在二元模型中我们可以发现用户购买与促销活动，或者商品价格变化之间的联系。并通过购买与促销活动一起出现的概率，和不同促销活动时购买出现的概率来降低不确定性。以下公式为属性A的信息条件熵。 用属性A出现的概率乘以属性A确定的情况下，相应分类的信息熵。 信息增益信息增益用来衡量信息之间相关性的指标。用于度量属性A降低样本集合X熵的贡献大小。信息增益越大，不确定性越小，越适于对X分类。具体的计算方法就熵与条件熵之间的差。公式如下： ID3 算法原理 奥卡姆剃刀（Occam’s Razor, Ockham’s Razor），又称“奥坎的剃刀”，是由14世纪逻辑学家、圣方济各会修士奥卡姆的威廉（William of Occam，约1285年至1349年）提出，他在《箴言书注》2卷15题说“切勿浪费较多东西，去做‘用较少的东西，同样可以做好的事情’。简单点说，便是：be simple。 ID3算法（Iterative Dichotomiser 3 迭代二叉树3代）是一个由Ross Quinlan发明的用于决策树的算法。这个算法便是建立在上述所介绍的奥卡姆剃刀的基础上：越是小型的决策树越优于大的决策树（be simple 简单理论）。尽管如此，该算法也不是总是生成最小的树形结构，而是一个启发式算法。 OK，从信息论知识中我们知道，期望信息越小，信息增益越大，从而纯度越高。ID3算法的核心思想就是以信息增益度量属性选择，选择分裂后信息增益(很快，由下文你就会知道信息增益又是怎么一回事)最大的属性进行分裂。该算法采用自顶向下的贪婪搜索遍历可能的决策树空间。 算法流程： 对当前样本集合计算所有属性的信息增益； 选择信息增益最大的属性作为测试属性，把测试属性取值相同的样本划为同一个子样本集； 若子样本集的类别属性只含有单个属性，则分支为叶子节点，判断其属性之并标上相应的符号，然后返回调用处；否则对子样本集递归调用本算法。 缺点由于ID3决策树算法采用信息增益作为选择测试属性的标准，会偏向于选择取值较多的，即所谓的高度分支属性，而这类属性并不一定是最优属性。并且其只能处理离散属性，对于连续类型属性需要对其进行离散化。为了解决倾向于选择高度分支属性的问题，采用信息增益率作为选择测试属性的标准，这样便有了C4.5决策树算法。常用的还有 CART,SLIQ,SPRINT,PUBLIC等。 决策树实例这是一家高尔夫球俱乐部的历史数据，里面记录了不同天气状况用户来打高尔夫球的历史记录。我们要做的是通过构建决策树来预测用户是否会来打高尔夫球。这里用户是否来打球是一个一元模型，具有不确定性，熵值很高。我们无法仅通过Yes和No的频率来判断用户明天是否会来。因此，需要借助天气的信息来减少不确定性。下面分别记录到了4种天气情况，我们通过计算条件熵和互信息来开始构建决策树的第一步：构建根决策点。 构建根决策节点构建根决策点的方法就是寻找4种天气情况中与打高尔夫球相关性最高的一个。首先我们来看Play Golf这个一元模型的熵，来看看这件事的不确定性有多高. 一元模型的熵即信息熵在一元模型中，仅通过历史数据的概率来看预测Play Golf是一件非常不确定的事情，在14条历史数据中，打球的概率为64%，不打球的概率为36%。熵值达到了0.940。这与之前抛硬币的例子很像。在无法改变历史数据的概率时，我们需要借助更多的信息来降低不确定性。也就是计算条件熵。 二元模型条件熵计算二元模型的条件熵需要知道Play Golf与4种天气情况一起出现的概率，以及在不同天气情况下Play Golf出现的条件概率。下面我们分别来计算这两类概率。 出现概率 条件概率 条件熵 信息增益在已知Play Golf的一元模型熵和不同天气条件下的二元模型熵后。我们就可以通过信息增益来度量哪种天气与Play Golf的相关性最高了。 构建根节点在整个决策树中，Outlook因为与Play Golf的相关性最高，所以作为决策树的根节点。以Outlook作为根节点后，决策树出现了三个分支，分别是Outlook的三个不同的取值Sunny，Overcast和Rainy。其中Overcast所对应的Play Golf都是Yes，因此这个分支的叶子节点为Yes。 构建分支节点另外两个分支我们将使用和前面一样的方法，通过计算熵，条件熵和信息增益来挑选下一个分支的决策节点。通过将决策树中每个决策点还原为原始数据表可以发现，每一个决策点都对应了一张数据表。从根决策节点开始，我们通过计算熵寻找与Play Golf最相关的天气信息，来建立决策点及分支，并反复迭代这一过程。直到最终构建完整的决策树。 使用决策树进行预测文章开始的时候我们说过，决策树是用来进行分类和预测的。具体过程如下。当我们构建好决策树后，当有新的信息发送时，我们利用已有的决策树逻辑对新的信息结构进行判断。当信息的内容与决策树一致时，就进入下一分支进行判断，并通过叶子节点获得分类的结果。例如，当新的一天开始时，我们就可以通过4个天气特征来判断用户是否会来打高尔夫球。以下是具体预测流程的示意图，首先寻找新信息中的根决策节点Outlook，根据Outlook的取值进入到Sunny分支，在Sunny分支中继续判断下一决策点Windy的取值，新的信息中Windy的取值为FALSE，根据决策树中的逻辑返回Yes。因此在新信息中通过对天气情况的判断预测用户会来打高尔夫球。 随机森林决策树是建立在已知的历史数据及概率上的，一课决策树的预测可能会不太准确，提高准确率最好的方法是构建随机森林(Random Forest)。所谓随机森林就是通过随机抽样的方式从历史数据表中生成多张抽样的历史表，对每个抽样的历史表生成一棵决策树。由于每次生成抽样表后数据都会放回到总表中，因此每一棵决策树之间都是独立的没有关联。将多颗决策树组成一个随机森林。当有一条新的数据产生时，让森林里的每一颗决策树分别进行判断，以投票最多的结果作为最终的判断结果。以此来提高正确的概率。 使用 Mahout 进行 Random Forests 实现数据集示例：12345678910111,1.52101,13.64,4.49,1.10,71.78,0.06,8.75,0.00,0.00,12,1.51761,13.89,3.60,1.36,72.73,0.48,7.83,0.00,0.00,13,1.51618,13.53,3.55,1.54,72.99,0.39,7.78,0.00,0.00,14,1.51766,13.21,3.69,1.29,72.61,0.57,8.22,0.00,0.00,15,1.51742,13.27,3.62,1.24,73.08,0.55,8.07,0.00,0.00,16,1.51596,12.79,3.61,1.62,72.97,0.64,8.07,0.00,0.26,17,1.51743,13.30,3.60,1.14,73.09,0.58,8.17,0.00,0.00,18,1.51756,13.15,3.61,1.05,73.24,0.57,8.24,0.00,0.00,19,1.51918,14.04,3.58,1.37,72.08,0.56,8.30,0.00,0.00,110,1.51755,13.00,3.60,1.36,72.99,0.57,8.40,0.00,0.11,1... 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162//上传数据集$ hadoop fs -put glass.txt /user/ubuntu/glass.txt//生成描述文件ubuntu@master:~/algorithm$ mahout describe -p /user/ubuntu/glass.txt -f glass.info -d I 9 N LMAHOUT_LOCAL is not set; adding HADOOP_CONF_DIR to classpath.Running on hadoop, using /home/ubuntu/cloud/hadoop-2.7.2/bin/hadoop and HADOOP_CONF_DIR=/home/ubuntu/cloud/hadoop-2.7.2/etc/hadoopMAHOUT-JOB: /home/ubuntu/cloud/mahout-0.10.1/mahout-examples-0.10.1-job.jar17/01/14 02:53:49 WARN MahoutDriver: No describe.props found on classpath, will use command-line arguments only17/01/14 02:53:49 INFO Describe: Generating the descriptor...17/01/14 02:53:50 INFO Describe: generating the dataset...17/01/14 02:53:51 INFO Describe: storing the dataset description17/01/14 02:53:51 INFO MahoutDriver: Program took 1584 ms (Minutes: 0.0264)// 生成随机森林$ mahout buildforest -d /user/ubuntu/glass.txt -ds glass.info -sl 3 -ms 3 -p -t 5 -o output....17/01/14 03:05:50 INFO HadoopUtil: Deleting hdfs://master:9000/user/ubuntu/output17/01/14 03:05:50 INFO BuildForest: Build Time: 0h 0m 15s 8617/01/14 03:05:50 INFO BuildForest: Forest num Nodes: 20317/01/14 03:05:50 INFO BuildForest: Forest mean num Nodes: 4017/01/14 03:05:50 INFO BuildForest: Forest mean max Depth: 917/01/14 03:05:50 INFO BuildForest: Storing the forest in: output/forest.seq17/01/14 03:05:50 INFO MahoutDriver: Program took 16124 ms (Minutes: 0.2687333333333333)// 对随机森林进行评估$ mahout testforest -i /user/ubuntu/glass.txt -ds glass.info -m output -mr -a -o test...17/01/14 03:09:48 INFO TestForest: =======================================================Summary-------------------------------------------------------Correctly Classified Instances : 191 89.2523%Incorrectly Classified Instances : 23 10.7477%Total Classified Instances : 214=======================================================Confusion Matrix-------------------------------------------------------a b c d e f &lt;--Classified as68 1 1 0 0 0 | 70 a = 16 68 1 0 0 1 | 76 b = 25 1 11 0 0 0 | 17 c = 30 0 0 10 0 3 | 13 d = 50 0 0 0 8 1 | 9 e = 61 1 1 0 0 26 | 29 f = 7=======================================================Statistics-------------------------------------------------------Kappa 0.7707Accuracy 89.2523%Reliability 72.3985%Reliability (standard deviation) 0.3365Weighted precision 0.897Weighted recall 0.8925Weighted F1 score 0.891417/01/14 03:09:48 INFO MahoutDriver: Program took 18829 ms (Minutes: 0.3138166666666667) 参考文章 张良均等.Hadoop大数据分析与挖掘实战.机械工业出版社.2016.10决策树分类和预测算法的原理及实现数据挖掘之决策树算法杂货铺——分类算法之决策树了解信息增益和决策树]]></content>
      <categories>
        <category>Machine learning</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[「China Daily」Tencent launches WeChat 'mini apps']]></title>
    <url>%2F2017%2F01%2F10%2FTencent%20launches%20WeChat%20mini%20app%2F</url>
    <content type="text"><![CDATA[By Fan Feifei | China Daily | Updated: 2017-01-10 07:10 http://www.chinadaily.com.cn/business/tech/2017-01/10/content_27906217.htm Chinese people check their WeChat 14.5 times on average and spend 48 minutes on social media per day. [Photo/IC] News textChinese internet giant Tencent Holdings Ltd launched “mini apps” on Monday, which let users interact with app-like services within its instant messaging app WeChat, without having to download and install them. Users just need to scan the QR code or search from their WeChat accounts to open these apps, where they could book tickets or do shopping, Tencent said. They can exit the apps easily without downloading them, these apps won’t disturb users, such as by sending notifications and subscription messages, according to Tencent. This is an important step in Tencent’s commercialization of its messaging service. WeChat now has 846 million users worldwide, according to Tencent’s fiscal 2016 third-quarter earnings report. Lu Zhenwang, CEO of Shanghai-based Wanqing Consultancy, said: “The launch of mini apps can solve some of the problems caused by apps, but it won’t take the place of AppStore or Android Market.” “Some ‘lightweight’ apps, which possess fewer functions and more simple algorithms, and apps that are not commonly used, are more suitable as WeChat’s mini apps. However, the commonly used ones should still be downloaded to your phone,” Lu added. Dong Xu, an analyst with Beijing-based consultancy Analysis, said: “With the mini apps, users could find and use them whenever they need, and won’t receive any advertising when they stop using them.” The aim is to ensure users spend more time on WeChat and extend Tencent’s lead as an app vendor. Mobile app developers are vying for users in the fiercely competitive and lucrative market. Chinese people check their WeChat 14.5 times on average and spend 48 minutes on social media per day, according to a report released by Chinese tech startup Kika. Wang Qian, an employee of an IT company, has tried the “mini apps” since they were launched on Monday. “The design of these apps is very clear, and will increase the utilization rate of the WeChat wallet. Maybe it could replace some apps, but I still prefer to download apps from my app store.” Words: Tencent Holdings Ltd : 腾讯控股有限公司 Ltd : 有限责任公司（Limited） QR-Code ：二维码 Quick Response(QR) code 快速响应矩阵码(QR)a machine-readable code consisting of an array of black and white squares, typically used for storing URLs or other information for reading by the camera on a smartphone. consultancy：咨询公司；顾问工作 lightweight：轻量级 fiercely：猛烈地；厉害地 lucrative： 有利可图的，赚钱的；合算的 vie : 争；竞争 vendor：卖主；小贩；供应商；[贸易] 自动售货机 Translation中国互联网巨头腾讯控股有限公司在周一上线了“小程序”。用户可以在即时通讯软件微信中与类似App的服务交互，而不需要下载和安装它们。 腾讯说，用户只需要扫描二维码或者从他们的微信账号中搜索以打开这些App。在小程序中用户可以订票或者购物。 他们不用下载就可以轻松地退出App，这些小程序也不会通过发送通知和订阅消息来打扰用户，据腾讯所说。 这是腾讯将其信息服务商业化中非常重要的一步。据腾讯2016年第三季度财报显示，微信目前在世界范围内拥有8.46亿用户。 Lu Zhenwang，上海万青咨询公司CEO，说：“小程序的上线可以解决一些App的问题，但是其无法代替App Store 和 Android Market”。 “一些有少量方法和简单算法的轻应用，和一些不经常使用的应用非常适合与微信小程序。然而，经常使用的应用还是应该下载道你的手机上。”Lu 补充说。 Dong Xu，北京咨询分析公司的分析师，说：“有了小程序，用户可以在需要时找到并使用它们，并且在停止使用后不会受到任何广告。” 其目标是为了确保用户在微信上花费更多时间，并延伸腾讯作为应用程序供应商的领军地位。移动应用开发者正在竞争激烈和利益丰厚的市场上竞争用户。 根据中国科技创业公司KIka发布的一份报告，中国人每天平均查看微信14.5次，花费48分钟在社交媒体上。 Wang Qian，一个IT公司的职员，从周一小程序上线就开始试用。“Apps的设计非常清晰，并且会增加微信钱包的使用率。或许其可以代替某些App，但是我依然喜欢从app store下载App。” Thought这是第一篇英文新闻的翻译，打算接下来按照这个模式，隔几天能翻译一篇新闻或者短文章，学习积累，并且在后面能够做自己的思考。 这两天微信小程序的上线，也是在各大科技媒体的首页上火了一番。好奇心使然，我也在上线第一天就尝试试用了几个小程序，比如有道词典、豆瓣评分、滴滴出行。使用下来，第一感受就是，确实如上China Daily的新闻中所说，小程序适合于轻应用。传统的类似图片处理、大型社交类应用还是应该下载App体验更加完善。 小程序是基于网页的，那么其功能性肯定受到限制。可以设想这样一些场景是比较适合的。一些你不常打开的小应用，比如豆瓣评分。在看电影前，通过微信小程序查询电影评分来决定是否要观看某一部影片；在餐厅通过点餐小程序进行点餐。目标是轻便、简单、直接。 微信小程序的入口是二维码，并且无推送，不能分享到朋友圈，这就决定了企业要通过小程序来赚取流量是一个相对困难的做法。订阅号解决了传统的短信推送问题，让用户的体验更好。我认为小程序解决的是工具问题，它不适合做媒体应用。类似于今日头条Lite这样的小程序，本质上还是把小程序当作第二个WEB新闻界面来做，我觉得用处并不大。我一直在想，目前二维码（QR Code）作为各种功能、页面的入口似乎有些不够便利，用户需要打开手机摄像头，扫码，再进入相应内容，并且很多页面由于微信的限制，还需要用手机浏览器中打开。这并不是我理想中的交互方式，觉得未来肯定会出现更加便利的解决方案。 小程序能够分享到微信群，微信好友，这是一个信息传递的过程，信息的分享、传递都在微信中就能实现，似乎微信想要把web搬到其中。知乎上一个回答说道，这就是在 Cosplay Web 。这似乎不无道理。 目前小程序的发布有严格限制，还不支持个人开发者，有点遗憾。兴趣使然，果断就找了个教程开始学起来，想要看看小程序的整个实现过程，才能深入理解它的精髓。 小程序到底会不会成功，拭目以待。]]></content>
      <categories>
        <category>好奇心英语</category>
      </categories>
      <tags>
        <tag>ChinaDaily</tag>
        <tag>News</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hive动态分区]]></title>
    <url>%2F2017%2F01%2F03%2Fhive%E5%8A%A8%E6%80%81%E5%88%86%E5%8C%BA%2F</url>
    <content type="text"><![CDATA[实际使用中需要对处理后的数据按照时间分开，后续有按照时间的插入和删除操作，所以用hive的分区表是一个很好的解决方案。比较蛋疼的是，由于 hive 不支持使用 load 语句进行动态分区插入数据，所以要新建一个表，再用 insert 语句把表中数据导入到新建的分区表中。 测试数据内容1234567103646 铁血皇城 3 2016-04-19 2016-04-19104046 铁骑三国 39 2016-04-19 2016-04-191041 九阴绝学 2 2016-03-26 2016-03-26104238 传奇盛世 3 2016-04-28 2016-04-28104928 天问 1 2016-02-27 2016-02-27106417 神曲 2 2016-04-15 2016-04-1510883 灵域 1 2016-04-15 2016-04-15 由于后续还需要使用分区后的数据进行MapReduce操作，所以在后面复制了一段时间字段（分区后，分区字段会变成hive中的文件夹名） 创建表并导入数据123456create table IGCT(id STRING,game STRING,count INT,time STRING,addtime STRING)row format delimitedfields terminated by '\t'stored as textfile;LOAD DATA INPATH '/recommend/hive/partitionTest' INTO TABLE IGCT_static; 建立分区表并导入数据123456789101112set hive.exec.dynamic.partition.mode=nonstrict; set hive.exec.dynamic.partition=true;CREATE TABLE IGCT_dynamic(id STRING,game STRING,count INT,time STRING) partitioned by(addtime STRING)ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'STORED AS TEXTFILEinsert overwrite table IGCT_dynamic partition(addtime) select * from IGCT; ` 查看数据12345678910hive&gt; select * from IGCT_dynamic;OK104928 天问 1 2016-02-27 2016-02-271041 九阴绝学 2 2016-03-26 2016-03-26106417 神曲 2 2016-04-15 2016-04-1510883 灵域 1 2016-04-15 2016-04-15103646 铁血皇城 3 2016-04-19 2016-04-19104046 铁骑三国 39 2016-04-19 2016-04-19104238 传奇盛世 3 2016-04-28 2016-04-28Time taken: 0.054 seconds, Fetched: 7 row(s) 删除分区1ALTER TABLE IGCT_dynamic DROP IF EXISTS PARTITION (addtime='2016-02-27'); 查看数据123456789hive&gt; select * from IGCT_dynamic2;OK1041 九阴绝学 2 2016-03-26 2016-03-26106417 神曲 2 2016-04-15 2016-04-1510883 灵域 1 2016-04-15 2016-04-15103646 铁血皇城 3 2016-04-19 2016-04-19104046 铁骑三国 39 2016-04-19 2016-04-19104238 传奇盛世 3 2016-04-28 2016-04-28Time taken: 0.062 seconds, Fetched: 6 row(s)]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2016 Summary]]></title>
    <url>%2F2017%2F01%2F02%2F2016%20Summary%2F</url>
    <content type="text"><![CDATA[转眼已经过了一年，前些日子累的不行，元旦昏睡了两日养足精神。来到实验室发现几天前买的风信子意外地开花了，淡紫色。那就写一篇简短的年终总结吧。 收获2016年没有什么意外地考上了研究生应该算是一个不小的收获。研究生确是完全不一样的学习时光，不禁想想本科真的是浪费生命。996的时间安排非常紧凑，好在实验室有一群有趣的小伙伴，生活也并不枯燥。暑假就被拉过来做项目，虽然困难重重，现在还没有上线，但至少实实在在地开始干了。慢慢发现敲代码也并不是那么无聊，特别是功能实现的时候，还是充满喜悦。不过自己还是经验少，不太会变通，往往一个问题卡一大段时间。 遗憾一个不小的遗憾应该就是没有去看成逼哥的跨年了吧。前几天看微博上看到跨年的信息还是满满的遗憾，今年还请了崔健当嘉宾，场馆也变成了奥体，看来真的是越来越出名了。谈了一场龙卷风似的恋爱，和歌里写的一模一样，也就对抢票不上心了，反正没人陪。 出去骑车的日子越来越少了。限于实验室的作息时间，休息日也只有一天，非常可惜。待在室内久了真的会压抑，希望接下来能多挤时间出去骑骑车，或者去别的地方看看也好。曾经脑子一热就骑车回家的做法真的太有趣了。 说好的周末抽时间学编曲荒废了实在可惜。本想做个爱好，偶尔能够写出一二首小歌，聊聊当下，多么快活。希望今年能够开始学。 之前考研的暑假有和小伙伴一起健身了一两个月。每天学习一天，抽一小时出来运动健身，这样的状态真的很不错。效果先撇开不说，自己的精神状态会好很多。多运动，少打游戏，做个阳光少年。 自己对于时间的合理安排还是欠缺，接下来需要好好得对自己的每周每日的时间做好规划。前些日子一个博主讲到自己的时间规划就是把当天的安排做完再休息，这也不失为一个好办法。对我这中拖延症患者，时间规划真的太重要了。 展望前些天看了一个学长的面试经历视频，发现自己不足之处还很多。要用研究生的时间补上浪费的四年的本科时光。现在开始起码每周研究一个算法，做一道面试题。 学到的东西还是要多用。最近的王菲「幻乐一场」演唱会争议很大，又是走音又是天价票，台下十年功，并不无道理，三年后再重返舞台绝对不是一回事。 2017让自己的生活丰富一点，对自己的规划明确一点。]]></content>
      <categories>
        <category>生活感悟</category>
      </categories>
      <tags>
        <tag>总结</tag>
        <tag>日常</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce程序异常捕获存储]]></title>
    <url>%2F2016%2F12%2F16%2FMapReduce%E7%A8%8B%E5%BA%8F%E5%BC%82%E5%B8%B8%E6%8D%95%E8%8E%B7%E5%AD%98%E5%82%A8%2F</url>
    <content type="text"><![CDATA[在Map或reduce中使用multipleOutput来进行异常存储：123456789try&#123; ... ... context.write(newKey,newValue);&#125;catch(Exception e)&#123; multipleOutput.write(new Text( null == e.getMessage()? ("error:"): e.getMessage),new Text(value.toString()),"_error/part"); e.printStackTrace();&#125;]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce应用中CombineFileInputFormat原理与用法]]></title>
    <url>%2F2016%2F12%2F12%2FMapReduce%E5%BA%94%E7%94%A8%E4%B8%ADCombineFileInputFormat%E5%8E%9F%E7%90%86%E4%B8%8E%E7%94%A8%E6%B3%95%2F</url>
    <content type="text"><![CDATA[转载自：http://ju.outofmemory.cn/entry/72024 HDFS本身被设计来存储大文件，但是有时难免会有小文件出现，有时很可能时大量的小文件。通过MapReduce处理大量小文件时会遇到些问题。 MapReduce程序会将输入的文件进行分片(Split)，每个分片对应一个map任务，而默认一个文件至少有一个分片，一个分片也只属于一个文件。这样大量的小文件会导致大量的map任务，导致资源过度消耗，且效率低下。Hadoop自身包含了CombineFileInputFormat，其功能是将多个小文件合并如一个分片，由一个map任务处理，这样就减少了不必要的map数量。 在了解CombineFileInputFormat之前，我们应了解其父类FileInputFormat的基本处理逻辑。注意这里的FileInputFormat的路径是org.apache.hadoop.mapreduce.lib.input.FileInputFormat，是新的MapReduce API。 mapred包下的FileInputFormat对应老的API，不再推荐使用。 FileInputFormat的基本处理逻辑FileInputFormat是基于文件的InputFormat的抽象基类，如上图所示，基于文件的衍生类有很多，如文本文件TextInputFormat，序列文件SequenceFileInputFormat等。 FileInputFormat提供了分片的基本实现getSplits(JobContext)，其子类可以重写isSplitable(JobContext, Path)方法，来使得输入的一个或多个文件是否不做分片，完整由被一个Map任务进行处理。 FileInputFormat有如下特点： 将以下划线&#8221;_&#8221;或点&#8221;.&#8221;开头的文件作为隐藏文件，不做为输入文件。 所有文件isSplitable(JobContext, Path)都是true，但是针对如果输入文件时压缩的、流式的，那么子类中应重新该函数，判断是否真的可以分片。 由于每个文件块大小可能不一样，所以每个文件分别计算分片大小，计算规则如下： 取值A： FormatMinSplitSize，本Format设置的最小Split大小，通过GetFormatMinSplitSize()获取，此类中定义为1个字节。 MinSplitSize，配置文件(配置键mapreduce.input.fileinputformat.split.minsize)或者直接设置，通过GetMinSplitSize()获取)，未设置则为0。 取两者较大值。 取值B: MaxSplitSize，通过配置文件设置或者SetMaxSplitSize()设置，通过GetMaxSplitSize()获取，无设置则取LONG.MAXVALUE。 文件块大小BLOCKSIZE 取两者较小值 再取A、B的较大值。 例：常用的TextInputFormat类中，没有对分片算法进行重写，那么我们可以认为，使用TextInputFormat时，在未做其他设置的情况下，默认分片大小等于BLOCKSIZE，如果设置了mapreduce.input.fileinputformat.split.minsize，则取其与BLOCK的较大值。 分片大小确定后，就将文件进行依次划分。 文本类型TextInputFormat的使用如上我们知道TextInputFormat是FileInputFormat的子类，其自定义内容很少，通过它可以大致知道扩展FileInputFormat大致需要做些什么。整个类如下： public class TextInputFormat extends FileInputFormat&amp;lt;LongWritable, Text&amp;gt; { @Override public RecordReader&amp;lt;LongWritable, Text&amp;gt; createRecordReader(InputSplit split, TaskAttemptContext context) { String delimiter = context.getConfiguration().get( &amp;quot;textinputformat.record.delimiter&amp;quot; ); byte[] recordDelimiterBytes = null; if ( null != delimiter) recordDelimiterBytes = delimiter.getBytes(Charsets. UTF_8); return new LineRecordReader(recordDelimiterBytes); } @Override protected boolean isSplitable(JobContext context, Path file) { final CompressionCodec codec = new CompressionCodecFactory(context.getConfiguration()).getCodec(file); if ( null == codec) { return true; } return codec instanceof SplittableCompressionCodec; } } 分片的方式采用的是父类FileInputFormat的逻辑，上文中已经说明。重写了isSplitable()，根据文本文件的压缩属性来判断是否可以进行分片。 而createRecorderReader()是定义文本文件的读取方式，实际文件读取是通过它返回的RecordReader&lt;LongWritable, Text&gt;类实现的。 这样，在整个输入文件读取过程，大致会涉及如下几个步骤： 指定输入文件路径，如FileInputFormat.addInputPaths(job, args[0]) 指定文件的处理类型，如job.setInputFormatClass(MyInputFormat. class) 在这个InputFormatClass内部，考虑: 是否可以进行分片 isSplitable(JobContext context, Path file) 如何分片 List&lt;InputSplit&gt; getSplits(JobContext job) 如何读取分片中的记录 RecordReader&lt;LongWritable, Text&gt; createRecordReader(InputSplit split,TaskAttemptContext context) 以上确定后，用户开发的Map任务中就可以直接处理每一条记录KeyValue。 这些步骤下，我们基本就可以理解TextInputFormat如何被使用了。其他类型的InputFormat子类，其流程步骤也基本与上相同，可以重写相关的类方法来实现不同处理方式。 从TextInputFormat中的分片逻辑(FileInputFormat的getSplits)中可以确定，分片都是针对单个文件而言的，如果文件本身较小，没有达到一个分片大小，那么每个小文件都是一个分片。而一个分片就对应一个Map任务。如果有大量的小文件作为Map的输入，那么其会导致生成大量Map任务，造成处理的缓慢、资源的浪费，如何减少map任务的数量提高处理效率呢？CombineInputFormat就是为解决这样的问题。 3.CombineInputFormat原理与用法CombineInputFormat的功能，是将一个目录（可能包括多个小文件，不包括子目录）作为一个map的输入，而不是通常使用一个文件作为输入。 CombineInputFormat本身是个抽象类，要使用它，涉及： 1)CombineFileSplit 我们的目标是使得一个split不是属于一个文件，而是可能包含多个文件，所以这里不再使用常用的FileSplit，而是CombineFileSplit，包括了各个文件的路径、长度、读的起始位置等信息。CombineFileSplit是CombineInputFormat中getSplits()的对象类型。 2)CombineInputFormat 核心处理类 2.1)其基本思想： 分片从指定路径下的多个文件构建，不同文件可以放入不同的pool，一个分片只能包含一个pool中的文件，可以包括多个文件的Block。pool其实是针对文件进行了逻辑划分，不同的pool中的文件分别进行分片。分片的逻辑如下文所示。 2.2)分片的逻辑： 如果指定了maxSplitSize(&#8220;mapreduce.input.fileinputformat.split.maxsize&#8221;)，那么在同一个节点上的Blocks合并，一个超过maxSplitSize就生成新分片。如果没有指定，则只汇总本节点BLock，暂不分片。 如果指定了minSizeNode(&#8220;mapreduce.input.fileinputformat.split.minsize.per.node&#8221;),那么会把1.中处理剩余的Block，进行合并，如果超过minSizeNode，那么全部作为一个分片。否则这些Block与同一机架Rack上的块进行合并。 每个节点上如上同样的方式处理，然后针对整个Rack的所有Block，按照1.方式处理。剩余部分，如果指定了minSizeRack(&#8220;mapreduce.input.fileinputformat.split.minsize.per.rack&#8221;)，并且超过minSizeRack，则全部作为一个分片，否则这些Block保留，等待与所有机架上的剩余Block进行汇总处理。 每个机架上都按照1，2，3方式处理，汇总所有处理剩下的部分，再按照1的逻辑处理。再剩余的，作为一个分片。 以上逻辑我们可以知道： 如果只设置maxSplitSize(如job.getConfiguration().set( &#8220;mapreduce.input.fileinputformat.split.maxsize&#8221; , &#8220;33554432&#8243;))，那么基本每个分片大小都需凑满maxSplitSize。 如果maxSplitSize，minSizeNode，minSizeRack三个都没有设置，那是所有输入整合成一个分片！ 3)CombineFileRecordReader 针对一个CombineFileSplit分片的通用RecordReader。CombineFileSplit中包含多个文件的块信息，CombineFileRecordReader是文件层面的处理，例如何时切换到分片中的下一个文件，而单个文件的处理，则需要自定义RecordReader的子类，读取文件的记录。 hadoop自带的示例应用org.apache.hadoop.examples.MultiFileWordCount，用到了CombineInputFormat，其处理流程： 要使用CombineInputFormat进行应用开发，可以参考org.apache.hadoop.examples.MultiFileWordCount中使用方式，需要自行实现CombineFileInputFormat的子类与实际读取逐条记录的RecordReader子类。 而MultiFileWordCount的使用如下：12345678910111213141516171819202122232425262728293031323334353637shell&amp;gt; hadoop fs -ls -h /tmp/carl/2013-07-12/ Found 4 items -rw-r&amp;#8211;r&amp;#8211; 3 hdfs hadoop 246.6 M 2013-12-03 13:07 /tmp/carl/2013-07-12/000059_0 -rw-r&amp;#8211;r&amp;#8211; 3 hdfs hadoop 244.9 M 2013-12-03 13:11 /tmp/carl/2013-07-12/000124_0 -rw-r&amp;#8211;r&amp;#8211; 3 hdfs hadoop 244.9 M 2013-12-03 13:15 /tmp/carl/2013-07-12/000126_0 -rw-r&amp;#8211;r&amp;#8211; 3 hdfs hadoop 85.2 M 2013-12-03 13:17 /tmp/carl/2013-07-12/000218_0shell&amp;gt; hadoop jar hadoop-mapreduce-examples-2.0.0-cdh4.4.0.jar multifilewc /tmp/carl/2013-07-12/ /tmp/carl/c8/ &amp;#8230; 13/12/24 19:31:23 INFO input.FileInputFormat: Total input paths to process : 4 13/12/24 19:31:23 INFO mapreduce.JobSubmitter: number of splits:1 &amp;#8230; Job Counters Killed reduce tasks=1 Launched map tasks=1 Launched reduce tasks=17 Other local map tasks=1 Total time spent by all maps in occupied slots (ms)=37923 Total time spent by all reduces in occupied slots (ms)=1392681 ... 因为MultiFileWordCount没有设置maxSplitSize，所以这里只有一个分片。 4.CombineInputFormat应用4.1.使用场景CombineInputFormat处理少量，较大的文件没有优势，相反，如果没有合理的设置maxSplitSize，minSizeNode，minSizeRack，则可能会导致一个map任务需要大量访问非本地的Block造成网络开销，反而比正常的非合并方式更慢。 而针对大量远小于块大小的小文件处理，CombineInputFormat的使用还是很有优势。 4.2.测试我们以hadoop的示例程序WordCount和MulitFileWordCount来处理1000个小文件为例进行对比。 生成小文件：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697shell&amp;gt; cat file_gen.sh #!/bin/sh for i in $(seq 1000) do echo &amp;#8220;abc asdf as df asd f sadf werweiro &amp;#8220;$i &amp;gt; file_$&#123;i&#125; done shell&amp;gt; ./file_gen.sh shell&amp;gt; ls -lh &amp;#8230; -rw-r&amp;#8211;r&amp;#8211; 1 root root 40 Dec 25 10:14 file_962 -rw-r&amp;#8211;r&amp;#8211; 1 root root 40 Dec 25 10:14 file_963 -rw-r&amp;#8211;r&amp;#8211; 1 root root 40 Dec 25 10:14 file_964 -rw-r&amp;#8211;r&amp;#8211; 1 root root 40 Dec 25 10:14 file_965 -rw-r&amp;#8211;r&amp;#8211; 1 root root 40 Dec 25 10:14 file_966 -rw-r&amp;#8211;r&amp;#8211; 1 root root 40 Dec 25 10:14 file_967 &amp;#8230;上面生成大量小文件，上传这些小文件：shell&amp;gt; hadoop fs -put file* /tmp/carl2/**wordcount**使用TextInputFormat方式，小文件一个个处理：shell&amp;gt; hadoop jar hadoop-mapreduce-examples-2.0.0-cdh4.4.0.jar wordcount /tmp/carl2/ /tmp/carl_result3/ &amp;#8230; 13/12/25 10:34:25 INFO input.FileInputFormat: Total input paths to process : 1000 13/12/25 10:34:27 INFO mapreduce.JobSubmitter: number of splits:1000 &amp;#8230; 13/12/25 10:34:33 INFO mapreduce.Job: map 0% reduce 0% 13/12/25 10:34:40 INFO mapreduce.Job: map 1% reduce 0% 13/12/25 10:34:41 INFO mapreduce.Job: map 2% reduce 0% &amp;#8230; 13/12/25 10:40:56 INFO mapreduce.Job: map 100% reduce 94% 13/12/25 10:40:57 INFO mapreduce.Job: map 100% reduce 100% &amp;#8230;可以看到，生成了1000个map任务，总耗时超过6分钟!**multifilewc**使用CombineInputFormat方式，没有设置maxSplitSize的情况下，所有小文件会汇总成一个Split。shell&amp;gt; hadoop jar hadoop-mapreduce-examples-2.0.0-cdh4.4.0.jar multifilewc /tmp/carl2/ /tmp/carl_result4/ &amp;#8230; 13/12/25 10:42:04 INFO input.FileInputFormat: Total input paths to process : 1000 13/12/25 10:42:07 INFO mapreduce.JobSubmitter: number of splits:1 &amp;#8230; 13/12/25 10:42:12 INFO mapreduce.Job: map 0% reduce 0% 13/12/25 10:42:19 INFO mapreduce.Job: map 100% reduce 0% 13/12/25 10:42:24 INFO mapreduce.Job: map 100% reduce 25% 13/12/25 10:42:25 INFO mapreduce.Job: map 100% reduce 31% 13/12/25 10:42:27 INFO mapreduce.Job: map 100% reduce 38% 13/12/25 10:42:28 INFO mapreduce.Job: map 100% reduce 56% 13/12/25 10:42:29 INFO mapreduce.Job: map 100% reduce 63% 13/12/25 10:42:30 INFO mapreduce.Job: map 100% reduce 69% 13/12/25 10:42:31 INFO mapreduce.Job: map 100% reduce 81% 13/12/25 10:42:32 INFO mapreduce.Job: map 100% reduce 88% 13/12/25 10:42:33 INFO mapreduce.Job: map 100% reduce 100% 可以看到，只用一个map任务进行处理，大量小文件可以使用网络迅速的汇总，总耗时不到30秒！ 测试的结果可以大致看出，针对大量小文件，使用CombineInputFormat具有较大优势。 参考： http://stackoverflow.com/questions/14541759/how-can-i-work-with-large-number-of-small-files-in-hadoop]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce 递归子目录和合并小文件]]></title>
    <url>%2F2016%2F12%2F12%2FMapReduce%20%E9%80%92%E5%BD%92%E5%AD%90%E7%9B%AE%E5%BD%95%E5%92%8C%E5%90%88%E5%B9%B6%E5%B0%8F%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[递归子目录设置mapreduce.input.fileinputformat.input.dir.recursive=true，这个参数是客户端参数，可以在MapReduce中设置，也可以在mapred-site.xml中设置.在mapreduce程序中如12// 递归子目录job.getConfiguration().setBoolean("mapreduce.input.fileinputformat.input.dir.recursive",true); CombineTextInputFormat 合并小文件1234567//设置split大小job.getConfiguration().setLong("mapreduce.input.fileinputformat.split.maxsize", 128 * 1024 * 1024);//job.setInputFormatClass(TextInputFormat.class); // 合并小文件job.setInputFormatClass(CombineTextInputFormat.class);]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java获取与当天相差几天的日期两种方式]]></title>
    <url>%2F2016%2F12%2F09%2FJava%E8%8E%B7%E5%8F%96%E4%B8%8E%E5%BD%93%E5%A4%A9%E7%9B%B8%E5%B7%AE%E5%87%A0%E5%A4%A9%E7%9A%84%E6%97%A5%E6%9C%9F%E4%B8%A4%E7%A7%8D%E6%96%B9%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[12345678Date date=new Date();//取时间 Calendar calendar = new GregorianCalendar(); calendar.setTime(date); calendar.add(calendar.DATE,1);//把日期往后增加一天.整数往后推,负数往前移动 date=calendar.getTime(); //这个时间就是日期往后推一天的结果 SimpleDateFormat formatter = new SimpleDateFormat("yyyy-MM-dd"); String dateString = formatter.format(date); System.out.println(dateString); 12345678910/** *获取两日期之间天数 */ public String getDate(Date d,long i)&#123; SimpleDateFormat df=new SimpleDateFormat("yyyy-MM-dd"); /*System.out.println("今天的日期："+df.format(d)); System.out.println("两天前的日期：" + df.format(new Date(d.getTime() - 2 * 24 * 60 * 60 * 1000))); System.out.println("三天后的日期：" + df.format(new Date(d.getTime() + 3 * 24 * 60 * 60 * 1000)));*/ return df.format(new Date(d.getTime() + i * 24 * 60 * 60 * 1000)); &#125;]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Log4j配置]]></title>
    <url>%2F2016%2F12%2F09%2FLog4j%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[加入log4j-1.2.8.jar到lib下。 在CLASSPATH下建立log4j.properties。 在bin目录下加入 log4j.properties 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178#①配置根Logger，其语法为： # #log4j.rootLogger = [level],appenderName,appenderName2,... #level是日志记录的优先级，分为OFF,TRACE,DEBUG,INFO,WARN,ERROR,FATAL,ALL ##Log4j建议只使用四个级别，优先级从低到高分别是DEBUG,INFO,WARN,ERROR #通过在这里定义的级别，您可以控制到应用程序中相应级别的日志信息的开关 #比如在这里定义了INFO级别，则应用程序中所有DEBUG级别的日志信息将不被打印出来 #appenderName就是指定日志信息输出到哪个地方。可同时指定多个输出目的 ################################################################################ ################################################################################ #②配置日志信息输出目的地Appender，其语法为： # #log4j.appender.appenderName = fully.qualified.name.of.appender.class #log4j.appender.appenderName.optionN = valueN # #Log4j提供的appender有以下几种： #1)org.apache.log4j.ConsoleAppender(输出到控制台) #2)org.apache.log4j.FileAppender(输出到文件) #3)org.apache.log4j.DailyRollingFileAppender(每天产生一个日志文件) #4)org.apache.log4j.RollingFileAppender(文件大小到达指定尺寸的时候产生一个新的文件) #5)org.apache.log4j.WriterAppender(将日志信息以流格式发送到任意指定的地方) # #1)ConsoleAppender选项属性 # -Threshold = DEBUG:指定日志消息的输出最低层次 # -ImmediateFlush = TRUE:默认值是true,所有的消息都会被立即输出 # -Target = System.err:默认值System.out,输出到控制台(err为红色,out为黑色) # #2)FileAppender选项属性 # -Threshold = INFO:指定日志消息的输出最低层次 # -ImmediateFlush = TRUE:默认值是true,所有的消息都会被立即输出 # -File = C:\log4j.log:指定消息输出到C:\log4j.log文件 # -Append = FALSE:默认值true,将消息追加到指定文件中，false指将消息覆盖指定的文件内容 # -Encoding = UTF-8:可以指定文件编码格式 # #3)DailyRollingFileAppender选项属性 # -Threshold = WARN:指定日志消息的输出最低层次 # -ImmediateFlush = TRUE:默认值是true,所有的消息都会被立即输出 # -File = C:\log4j.log:指定消息输出到C:\log4j.log文件 # -Append = FALSE:默认值true,将消息追加到指定文件中，false指将消息覆盖指定的文件内容 # -DatePattern='.'yyyy-ww:每周滚动一次文件,即每周产生一个新的文件。还可以按用以下参数: # '.'yyyy-MM:每月 # '.'yyyy-ww:每周 # '.'yyyy-MM-dd:每天 # '.'yyyy-MM-dd-a:每天两次 # '.'yyyy-MM-dd-HH:每小时 # '.'yyyy-MM-dd-HH-mm:每分钟 # -Encoding = UTF-8:可以指定文件编码格式 # #4)RollingFileAppender选项属性 # -Threshold = ERROR:指定日志消息的输出最低层次 # -ImmediateFlush = TRUE:默认值是true,所有的消息都会被立即输出 # -File = C:/log4j.log:指定消息输出到C:/log4j.log文件 # -Append = FALSE:默认值true,将消息追加到指定文件中，false指将消息覆盖指定的文件内容 # -MaxFileSize = 100KB:后缀可以是KB,MB,GB.在日志文件到达该大小时,将会自动滚动.如:log4j.log.1 # -MaxBackupIndex = 2:指定可以产生的滚动文件的最大数 # -Encoding = UTF-8:可以指定文件编码格式 ################################################################################ ################################################################################ #③配置日志信息的格式(布局)，其语法为： # #log4j.appender.appenderName.layout = fully.qualified.name.of.layout.class #log4j.appender.appenderName.layout.optionN = valueN # #Log4j提供的layout有以下几种： #5)org.apache.log4j.HTMLLayout(以HTML表格形式布局) #6)org.apache.log4j.PatternLayout(可以灵活地指定布局模式) #7)org.apache.log4j.SimpleLayout(包含日志信息的级别和信息字符串) #8)org.apache.log4j.TTCCLayout(包含日志产生的时间、线程、类别等等信息) #9)org.apache.log4j.xml.XMLLayout(以XML形式布局) # #5)HTMLLayout选项属性 # -LocationInfo = TRUE:默认值false,输出java文件名称和行号 # -Title=Struts Log Message:默认值 Log4J Log Messages # #6)PatternLayout选项属性 # -ConversionPattern = %m%n:格式化指定的消息(参数意思下面有) # #9)XMLLayout选项属性 # -LocationInfo = TRUE:默认值false,输出java文件名称和行号 # #Log4J采用类似C语言中的printf函数的打印格式格式化日志信息，打印参数如下： # %m 输出代码中指定的消息 # %p 输出优先级，即DEBUG,INFO,WARN,ERROR,FATAL # %r 输出自应用启动到输出该log信息耗费的毫秒数 # %c 输出所属的类目,通常就是所在类的全名 # %t 输出产生该日志事件的线程名 # %n 输出一个回车换行符，Windows平台为“\r\n”，Unix平台为“\n” # %d 输出日志时间点的日期或时间，默认格式为ISO8601，也可以在其后指定格式 # 如：%d&#123;yyyy年MM月dd日 HH:mm:ss,SSS&#125;，输出类似：2012年01月05日 22:10:28,921 # %l 输出日志事件的发生位置，包括类目名、发生的线程，以及在代码中的行数 # 如：Testlog.main(TestLog.java:10) # %F 输出日志消息产生时所在的文件名称 # %L 输出代码中的行号 # %x 输出和当前线程相关联的NDC(嵌套诊断环境),像java servlets多客户多线程的应用中 # %% 输出一个"%"字符 # # 可以在%与模式字符之间加上修饰符来控制其最小宽度、最大宽度、和文本的对齐方式。如： # %5c: 输出category名称，最小宽度是5，category&lt;5，默认的情况下右对齐 # %-5c:输出category名称，最小宽度是5，category&lt;5，"-"号指定左对齐,会有空格 # %.5c:输出category名称，最大宽度是5，category&gt;5，就会将左边多出的字符截掉，&lt;5不会有空格 # %20.30c:category名称&lt;20补空格，并且右对齐，&gt;30字符，就从左边交远销出的字符截掉 ################################################################################ ################################################################################ #④指定特定包的输出特定的级别 #log4j.logger.org.springframework=DEBUG ################################################################################ #OFF,systemOut,logFile,logDailyFile,logRollingFile,logMail,logDB,ALL log4j.rootLogger =ALL,systemOut,logFile,logDailyFile,logRollingFile,logMail,logDB #输出到控制台 log4j.appender.systemOut = org.apache.log4j.ConsoleAppender log4j.appender.systemOut.layout = org.apache.log4j.PatternLayout log4j.appender.systemOut.layout.ConversionPattern = [%-5p][%-22d&#123;yyyy/MM/dd HH:mm:ssS&#125;][%l]%n%m%n log4j.appender.systemOut.Threshold = DEBUG log4j.appender.systemOut.ImmediateFlush = TRUE log4j.appender.systemOut.Target = System.out #输出到文件 log4j.appender.logFile = org.apache.log4j.FileAppender log4j.appender.logFile.layout = org.apache.log4j.PatternLayout log4j.appender.logFile.layout.ConversionPattern = [%-5p][%-22d&#123;yyyy/MM/dd HH:mm:ssS&#125;][%l]%n%m%n log4j.appender.logFile.Threshold = DEBUG log4j.appender.logFile.ImmediateFlush = TRUE log4j.appender.logFile.Append = TRUE log4j.appender.logFile.File = ../Struts2/WebRoot/log/File/log4j_Struts.log log4j.appender.logFile.Encoding = UTF-8 #按DatePattern输出到文件 log4j.appender.logDailyFile = org.apache.log4j.DailyRollingFileAppender log4j.appender.logDailyFile.layout = org.apache.log4j.PatternLayout log4j.appender.logDailyFile.layout.ConversionPattern = [%-5p][%-22d&#123;yyyy/MM/dd HH:mm:ssS&#125;][%l]%n%m%n log4j.appender.logDailyFile.Threshold = DEBUG log4j.appender.logDailyFile.ImmediateFlush = TRUE log4j.appender.logDailyFile.Append = TRUE log4j.appender.logDailyFile.File = ../Struts2/WebRoot/log/DailyFile/log4j_Struts log4j.appender.logDailyFile.DatePattern = '.'yyyy-MM-dd-HH-mm'.log' log4j.appender.logDailyFile.Encoding = UTF-8 #设定文件大小输出到文件 log4j.appender.logRollingFile = org.apache.log4j.RollingFileAppender log4j.appender.logRollingFile.layout = org.apache.log4j.PatternLayout log4j.appender.logRollingFile.layout.ConversionPattern = [%-5p][%-22d&#123;yyyy/MM/dd HH:mm:ssS&#125;][%l]%n%m%n log4j.appender.logRollingFile.Threshold = DEBUG log4j.appender.logRollingFile.ImmediateFlush = TRUE log4j.appender.logRollingFile.Append = TRUE log4j.appender.logRollingFile.File = ../Struts2/WebRoot/log/RollingFile/log4j_Struts.log log4j.appender.logRollingFile.MaxFileSize = 1MB log4j.appender.logRollingFile.MaxBackupIndex = 10 log4j.appender.logRollingFile.Encoding = UTF-8 #用Email发送日志 log4j.appender.logMail = org.apache.log4j.net.SMTPAppender log4j.appender.logMail.layout = org.apache.log4j.HTMLLayout log4j.appender.logMail.layout.LocationInfo = TRUE log4j.appender.logMail.layout.Title = Struts2 Mail LogFile log4j.appender.logMail.Threshold = DEBUG log4j.appender.logMail.SMTPDebug = FALSE log4j.appender.logMail.SMTPHost = SMTP.163.com log4j.appender.logMail.From = xly3000@163.com log4j.appender.logMail.To = xly3000@gmail.com #log4j.appender.logMail.Cc = xly3000@gmail.com #log4j.appender.logMail.Bcc = xly3000@gmail.com log4j.appender.logMail.SMTPUsername = xly3000 log4j.appender.logMail.SMTPPassword = 1234567 log4j.appender.logMail.Subject = Log4j Log Messages #log4j.appender.logMail.BufferSize = 1024 #log4j.appender.logMail.SMTPAuth = TRUE #将日志登录到MySQL数据库 log4j.appender.logDB = org.apache.log4j.jdbc.JDBCAppender log4j.appender.logDB.layout = org.apache.log4j.PatternLayout log4j.appender.logDB.Driver = com.mysql.jdbc.Driver log4j.appender.logDB.URL = jdbc:mysql://127.0.0.1:3306/xly log4j.appender.logDB.User = root log4j.appender.logDB.Password = 123456 log4j.appender.logDB.Sql = INSERT INTOT_log4j(project_name,create_date,level,category,file_name,thread_name,line,all_category,message)values('Struts2','%d&#123;yyyy-MM-ddHH:mm:ss&#125;','%p','%c','%F','%t','%L','%l','%m')]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mapreduce调优]]></title>
    <url>%2F2016%2F12%2F02%2Fmapreduce%E8%B0%83%E4%BC%98%2F</url>
    <content type="text"><![CDATA[对应用程序进行调优 避免输入大量小文件。大量的小文件(不足一个block大小)作为输入数据会产生很多的Map任务(默认一个分片对应一个Map任务)，而每个Map任务实际工作量又非常小，系统要花更多的时间来将这些Map任务的输出进行整合。如果将大量的小文件进行预处理合并成一个或几个大文件，任务执行的效率可能会提升几十倍。可手动将小文件合并成大文件，或通过Hadoop的SequenceFile、CombineFileInputFormat将多个文件打包到一个输入单元中，使得每个Map处理更多的数据，从而提高性能。 输入文件size巨大，但不是小文件。这种情况可以通过增大每个mapper的input size，即增大minSize或者增大blockSize来减少所需的mapper的数量。增大blockSize通常不可行，因为当HDFS被hadoop namenode -format之后，blockSize就已经确定了（由格式化时dfs.block.size决定），如果要更改blockSize，需要重新格式化HDFS，这样当然会丢失已有的数据。所以通常情况下只能通过增大minSize，即增大mapred.min.split.size的值。 预判并过滤无用数据。可以使用一些过滤工具，在作业执行之前将数据中无用的数据进行过滤，可极大提高MapReduce执行效率。Bloom Filter是一种功能强大的过滤器，执行效率高，时间复杂度为O(1)，缺点是存在一定的误判可能，详细参考《Bloom Filter概念和原理》。当将一个非常大的表和一个非常小的表进行表连接操作时，可以使用Bloom Filter将小表数据作为Bloom Filter的输入数据，将大表的原始数据进行过滤(过滤不通过的数据一定是不可用的，过滤通过的数据可能有用可能无用)，可提高程序执行的效率。 合理使用分布式缓存DistributedCache。DistributedCache可以将一些字典、jar包、配置文件等缓存到需要执行map任务的节点中，避免map任务多次重复读取这些资源，尤其在join操作时，使用DistributedCache缓存小表数据在map端进行join操作，可避免shuffle、reduce等操作，提高程序运行效率。 重用Writable类型。避免大量多次new这些Writable对象，这会花费java垃圾收集器大量的清理工作，建议在map函数外定义这些Writable对象，如下所示： 12345678910class MyMapper … &#123; Text wordText = new Text(); IntWritable one = new IntWritable(1); public void map(...) &#123; for (String word: words) &#123; wordText.set(word); context.write(wordText, one); &#125; &#125;&#125; 合理设置Combiner。Combine阶段处于Map端操作的最后一步，设置Combine操作可大大提高MapReduce的执行效率，前提是增加Combine不能改变最终的结果值，换句话说，不是所有的MapReduce程序都能添加Combine，如求平均数的MapReduce程序就不适合设置Combine操作。通常Combine函数与Reduce函数一致 对参数进行调优（基于hadoop2.6.0）HDFS参数调优(hdfs-site.xml) dfs.namenode.handler.count：namenode用于处理RPC的线程数，默认值10，可根据NameNode所在节点机器配置适当调大，如32、64； dfs.datanode.handler.count：datanode上用于处理RPC的线程数，2.6版本默认值10，早期1.x版本默认值为3，可根据datanode节点的配置适当调整； MapReduce参数调优(mapred-site.xml) mapreduce.tasktracker.map.tasks.maximum：每个nodemanager节点上可运行的最大map任务数，默认值2，可根据实际值调整为10~100； mapreduce.tasktracker.reduce.tasks.maximum：每个nodemanager节点上可运行的最大reduce任务数，默认值2，可根据实际值调整为10~100； mapreduce.output.fileoutputformat.compress：是否对任务输出产生的结果进行压缩，默认值false。对传输数据进行压缩，既可以减少文件的存储空间，又可以加快数据在网络不同节点之间的传输速度。 mapreduce.output.fileoutputformat.compress.type：输出产生任务数据的压缩方式，默认值RECORD，可配置值有：NONE、RECORD、BLOCK mapreduce.map.output.compress：map端压缩 mapreduce.map.output.compress.codec：map压缩格式 mapreduce.task.io.sort.mb：map任务输出结果的内存环形缓冲区大小，默认值100M，可根据map节点的机器进行配置，貌似不能超过值mapred.child.java.opts； mapreduce.map.sort.spill.percent：map任务输出环形缓冲区大小溢写触发最大比例，默认值80%，这个值一般不建议修改； mapreduce.reduce.shuffle.parallelcopies：reduce节点通过http拷贝map输出结果数据到本地的最大工作线程数，默认值5，可根据节点机器配置适当修改； mapreduce.reduce.shuffle.input.buffer.percent：reduce节点在shuffle阶段拷贝map输出结果数据到本地时，内存缓冲区大小所占JVM内存的比例，默认值0.7，一般不建议修改； mapreduce.reduce.shuffle.merge.percent：reduce节点shuffle内存缓冲区溢写触发最大比例，默认值0.66，一般不建议修改； mapred.child.java.opts：配置每个map或reduce使用的内存数量，默认值-Xmx200m，即200M。如果nodemanager所在节点 Map和Reduce个数设置 map的数量map的数量通常是由hadoop集群的DFS块大小确定的，也就是输入文件的总块数，正常的map数量的并行规模大致是每一个Node是10~100个，对于CPU消耗较小的作业可以设置Map数量为300个左右，但是由于hadoop的没一个任务在初始化时需要一定的时间，因此比较合理的情况是每个map执行的时间至少超过1分钟。具体的数据分片是这样的，InputFormat在默认情况下会根据hadoop集群的DFS块大小进行分片，每一个分片会由一个map任务来进行处理，当然用户还是可以通过参数mapred.min.split.size参数在作业提交客户端进行自定义设置。还有一个重要参数就是mapred.map.tasks，这个参数设置的map数量仅仅是一个提示，只有当InputFormat 决定了map任务的个数比mapred.map.tasks值小时才起作用。同样，Map任务的个数也能通过使用JobConf 的conf.setNumMapTasks(int num)方法来手动地设置。这个方法能够用来增加map任务的个数，但是不能设定任务的个数小于Hadoop系统通过分割输入数据得到的值。因此，如果你有一个大小是10TB的输入数据，并设置DFS块大小为 128M，你必须设置至少82K个map任务，除非你设置的mapred.map.tasks参数比这个数还要大。当然为了提高集群的并发效率，可以设置一个默认的map数量，当用户的map数量较小或者比本身自动分割的值还小时可以使用一个相对交大的默认值，从而提高整体hadoop集群的效率。 reduece的数量reduce在运行时往往需要从相关map端复制数据到reduce节点来处理，因此相比于map任务。reduce节点资源是相对比较缺少的，同时相对运行较慢，正确的reduce任务的个数应该是0.95或者1.75 *（节点数 ×mapred.tasktracker.tasks.maximum参数值）。mapred.tasktracker.tasks.reduce.maximum的数量一般设置为各节点cpu core数量,或者数量减1，即能同时计算的slot数量。如果任务数是节点个数的0.95倍，那么所有的reduce任务能够在 map任务的输出传输结束后同时开始运行。如果任务数是节点个数的1.75倍，那么高速的节点会在完成他们第一批reduce任务计算之后开始计算第二批 reduce任务，这样的情况更有利于负载均衡。同时需要注意增加reduce的数量虽然会增加系统的资源开销，但是可以改善负载匀衡，降低任务失败带来的负面影响。同样，Reduce任务也能够与 map任务一样，通过设定JobConf 的conf.setNumReduceTasks(int num)方法来增加任务个数。cpu数量 = 服务器CPU总核数 / 每个CPU的核数服务器CPU总核数 = more /proc/cpuinfo | grep ‘processor’ | wc -l每个CPU的核数 = more /proc/cpuinfo | grep ‘cpu cores’ reduce数量为0有些作业不需要进行归约进行处理，那么就可以设置reduce的数量为0来进行处理，这种情况下用户的作业运行速度相对较高，map的输出会直接写入到 SetOutputPath(path)设置的输出目录，而不是作为中间结果写到本地。同时Hadoop框架在写入文件系统前并不对之进行排序。 参考转载 http://www.cnblogs.com/hanganglin/p/4563716.htmlhttps://my.oschina.net/Chanthon/blog/150500]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Jena学习笔记（二） SPARQL]]></title>
    <url>%2F2016%2F11%2F29%2FJena%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89%20SPARQL%2F</url>
    <content type="text"><![CDATA[官方描述：Apache Jena（或简称Jena）是一个用于构建语义Web和关联数据应用程序的自由和开源的Java框架。 该框架由不同的API组成，用于处理RDF数据。 Jena是一个用于Java语义Web应用程序的API（应用程序编程接口）。它不是一个程序或工具，如果这是你正在寻找，我建议或许TopBraid Composer作为一个好的选择。因此，Jena的主要用途是帮助您编写处理RDF和OWL文档和描述的Java代码。 SPARQL是用于访问由W3C RDF数据访问工作组设计的RDF的查询语言和协议。 作为一种查询语言，SPARQL是“数据导向的”，因为它只查询模型中保存的信息;在查询语言本身没有推理。当然，Jena模型是“聪明的”，因为它提供了某些三元组存在的印象，即按需创建它们，包括OWL推理。除了以查询的形式获取应用程序想要的描述外，SPARQL不执行任何操作，并以一组bindings或RDF图形的形式返回该信息。 官方网站：http://jena.apache.org/index.htmlSPARQL教程：http://jena.apache.org/tutorials/sparql.html 设置环境变量1234567891011Setting up your EnvironmentAn environment variable JENAROOT is used by all the command line tools to configure the class path automatically for you. You can set this up as follows:On Linux / Macexport JENAROOT=the directory you downloaded Jena toexport PATH=$PATH:$JENAROOT/binOn WindowsSET JENAROOT=the directory you downloaded Jena toSET PATH=%PATH%;%JENAROOT%\bat 数据格式首先，我们需要清楚查询要查询的数据。 SPARQL查询RDF图。 RDF图是一组三元组（Jena调用RDF图“模型”和三元组“语句”，因为这是他们在第一次设计Jena API时调用的）。 重要的是要意识到三元组的重要性，而不是序列化。序列化只是一种写三元组的方式。 RDF / XML是W3C的建议，但是可能很难看到序列化形式的三元组，因为有多种方法来编码同一个图。在本教程中，我们使用了一个更像“三元组”的序列化，称为Turtle（另请参阅W3C语义网络引文中描述的N3语言）。 我们将从vc-db-1.rdf中的简单数据开始：此文件包含用于多个vCard人员描述的RDF。 vCard在RFC2426中描述，并且RDF翻译在W3C笔记“在RDF / XML中表示vCard对象”中描述。我们的示例数据库只包含一些名称信息。 图形上，数据看起来像： 在三元组中，如下所示：1234567891011121314151617181920212223242526272829303132333435@prefix vCard: &lt;http://www.w3.org/2001/vcard-rdf/3.0#&gt; .@prefix rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt; .@prefix : &lt;#&gt; .&lt;http://somewhere/MattJones/&gt; vCard:FN "Matt Jones" ; vCard:N [ vCard:Family "Jones" ; vCard:Given "Matthew" ] .&lt;http://somewhere/RebeccaSmith/&gt; vCard:FN "Becky Smith" ; vCard:N [ vCard:Family "Smith" ; vCard:Given "Rebecca" ] .&lt;http://somewhere/JohnSmith/&gt; vCard:FN "John Smith" ; vCard:N [ vCard:Family "Smith" ; vCard:Given "John" ] .&lt;http://somewhere/SarahJones/&gt; vCard:FN "Sarah Jones" ; vCard:N [ vCard:Family "Jones" ; vCard:Given "Sarah" ] . 更加明确的：1234567891011121314151617181920212223@prefix vCard: &lt;http://www.w3.org/2001/vcard-rdf/3.0#&gt; .@prefix rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt; .&lt;http://somewhere/MattJones/&gt; vCard:FN "Matt Jones" .&lt;http://somewhere/MattJones/&gt; vCard:N _:b0 ._:b0 vCard:Family "Jones" ._:b0 vCard:Given "Matthew" .&lt;http://somewhere/RebeccaSmith/&gt; vCard:FN "Becky Smith" .&lt;http://somewhere/RebeccaSmith/&gt; vCard:N _:b1 ._:b1 vCard:Family "Smith" ._:b1 vCard:Given "Rebecca" .&lt;http://somewhere/JohnSmith/&gt; vCard:FN "John Smith" .&lt;http://somewhere/JohnSmith/&gt; vCard:N _:b2 ._:b2 vCard:Family "Smith" ._:b2 vCard:Given "John" .&lt;http://somewhere/SarahJones/&gt; vCard:FN "Sarah Jones" .&lt;http://somewhere/SarahJones/&gt; vCard:N _:b3 ._:b3 vCard:Family "Jones" ._:b3 vCard:Given "Sarah" . 重要的是要意识到这些是相同的RDF图，并且图中的三元组没有特定的顺序。计算机不在乎其顺序。 第一个SPARQL查询看一个简单的查询并展示如何使用Jena执行它。12SELECT ?xWHERE &#123; ?x &lt;http://www.w3.org/2001/vcard-rdf/3.0#FN&gt; "John Smith" &#125; 用命令行查询应用程序执行所述查询:12345---------------------------------| x |=================================| &lt;http://somewhere/JohnSmith/&gt; |--------------------------------- 这通过将WHERE子句中的三元模式与RDF图中的三元组进行匹配来实现。三元组的谓词和对象是固定值，因此模式将只匹配与这些值的三元组。主体是一个变量，并且对变量没有其他限制。模式匹配任何三元组与这些谓词和对象值，它匹配x的结果。 &lt;&gt;中包含的项目是一个URI（实际上是一个IRI），而包含在“”中的项目是一个普通的字面量。就像Turtle，N3或N-triples一样，输入的文字用\ ^ \ ^编写，语言标签可以用@添加。 ？x是一个称为x的变量。？不会形成名称的一部分，这就是为什么它不会出现在表的输出中。该查询返回x查询变量中的匹配项。所示的输出是通过一条ARQ的命令获得的。 执行查询12345678910Windows setupExecute:bat\sparql.bat --data=doc\Tutorial\vc-db-1.rdf --query=doc\Tutorial\q1.rqYou can just put the bat/ directory on your classpath or copy the programs out of it.bash scripts for Linux/Cygwin/UnixExecute:bin/sparql --data=doc/Tutorial/vc-db-1.rdf --query=doc/Tutorial/q1.rq 在Linux中执行结果 获取所有人的FullName12SELECT ?x ?fnameWHERE &#123;?x &lt;http://www.w3.org/2001/vcard-rdf/3.0#FN&gt; ?fname&#125; 12345678910//执行ubuntu@master:~/paper/doc$ sparql --data=vc-db-1.rdf --query=q-bp1.rq----------------------------------------------------| x | fname |====================================================| &lt;http://somewhere/JohnSmith/&gt; | "John Smith" || &lt;http://somewhere/SarahJones/&gt; | "Sarah Jones" || &lt;http://somewhere/MattJones/&gt; | "Matt Jones" || &lt;http://somewhere/RebeccaSmith/&gt; | "Becky Smith" |---------------------------------------------------- 指定前缀123456ubuntu@master:~/paper/doc$ cat q2.rq PREFIX vCard: &lt;http://www.w3.org/2001/vcard-rdf/3.0#&gt;SELECT ?xWHERE &#123; ?x vCard:FN "John Smith" &#125; 123456ubuntu@master:~/paper/doc$ sparql --data=vc-db-1.rdf --query=q2.rq---------------------------------| x |=================================| &lt;http://somewhere/JohnSmith/&gt; |---------------------------------]]></content>
      <categories>
        <category>paper</category>
      </categories>
      <tags>
        <tag>Jena</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Jena学习笔记（一） RDF]]></title>
    <url>%2F2016%2F11%2F26%2FJena%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89%20RDF%2F</url>
    <content type="text"><![CDATA[官方描述：Apache Jena（或简称Jena）是一个用于构建语义Web和关联数据应用程序的自由和开源的Java框架。 该框架由不同的API组成，用于处理RDF数据。 Jena是一个用于Java语义Web应用程序的API（应用程序编程接口）。它不是一个程序或工具，如果这是你正在寻找，我建议或许TopBraid Composer作为一个好的选择。因此，Jena的主要用途是帮助您编写处理RDF和OWL文档和描述的Java代码。 官方网站：http://jena.apache.org/index.htmlRDF API教程：http://jena.apache.org/tutorials/rdf_api.html RDF定义资源描述框架（RDF）是最初设计为元数据数据模型的万维网联盟（W3C）规范[1]的家族。 它已经被用作用于概念描述或者在web资源中实现的信息的建模的一般方法，使用各种语法符号和数据串行化格式。 它也用于知识管理应用程序中。RDF数据模型类似于诸如实体关系或类图的经典概念建模方法，因为它基于以主语谓词对象的形式对资源（特别是web资源）进行语句表。这些表达式在RDF术语中称为三元组。主语表示资源，谓词表示资源的特征或方面，并且表示主语和对象之间的关系。例如，在RDF中表示“天空具有蓝色”的概念的一种方式是作为三元组：表示“天空”的主题，表示“具有颜色”的谓词和表示“蓝色”的对象。因此，RDF将面向对象设计中的实体 - 属性 - 值模型的经典符号中使用的对象交换对象;实体（天空），属性（颜色）和值（蓝色）。 RDF是具有几种串行格式（即，文件格式）的抽象模型，因此资源或三元组被编码的特定方式随格式而变化。 下载Jena并从Eclipse建立工程，导入jar RDF API 教程 http://jena.apache.org/tutorials/rdf_api.html 建立如下RDF图并打印1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950package com.hdu.rdf;import org.apache.jena.rdf.model.Model;import org.apache.jena.rdf.model.ModelFactory;import org.apache.jena.rdf.model.Property;import org.apache.jena.rdf.model.RDFNode;import org.apache.jena.rdf.model.Resource;import org.apache.jena.rdf.model.Statement;import org.apache.jena.rdf.model.StmtIterator;import org.apache.jena.vocabulary.VCARD;public class Tutorial2AddProperty &#123; public static void main(String[] args) &#123; // some definitions String personURI = "http://somewhere/JohnSmith"; String givenName = "John"; String familyName = "Smith"; String fullName = givenName + " " + familyName; // create an empty model Model model = ModelFactory.createDefaultModel(); // create the resource // and add the properties cascading style Resource johnSmith = model.createResource(personURI).addProperty(VCARD.FN, fullName).addProperty(VCARD.N, model.createResource().addProperty(VCARD.Given, givenName).addProperty(VCARD.Family, familyName)); // model.write(System.out); // list the statements in the graph StmtIterator iter = model.listStatements(); // print out the predicate, subject and object of each statement while (iter.hasNext()) &#123; Statement stmt = iter.nextStatement(); Resource subject = stmt.getSubject(); // get the subject Property predicate = stmt.getPredicate(); // get the predicate RDFNode object = stmt.getObject(); // get the object System.out.print(subject.toString()); System.out.print(" " + predicate.toString() + " "); if (object instanceof Resource) &#123; System.out.print(object.toString()); &#125; else &#123; // object is a literal System.out.print(" \"" + object.toString() + "\""); &#125; System.out.println(" ."); &#125; &#125;&#125; 结果1234567891011121314151617181920//iterator遍历 每行包含三个字段，表示每个语句的主题，谓词和对象http://somewhere/JohnSmith http://www.w3.org/2001/vcard-rdf/3.0#N anon:14df86:ecc3dee17b:-7fff .anon:14df86:ecc3dee17b:-7fff http://www.w3.org/2001/vcard-rdf/3.0#Family "Smith" .anon:14df86:ecc3dee17b:-7fff http://www.w3.org/2001/vcard-rdf/3.0#Given "John" .http://somewhere/JohnSmith http://www.w3.org/2001/vcard-rdf/3.0#FN "John Smith" .//write输出 xml格式&lt;rdf:RDF xmlns:rdf='http://www.w3.org/1999/02/22-rdf-syntax-ns#' xmlns:vcard='http://www.w3.org/2001/vcard-rdf/3.0#' &gt; &lt;rdf:Description rdf:about='http://somewhere/JohnSmith'&gt; &lt;vcard:FN&gt;John Smith&lt;/vcard:FN&gt; &lt;vcard:N rdf:nodeID="A0"/&gt; &lt;/rdf:Description&gt; &lt;rdf:Description rdf:nodeID="A0"&gt; &lt;vcard:Given&gt;John&lt;/vcard:Given&gt; &lt;vcard:Family&gt;Smith&lt;/vcard:Family&gt; &lt;/rdf:Description&gt;&lt;/rdf:RDF&gt; 说明 他的RDF规范指定如何将RDF表示为XML。 RDF XML语法相当复杂。读者参考由RDFCore WG开发的引物以进行更详细的介绍。但是，让我们快速看看如何解释上面的。 RDF通常嵌入在元素中。如果有其他方式知道某些XML是RDF，但是它通常存在，那么该元素是可选的。 RDF元素定义文档中使用的两个命名空间。然后是一个元素，描述其URI为“http：// somewhere / JohnSmith”的资源。如果缺少rdf：about属性，则此元素将表示空白节点。 元素描述了资源的属性。属性名称是vcard命名空间中的“FN”。 RDF通过连接命名空间前缀的URI引用和名称的本地名称部分的“FN”，将其转换为URI引用。这提供了一个URI引用”http://www.w3.org/2001/vcard-rdf/3.0#FN&quot;。属性的值是文字&quot;John Smith”。 元素是一个资源。在这种情况下，资源由相对URI引用表示。 RDF通过将其与当前文档的基本URI连接，将其转换为绝对URI引用。 此RDF XML中有错误;它不完全代表我们创建的模型。模型中的空白节点已经被赋予URI引用。它不再是空白。 RDF / XML语法不能表示所有RDF模型;例如它不能表示作为两个语句的对象的空白节点。我们用来编写这个RDF / XML的’dumb’作者没有尝试正确地写入可以正确写入的Models子集。它为每个空白节点提供一个URI，使其不再为空。 12345// now write the model in XML form to a filemodel.write(System.out, "RDF/XML-ABBREV");// now write the model in N-TRIPLES form to a filemodel.write(System.out, "N-TRIPLES"); 123456789101112131415&lt;rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:vcard="http://www.w3.org/2001/vcard-rdf/3.0#"&gt; &lt;rdf:Description rdf:about="http://somewhere/JohnSmith"&gt; &lt;vcard:N rdf:parseType="Resource"&gt; &lt;vcard:Family&gt;Smith&lt;/vcard:Family&gt; &lt;vcard:Given&gt;John&lt;/vcard:Given&gt; &lt;/vcard:N&gt; &lt;vcard:FN&gt;John Smith&lt;/vcard:FN&gt; &lt;/rdf:Description&gt;&lt;/rdf:RDF&gt;&lt;http://somewhere/JohnSmith&gt; &lt;http://www.w3.org/2001/vcard-rdf/3.0#N&gt; _:BX2D71b38d8eX3A1589596bd63X3AX2D7fff .&lt;http://somewhere/JohnSmith&gt; &lt;http://www.w3.org/2001/vcard-rdf/3.0#FN&gt; "John Smith" ._:BX2D71b38d8eX3A1589596bd63X3AX2D7fff &lt;http://www.w3.org/2001/vcard-rdf/3.0#Family&gt; "Smith" ._:BX2D71b38d8eX3A1589596bd63X3AX2D7fff &lt;http://www.w3.org/2001/vcard-rdf/3.0#Given&gt; "John" . 读取RDF1234567891011121314151617static final String inputFileName = "vc-db-1.rdf"; // create an empty model Model model = ModelFactory.createDefaultModel(); // use the FileManager to find the input file InputStream in = FileManager.get().open( inputFileName );if (in == null) &#123; throw new IllegalArgumentException( "File: " + inputFileName + " not found");&#125;// read the RDF/XML filemodel.read(in, null);// write it to standard outmodel.write(System.out); 读取结果12345678910111213141516171819202122232425262728293031323334353637&lt;rdf:RDF xmlns:rdf='http://www.w3.org/1999/02/22-rdf-syntax-ns#' xmlns:vcard='http://www.w3.org/2001/vcard-rdf/3.0#' &gt; &lt;rdf:Description rdf:nodeID="A0"&gt; &lt;vcard:Family&gt;Smith&lt;/vcard:Family&gt; &lt;vcard:Given&gt;John&lt;/vcard:Given&gt; &lt;/rdf:Description&gt; &lt;rdf:Description rdf:about='http://somewhere/JohnSmith/'&gt; &lt;vcard:FN&gt;John Smith&lt;/vcard:FN&gt; &lt;vcard:N rdf:nodeID="A0"/&gt; &lt;/rdf:Description&gt; &lt;rdf:Description rdf:about='http://somewhere/SarahJones/'&gt; &lt;vcard:FN&gt;Sarah Jones&lt;/vcard:FN&gt; &lt;vcard:N rdf:nodeID="A1"/&gt; &lt;/rdf:Description&gt; &lt;rdf:Description rdf:about='http://somewhere/MattJones/'&gt; &lt;vcard:FN&gt;Matt Jones&lt;/vcard:FN&gt; &lt;vcard:N rdf:nodeID="A2"/&gt; &lt;/rdf:Description&gt; &lt;rdf:Description rdf:nodeID="A3"&gt; &lt;vcard:Family&gt;Smith&lt;/vcard:Family&gt; &lt;vcard:Given&gt;Rebecca&lt;/vcard:Given&gt; &lt;/rdf:Description&gt; &lt;rdf:Description rdf:nodeID="A1"&gt; &lt;vcard:Family&gt;Jones&lt;/vcard:Family&gt; &lt;vcard:Given&gt;Sarah&lt;/vcard:Given&gt; &lt;/rdf:Description&gt; &lt;rdf:Description rdf:nodeID="A2"&gt; &lt;vcard:Family&gt;Jones&lt;/vcard:Family&gt; &lt;vcard:Given&gt;Matthew&lt;/vcard:Given&gt; &lt;/rdf:Description&gt; &lt;rdf:Description rdf:about='http://somewhere/RebeccaSmith/'&gt; &lt;vcard:FN&gt;Becky Smith&lt;/vcard:FN&gt; &lt;vcard:N rdf:nodeID="A3"/&gt; &lt;/rdf:Description&gt;&lt;/rdf:RDF&gt; Jena RDF包Jena是用于语义Web应用程序的Java API。应用程序开发人员的关键RDF包是org.apache.jena.rdf.model。 API已经根据接口进行了定义，以便应用程序代码可以在不改变的情况下与不同的实现工作。此包包含用于表示模型，资源，属性，文字，语句和RDF的所有其他关键概念的接口，以及用于创建模型的ModelFactory。所以应用程序代码保持独立的实现，最好是尽可能使用接口，而不是特定的类实现。 org.apache.jena.tutorial包包含本教程中使用的所有示例的工作源代码。 org.apache.jena … impl包包含可能对许多实现通用的实现类。例如，它们定义类ResourceImpl，PropertyImpl和LiteralImpl，它们可以被不同的实现直接使用或子类化。应用程序应该很少，如果有的话，直接使用这些类。例如，不是创建ResourceImpl的新实例，最好使用正在使用的任何模型的createResource方法。这样，如果模型实现使用了Resource的优化实现，则两种类型之间不需要转换。 浏览模型给定资源的URI，可以使用Model.getResource（String uri）方法从模型中检索资源对象。 此方法定义为返回一个Resource对象（如果模型中存在），否则创建一个新对象。1234567891011121314151617181920212223242526272829303132333435363738394041static final String inputFileName = "vc-db-1.rdf"; static final String johnSmithURI = "http://somewhere/JohnSmith/"; public static void main (String args[]) &#123; // create an empty model Model model = ModelFactory.createDefaultModel(); // use the FileManager to find the input file InputStream in = FileManager.get().open(inputFileName); if (in == null) &#123; throw new IllegalArgumentException( "File: " + inputFileName + " not found"); &#125; // read the RDF/XML file model.read(new InputStreamReader(in), ""); // retrieve the Adam Smith vcard resource from the model Resource vcard = model.getResource(johnSmithURI); // retrieve the value of the N property Resource name = (Resource) vcard.getRequiredProperty(VCARD.N) .getObject(); // retrieve the given name property String fullName = vcard.getRequiredProperty(VCARD.FN) .getString(); // add two nick name properties to vcard // 获取johnSmithURI资源后，对其添加两个nickname属性 vcard.addProperty(VCARD.NICKNAME, "Smithy") .addProperty(VCARD.NICKNAME, "Adman"); // set up the output System.out.println("The nicknames of \"" + fullName + "\" are:"); // list the nicknames //返回声明迭代器，获取对象 StmtIterator iter = vcard.listProperties(VCARD.NICKNAME); while (iter.hasNext()) &#123; System.out.println(" " + iter.nextStatement().getObject() .toString()); &#125; &#125; 123The nicknames of "John Smith" are: Adman Smithy 查询模型上一节讨论从具有已知URI的资源导航模型的情况。 本节介绍搜索模型。 核心Jena API仅支持有限的查询原语。 更强大的查询功能在SPARQL中。 Model.listStatements（）方法列出了模型中的所有语句，也许是查询模型的最粗糙的方法。 它的使用不推荐在非常大的模型。 Model.listSubjects（）是类似的，但返回一个迭代器在所有具有属性的资源，即一些语句的主题。 Model.listSubjectsWithProperty（Property p，RDFNode o）将返回所有资源的迭代器，这些资源具有值为o的属性p。 如果我们假设只有vcard资源会有vcard：FN属性，并且在我们的数据中，所有这些资源都有这样的属性，那么我们可以找到所有的vcards： 12345678910111213141516171819202122232425262728static final String inputFileName = "vc-db-1.rdf"; public static void main (String args[]) &#123; // create an empty model Model model = ModelFactory.createDefaultModel(); // use the FileManager to find the input file InputStream in = FileManager.get().open(inputFileName); if (in == null) &#123; throw new IllegalArgumentException( "File: " + inputFileName + " not found"); &#125; // read the RDF/XML file model.read( in, ""); // select all the resources with a VCARD.FN property ResIterator iter = model.listResourcesWithProperty(VCARD.FN); if (iter.hasNext()) &#123; System.out.println("The database contains vcards for:"); while (iter.hasNext()) &#123; System.out.println(" " + iter.nextResource() .getRequiredProperty(VCARD.FN) .getString() ); &#125; &#125; else &#123; System.out.println("No vcards were found in the database"); &#125; &#125; 12345The database contains vcards for: Sarah Jones John Smith Becky Smith Matt Jones 模型操作Jena提供了操作模型作为一个整体的三个操作,分別是并集，交集和差的运算。 两个模型的并集是表示每个模型的语句集合的合并。这是RDF设计支持的关键操作之一。它允许合并来自不同数据源的数据。 考虑以下两个模型： 当这些被合并时，两个http：//…JohnSmith节点被合并成一个，并且vcard：FN 弧被丢弃以产生： 12345678910111213141516171819202122232425262728static final String inputFileName1 = "vc-db-3.rdf"; static final String inputFileName2 = "vc-db-4.rdf"; public static void main (String args[]) &#123; // create an empty model Model model1 = ModelFactory.createDefaultModel(); Model model2 = ModelFactory.createDefaultModel(); // use the class loader to find the input file InputStream in1 = FileManager.get().open(inputFileName1); if (in1 == null) &#123; throw new IllegalArgumentException( "File: " + inputFileName1 + " not found"); &#125; InputStream in2 = FileManager.get().open(inputFileName2); if (in2 == null) &#123; throw new IllegalArgumentException( "File: " + inputFileName2 + " not found"); &#125; // read the RDF/XML files model1.read( in1, "" ); model2.read( in2, "" ); // merge the graphs Model model = model1.union(model2); // print the graph as RDF/XML model.write(System.out, "RDF/XML-ABBREV"); System.out.println(); 12345678910111213141516&lt;rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:vcard="http://www.w3.org/2001/vcard-rdf/3.0#"&gt; &lt;rdf:Description rdf:about="http://somewhere/JohnSmith/"&gt; &lt;vcard:N rdf:parseType="Resource"&gt; &lt;vcard:Given&gt;John&lt;/vcard:Given&gt; &lt;vcard:Family&gt;Smith&lt;/vcard:Family&gt; &lt;/vcard:N&gt; &lt;vcard:EMAIL&gt; &lt;vcard:internet&gt; &lt;rdf:value&gt;John@somewhere.com&lt;/rdf:value&gt; &lt;/vcard:internet&gt; &lt;/vcard:EMAIL&gt; &lt;vcard:FN&gt;John Smith&lt;/vcard:FN&gt; &lt;/rdf:Description&gt;&lt;/rdf:RDF&gt; 容器RDF定义了一种特殊的资源来表示事物的集合。 这些资源称为容器。 容器的成员可以是文字或资源。 有三种容器： BAG是一个无序的集合ALT是旨在表示可选的无序集合SEQ是有序集合容器由资源表示。 该资源将有一个rdf：type属性，其值应为rdf：Bag，rdf：Alt或rdf：Seq之一，或其中一个的子类，具体取决于容器的类型。 容器的第一个成员是容器的rdf：_1属性的值; 容器的第二个成员是容器的rdf：_2属性的值，等等。 rdf：_nnn属性被称为序数属性。 例如，包含Smith的vcards的简单包的模型可能如下所示：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051static final String inputFileName = "vc-db-1.rdf"; public static void main (String args[]) &#123; // create an empty model Model model = ModelFactory.createDefaultModel(); // use the class loader to find the input file InputStream in = FileManager.get().open( inputFileName ); if (in == null) &#123; throw new IllegalArgumentException( "File: " + inputFileName + " not found"); &#125; // read the RDF/XML file model.read(new InputStreamReader(in), ""); // create a bag Bag smiths = model.createBag(); // select all the resources with a VCARD.FN property // whose value ends with "Smith" StmtIterator iter = model.listStatements( new SimpleSelector(null, VCARD.FN, (RDFNode) null) &#123; @Override public boolean selects(Statement s) &#123; return s.getString().endsWith("Smith"); &#125; &#125;); // add the Smith's to the bag while (iter.hasNext()) &#123; smiths.add( iter.nextStatement().getSubject()); &#125; // print the graph as RDF/XML model.write(new PrintWriter(System.out)); System.out.println(); // print out the members of the bag NodeIterator iter2 = smiths.iterator(); if (iter2.hasNext()) &#123; System.out.println("The bag contains:"); while (iter2.hasNext()) &#123; System.out.println(" " + ((Resource) iter2.next()) .getRequiredProperty(VCARD.FN) .getString()); &#125; &#125; else &#123; System.out.println("The bag is empty"); &#125; &#125; 123456789101112131415161718192021222324252627282930313233343536373839404142&lt;rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:vcard="http://www.w3.org/2001/vcard-rdf/3.0#"&gt; &lt;rdf:Description rdf:about="http://somewhere/SarahJones/"&gt; &lt;vcard:N rdf:parseType="Resource"&gt; &lt;vcard:Given&gt;Sarah&lt;/vcard:Given&gt; &lt;vcard:Family&gt;Jones&lt;/vcard:Family&gt; &lt;/vcard:N&gt; &lt;vcard:FN&gt;Sarah Jones&lt;/vcard:FN&gt; &lt;/rdf:Description&gt; &lt;rdf:Bag&gt; &lt;rdf:li&gt; &lt;rdf:Description rdf:about="http://somewhere/RebeccaSmith/"&gt; &lt;vcard:N rdf:parseType="Resource"&gt; &lt;vcard:Given&gt;Rebecca&lt;/vcard:Given&gt; &lt;vcard:Family&gt;Smith&lt;/vcard:Family&gt; &lt;/vcard:N&gt; &lt;vcard:FN&gt;Becky Smith&lt;/vcard:FN&gt; &lt;/rdf:Description&gt; &lt;/rdf:li&gt; &lt;rdf:li&gt; &lt;rdf:Description rdf:about="http://somewhere/JohnSmith/"&gt; &lt;vcard:N rdf:parseType="Resource"&gt; &lt;vcard:Given&gt;John&lt;/vcard:Given&gt; &lt;vcard:Family&gt;Smith&lt;/vcard:Family&gt; &lt;/vcard:N&gt; &lt;vcard:FN&gt;John Smith&lt;/vcard:FN&gt; &lt;/rdf:Description&gt; &lt;/rdf:li&gt; &lt;/rdf:Bag&gt; &lt;rdf:Description rdf:about="http://somewhere/MattJones/"&gt; &lt;vcard:N rdf:parseType="Resource"&gt; &lt;vcard:Given&gt;Matthew&lt;/vcard:Given&gt; &lt;vcard:Family&gt;Jones&lt;/vcard:Family&gt; &lt;/vcard:N&gt; &lt;vcard:FN&gt;Matt Jones&lt;/vcard:FN&gt; &lt;/rdf:Description&gt;&lt;/rdf:RDF&gt;The bag contains: Becky Smith John Smith 词汇表空白节点（Blank Node）表示资源，但不指示资源的URI。空白节点表现为一阶逻辑中存在的合格变量。Dublin Core关于Web资源的元数据标准。更多信息可以在Dublin Core网站上找到。文字（Literal）可以是属性值的字符串。对象（Object）三元组的一部分，即语句的值。谓词（Predicate）三元组的属性部分。属性（Property）属性是资源的属性。例如DC.title是一个属性，和RDF.type一样。资源（Resource）一些实体。它可以是诸如网页的web资源，或者它可以是具体的物理事物，例如树或汽车。它可以是一个抽象的想法，如象棋或足球。资源由URI命名。声明（Statement）RDF模型中的弧，通常被解释为事实。主语Subject）作为RDF模型中的弧源的资源三元组（Triple）包含主语，谓词和对象的结构。语句的另一个术语。 脚注RDF资源的标识符可以包括片段标识符，例如， http：// hostname / rdf / tutorial /＃ch-简介，因此，严格地说，RDF资源由URI引用标识。除了作为一个字符串，字面量还有一个可选的语言编码来表示字符串的语言。例如，文字“two”对于英语可能具有“en”的语言编码，而对于法国，文字“deux”可能具有“fr”的语言编码。]]></content>
      <categories>
        <category>paper</category>
      </categories>
      <tags>
        <tag>Jena</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Distributed Cache在mapreduce中读取小文件]]></title>
    <url>%2F2016%2F11%2F26%2FDistributed%20Cache%E5%9C%A8mapreduce%E4%B8%AD%E8%AF%BB%E5%8F%96%E5%B0%8F%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[Distributed Cache 在 MapReduce 任务中应用很广， 它可以大大提高一些被频繁读取文件的访问速度。被添加到 Distributed Cache 的文件会被拷贝到 Mapper 和 Reducer 的运行目录中。 在job添加如下方法 12remoteReGamePath为hdfs文件路径字符串job.addCacheFile(new Path(remoteReGamePath).toUri()); 以下例子为在map中读取此文件并存入集合123456789101112131415161718192021222324252627282930313233343536373839404142private Set&lt;String&gt; recommendGame = new HashSet&lt;String&gt;();/** * 读取推荐游戏文件 * * @param uri */ private void readReGame(URI uri) &#123; try &#123; Path patternsPath = new Path(uri.getPath()); String patternsFileName = patternsPath.getName().toString(); BufferedReader reader = new BufferedReader(new FileReader( patternsFileName)); String line; while ((line = reader.readLine()) != null) &#123; // TODO: your code here // recommendGame.add(line.split(",")[0]); &#125; reader.close(); &#125; catch (FileNotFoundException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; @Override protected void setup( Mapper&lt;LongWritable, Text, Text, Text&gt;.Context context) throws IOException, InterruptedException &#123; // TODO Auto-generated method stub super.setup(context); //获取cache uri URI[] uri = context.getCacheFiles(); readReGame(uri[0]); &#125;]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mapreduce多目录输出笔记]]></title>
    <url>%2F2016%2F11%2F26%2Fmapreduce%E5%A4%9A%E7%9B%AE%E5%BD%95%E8%BE%93%E5%87%BA%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[使用MultipleOutputs实现多目录/文件输出org.apache.hadoop.mapreduce.lib.output.MultipleOutputs在map或者reduce类中加入如下方法1234567891011121314151617private MultipleOutputs&lt;Text, NullWritable&gt; mos; @Override protected void setup(Reducer&lt;Text, NullWritable, Text, NullWritable&gt;.Context context) throws IOException, InterruptedException &#123; // TODO Auto-generated method stub super.setup(context); mos = new MultipleOutputs&lt;Text, NullWritable&gt;(context);// 初始化mos &#125; @Override protected void cleanup(Reducer&lt;Text, NullWritable, Text, NullWritable&gt;.Context context) throws IOException, InterruptedException &#123; // TODO Auto-generated method stub super.cleanup(context); mos.close(); &#125; 在需要输出数据的地方，可以使用定义好的 mos 进行输出123mos.write("outputName", key, value);mos.write("outputName", key, value, "filePrefix"); mos.write("outputName", key, value, "path/filePrefix");//到文件夹 在Job Driver 时定义一些 Named Output1234MultipleOutputs.addNamedOutput(job, "outputXName", XXXOutputFormat.class, OutputXKey.class, OutputXValue.class);MultipleOutputs.addNamedOutput(job, "outputYName", YYYOutputFormat.class, OutputYKey.class, OutputYValue.class); 取消类似part-r-00000的空文件LazyOutputFormat.setOutputFormatClass(job, TextOutputFormat.class)例子123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140package com.hdu.recommend.mr;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import org.apache.hadoop.mapreduce.lib.output.LazyOutputFormat;import org.apache.hadoop.mapreduce.lib.output.MultipleOutputs;import org.apache.hadoop.mapreduce.lib.output.NullOutputFormat;import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;import org.apache.hadoop.yarn.conf.YarnConfiguration; * @author Skye * */public class DataCleanIconAndWeb &#123; public static class QLMapper extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt; &#123; private String webGame = "网页游戏"; Text outputValue = new Text(); // 设置多文件输出 private MultipleOutputs&lt;Text,NullWritable&gt; mos; @Override protected void setup(Mapper&lt;LongWritable, Text, Text, NullWritable&gt;.Context context) throws IOException, InterruptedException &#123; // TODO Auto-generated method stub super.setup(context); mos = new MultipleOutputs&lt;Text, NullWritable&gt;(context);// 初始化mos &#125; @Override protected void cleanup(Mapper&lt;LongWritable, Text, Text, NullWritable&gt;.Context context) throws IOException, InterruptedException &#123; // TODO Auto-generated method stub super.cleanup(context); mos.close(); &#125; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 接收数据v1 String line = value.toString(); // 切分数据 String[] words = line.split(""); // String[] words = line.split("\t"); boolean isWeb = false; boolean flag = true; //一系列处理代码 //*** //*** //*** String action = words[1] + "\t" + words[0] + "\t" + words[2] + "\t" + words[3] + "\t" + words[5]; outputValue.set(action); mos.write("iconRecord", outputValue, NullWritable.get(),"iconRecord/icon"); String action = words[1] + "\t" + words[0] + "\t" + words[2] + "\t" + words[3] + "\t" + words[4] + "\t" + words[5]; outputValue.set(action); mos.write( "webRecord",outputValue, NullWritable.get(),"webRecord/web"); &#125; &#125; public static void run(String originalDataPath, String dataCleanOutputFile) throws Exception &#123; // 构建Job对象 Configuration conf = new Configuration(); Job job = Job.getInstance(conf); // 注意：main方法所在的类 job.setJarByClass(DataCleanIconAndWeb.class); job.getConfiguration().setBoolean("mapreduce.output.fileoutputformat.compress", false); job.getConfiguration().setStrings( "mapreduce.reduce.shuffle.input.buffer.percent", "0.1"); job.getConfiguration().setBoolean("mapreduce.output.fileoutputformat.compress", false); job.setNumReduceTasks(3); // 设置Mapper相关属性 job.setMapperClass(QLMapper.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(NullWritable.class); FileInputFormat.setInputPaths(job, new Path(originalDataPath)); // 设置Reducer相关属性 job.setOutputKeyClass(Text.class); job.setOutputValueClass(NullWritable.class); FileOutputFormat.setOutputPath(job, new Path(dataCleanOutputFile)); MultipleOutputs.addNamedOutput(job, "iconRecord", TextOutputFormat.class, Text.class, NullWritable.class); MultipleOutputs.addNamedOutput(job, "webRecord", TextOutputFormat.class, Text.class, NullWritable.class); // 文件格式 job.setInputFormatClass(TextInputFormat.class); //取消part-r-00000新式文件输出 LazyOutputFormat.setOutputFormatClass(job, TextOutputFormat.class); //job.setOutputFormatClass(TextOutputFormat.class); // 提交任务 job.waitForCompletion(true); long endTime = System.currentTimeMillis(); &#125; &#125; 参考http://gnailuy.com/dataplatform/2015/11/22/common-techniques-for-mapreduce/http://blog.csdn.net/zgc625238677/article/details/51524786https://www.iteblog.com/archives/848]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hive数据查询导出]]></title>
    <url>%2F2016%2F11%2F26%2Fhive%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%85%A5%2F</url>
    <content type="text"><![CDATA[hive数据查询导出123456insert overwrite directory '/user/hdu/recommend/gameRecommendNew4/test11.26/gameprestep1'row format delimitedfields terminated by '\t'SELECT userid , gamename , COUNT(*) AS count , MAX(gamestarttime) AS lasttimeFROM userdetailtwoGROUP BY userid , gamename 出错FAILED: ParseException line 2:0 cannot recognize input near &#39;row&#39; &#39;format&#39; &#39;delimited&#39; in statement 原因This is because the hive query will by default use the ^ as the delimiter. You can try the same by exporting to local file system.That should be supported. 解决create an external table to the location where you want your output file.Use create table as command and insert the required data into the external table.By that you will get the data in the HDFS location12345678910create external table user_game_count_lasttime2(userid STRING ,gamename STRING,count INT,lasttime STRING)ROW FORMAT DELIMITEDFIElDS TERMINATED BY '\t'STORED AS TEXTFILELOCATION '/user/hdu/recommend/gameRecommendNew4/test11.26/gameprestep1_2';insert overwrite table user_game_count_lasttime2 SELECT userid , gamename , COUNT(*) AS count , MAX(gamestarttime) AS lasttimeFROM userdetailtwoGROUP BY userid , gamename;]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop Job相关命令]]></title>
    <url>%2F2016%2F11%2F05%2Fhadoop%20Job%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[hadoop命令行 与job相关的：命令行工具 •1.查看 Job 信息：hadoop job -list2.杀掉 Job：hadoop job -kill job_id3.指定路径下查看历史日志汇总：hadoop job -history output-dir4.作业的更多细节：hadoop job -history all output-dir5.打印map和reduce完成百分比和所有计数器：hadoop job -status job_id6.杀死任务。被杀死的任务不会不利于失败尝试：hadoop jab -kill-task 7.使任务失败。被失败的任务会对失败尝试不利：hadoop job -fail-task]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mapreduce运行内存及JVM配置错误running beyond physical memory limits]]></title>
    <url>%2F2016%2F11%2F03%2Fmapreduce%E8%BF%90%E8%A1%8C%E5%86%85%E5%AD%98%E5%8F%8AJVM%E9%85%8D%E7%BD%AE%E9%94%99%E8%AF%AFrunning%20beyond%20physical%20%2F</url>
    <content type="text"><![CDATA[错误描述：123416/11/02 21:52:58 INFO mapreduce.Job: Task Id : attempt_1476760655616_0575_r_000000_2, Status : FAILEDContainer [pid=24537,containerID=container_1476760655616_0575_01_000052] is running beyond physical memory limits. Current usage: 4.0 GB of 3 GB physical memory used; 6.9 GB of 6.3 GB virtual memory used. Killing container.Dump of the process-tree for container_1476760655616_0575_01_000052 : |- PID PPID PGRPID SESSID CMD_NAME USER_MODE_TIME(MILLIS) SYSTEM_TIME(MILLIS) VMEM_USAGE(BYTES) RSSMEM_USAGE(PAGES) FULL_CMD_LINE 解决：查看hadoop配置参数1234567891011121314151617&lt;property&gt; &lt;name&gt;mapreduce.map.memory.mb&lt;/name&gt; &lt;value&gt;3072&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.reduce.memory.mb&lt;/name&gt; &lt;value&gt;3072&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.map.java.opts&lt;/name&gt; &lt;value&gt;-Xmx3072m&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.reduce.java.opts&lt;/name&gt; &lt;value&gt;-Xmx6144m&lt;/value&gt; &lt;/property&gt;&lt;property&gt; mapreduce.reduce.java.opts参数大于mapreduce.reduce.memory.mb,需小于才行 在mapreduce执行函数中设置参数1conf.setInt("mapreduce.reduce.memory.mb", 6144); 1. 参考 http://stackoverflow.com/questions/21005643/container-is-running-beyond-memory-limits 我们集群中的每台机器都有48 GB的RAM。此RAM的一些应保留为操作系统使用。在每个节点上，我们将为YARN分配40 GB RAM以使用操作系统并保留8 GB 对于我们的示例集群，我们有一个容器的最小RAM（yarn.scheduler.minimum-allocation-mb）= 2 GB。因此，我们将为Map任务容器分配4 GB，为Reduce任务容器分配8 GB。 在mapred-site.xml中：123mapreduce.map.memory.mb：4096mapreduce.reduce.memory.mb：8192 每个容器将运行Map和Reduce任务的JVM。 JVM堆大小应设置为低于上面定义的Map和Reduce内存，以使它们在YARN分配的Container内存的边界内。 在mapred-site.xml中：123mapreduce.map.java.opts：-Xmx3072mmapreduce.reduce.java.opts：-Xmx6144m 以上设置配置Map和Reduce任务将使用的物理RAM的上限。。 2. http://blog.chinaunix.net/uid-25691489-id-5587957.html 大概是job运行超过了map和reduce设置的内存大小，导致任务失败，调整增加了map和reduce的内容，问题排除，一些参数介绍如下： RM的内存资源配置，主要是通过下面的两个参数进行的（这两个值是Yarn平台特性，应在yarn-site.xml中配置好）：yarn.scheduler.minimum-allocation-mbyarn.scheduler.maximum-allocation-mb说明：单个容器可申请的最小与最大内存，应用在运行申请内存时不能超过最大值，小于最小值则分配最小值，从这个角度看，最小值有点想操作系统中的页。最小值还有另外一种用途，计算一个节点的最大container数目注：这两个值一经设定不能动态改变(此处所说的动态改变是指应用运行时)。 NM的内存资源配置，主要是通过下面两个参数进行的（这两个值是Yarn平台特性，应在yarn-sit.xml中配置） ：12yarn.nodemanager.resource.memory-mbyarn.nodemanager.vmem-pmem-ratio 说明：每个节点可用的最大内存，RM中的两个值不应该超过此值。此数值可以用于计算container最大数目，即：用此值除以RM中的最小容器内存。虚拟内存率，是占task所用内存的百分比，默认值为2.1倍;注意：第一个参数是不可修改的，一旦设置，整个运行过程中不可动态修改，且该值的默认大小是8G，即使计算机内存不足8G也会按着8G内存来使用。 AM内存配置相关参数，此处以MapReduce为例进行说明（这两个值是AM特性，应在mapred-site.xml中配置），如下：12mapreduce.map.memory.mbmapreduce.reduce.memory.mb 说明：这两个参数指定用于MapReduce的两个任务（Map and Reduce task）的内存大小，其值应该在RM中的最大最小container之间。如果没有配置则通过如下简单公式获得：max(MIN_CONTAINER_SIZE, (Total Available RAM) / containers))一般的reduce应该是map的2倍。注：这两个值可以在应用启动时通过参数改变； AM中其它与内存相关的参数，还有JVM相关的参数，这些参数可以通过，如下选项配置：mapreduce.map.java.optsmapreduce.reduce.java.opts说明：这两个参主要是为需要运行JVM程序（java、scala等）准备的，通过这两个设置可以向JVM中传递参数的，与内存有关的是，-Xmx，-Xms等选项。此数值大小，应该在AM中的map.mb和reduce.mb之间。 我们对上面的内容进行下总结，当配置Yarn内存的时候主要是配置如下三个方面：每个Map和Reduce可用物理内存限制；对于每个任务的JVM对大小的限制；虚拟内存的限制； 下面通过一个具体错误实例，进行内存相关说明，错误如下：1Container[pid=41884,containerID=container_1405950053048_0016_01_000284] is running beyond virtual memory limits. Current usage: 314.6 MB of 2.9 GB physical memory used; 8.7 GB of 6.2 GB virtual memory used. Killing container. 配置如下：12345678910111213141516&lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt; &lt;value&gt;100000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt; &lt;value&gt;10000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt; &lt;value&gt;3000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.reduce.memory.mb&lt;/name&gt; &lt;value&gt;2000&lt;/value&gt; &lt;/property&gt; 通过配置我们看到，容器的最小内存和最大内存分别为：3000m和10000m，而reduce设置的默认值小于2000m，map没有设置，所以两个值均为3000m，也就是log中的“2.9 GB physicalmemory used”。而由于使用了默认虚拟内存率(也就是2.1倍)，所以对于Map Task和Reduce Task总的虚拟内存为都为3000*2.1=6.2G。而应用的虚拟内存超过了这个数值，故报错 。解决办法：在启动Yarn是调节虚拟内存率或者应用运行时调节内存大小。]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mapreduce执行错误Mapper.错误]]></title>
    <url>%2F2016%2F11%2F03%2Fmapreduce%E6%89%A7%E8%A1%8C%E9%94%99%E8%AF%AFMapper.init%E9%94%99%E8%AF%AF%2F</url>
    <content type="text"><![CDATA[参考http://blog.itpub.net/30066956/viewspace-2107549/ 错误详情：12345678910111213141516/11/02 21:37:26 INFO mapreduce.Job: Task Id : attempt_1476760655616_0574_m_000027_2, Status : FAILEDError: java.lang.RuntimeException: java.lang.NoSuchMethodException: com.hdu.recommend.tools.CopyData$QLMapper.&lt;init&gt;() at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:131) at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:742) at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341) at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:415) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1614) at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)Caused by: java.lang.NoSuchMethodException: com.hdu.recommend.tools.CopyData$QLMapper.&lt;init&gt;() at java.lang.Class.getConstructor0(Class.java:2849) at java.lang.Class.getDeclaredConstructor(Class.java:2053) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:125) ... 7 more 解决：注意Mapper 与 Reducer 类写成内部类，一定要加static ！！！！]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[eclipse中操作hive错误org.apache.hadoop.security.AccessControlException]]></title>
    <url>%2F2016%2F11%2F03%2Feclipse%E4%B8%AD%E6%93%8D%E4%BD%9Chive%E9%94%99%E8%AF%AForg.apache.hadoop.security.AccessControlException%2F</url>
    <content type="text"><![CDATA[错误：123java.sql.SQLException: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.mr.MapRedTaskJob Submission failed with exception 'org.apache.hadoop.security.AccessControlException(Permission denied: user=anonymous, access=WRITE, inode="/user":hdfs:supergroup:drwxr-xr-x 解决：权限问题1hadoop fs -chmod -R 777 /]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git SSH公钥配置]]></title>
    <url>%2F2016%2F10%2F28%2FGit%20SSH%E5%85%AC%E9%92%A5%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[Coding.net配置 账户 SSH 公钥账户 SSH 公钥是跟用户账户关联的公钥，一旦设置，SSH 就拥有账户下所有项目仓库的读写权限。 设置“账户 SSH 公钥”是开发者使用 SSH 方式访问/修改代码仓库的“前置工作”，分为“获取 SSH 协议地址”、“生成公钥”、“在 Coding.net 添加公钥”三个步骤。 获取 SSH 协议地址在项目的代码页面点击 SSH 切换到 SSH 协议， 获得 clone 地址，形如git@git.coding.net:wzw/leave-a-message.git。 请使用这个地址来访问您的代码仓库。 生成公钥Mac/Linux 打开命令行终端, Windows 打开 Git Bash 。 输入ssh-keygen -t rsa -C &quot;username@example.com&quot;,( 注册的邮箱)，接下来点击enter键即可（也可以输入密码）。12345678910$ssh-keygen -t rsa -b 4096 -C "your_email@example.com"# Creates a new ssh key, using the provided email as a label# Generating public/private rsa key pair.Enter file in which to save the key (/Users/you/.ssh/id_rsa): [Press enter] // 推荐使用默认地址,如果使用非默认地址可能需要配置 .ssh/config成功之后Your identification has been saved in /Users/you/.ssh/id_rsa.# Your public key has been saved in /Users/you/.ssh/id_rsa.pub.# The key fingerprint is:# 01:0f:f4:3b:ca:85:d6:17:a1:7d:f0:68:9d:f0:a2:db your_email@example.com 在 Coding.net 添加公钥本地打开 id_rsa.pub 文件（或执行 $cat id_rsa.pub ），复制其中全部内容，添加到账户“SSH 公钥”页面 中，公钥名称可以随意起名字。完成后点击“添加”，然后输入密码或动态码即可添加完成。 完成后在命令行测试，首次建立链接会要求信任主机123$ ssh -T git@git.coding.net // 注意 git.coding.net 接入到 CDN 上所以会解析多个不同的 host ip The authenticity of host ‘git.coding.net (61.146.73.68)’ can not be established. RSA key fingerprint is 98:ab:2b:30:60:00:82:86:bb:85:db:87:22:c4:4f:b1. Are you sure you want to continue connecting (yes/no)? yes Warning: Permanently added ‘git.coding.net,61.146.73.68’ (RSA) to the list of kn own hosts.Enter passphrase for key ‘/c/Users/Yuankai/.ssh/id_rsa’: Coding.net Tips : [ Hello Kyle_lyk! You have connected to Coding.net by SSH successfully! ] 接下来就可以用git操作仓库了，是使用SourceTree非常方便]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark-1.6.0安装]]></title>
    <url>%2F2016%2F10%2F27%2Fspark-1.6.0%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[已安装Hadoop2.7.2的三节点集群 安装Scala 下载scala-2.11.7.tgz 解压Scala 1$ tar -zxvf scala-2.11.7.tgz -C ~/cloud/ 配置环境变量 12export SCALA_HOME=/home/ubuntu/cloud/scala-2.11.7export PATH = $PATH:$SCALA_HOME/bin 测试 12scala -versionScala code runner version 2.11.7 -- Copyright 2002-2013, LAMP/EPFL 配置到每台节点 安装Spark #Sparkexport SPARK_HOME=/home/ubuntu/cloud/spark-1.6.0export PATH=$PATH:$SPARK_HOME/bin export JAVA_HOME=/usr/java/jdk1.8.0_91export SPARK_MASTER_IP=10.0.0.7 #export SPARK_WORKER_MEMORY=2gexport HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop slave1slave2]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[name node is in safe mode问题]]></title>
    <url>%2F2016%2F10%2F18%2Fname%20node%20is%20in%20safe%20mode%2F</url>
    <content type="text"><![CDATA[name node is in safe mode问题： 向hdfs put数据的时候，导致了 name node is in safe mode，然后使用 Hadoop dfsadmin -safemode leave 后， 解除了安全模式。可是再次使用hdfs put或rm数据，仍旧导致name node 进入安全模式。 答案：分析了一下，问题是namenode所在机器的硬盘满了。因此即使使用了 hadoop dfsadmin -safemode leave 之后， 仍旧不能使用hdfs。 解决办法： 删除namenode所在机器的一些数据（本地数据） 结束安全模式 hadoop dfsadmin -safemode leave 可以正常使用hdfs了]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LinkedHashMap和TreeMap的区别]]></title>
    <url>%2F2016%2F10%2F18%2FLinkedHashMap%E5%92%8CTreeMap%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[LinkedHashMap和TreeMap的区别首先2个都是map，所以用key取值肯定是没区别的，区别在于用Iterator遍历的时候LinkedHashMap保存了记录的插入顺序，先插入的先遍历到TreeMap默认是按升序排，也可以指定排序的比较器。遍历的时候按升序遍历。例如：a是LinkedHashMap，b是TreeMap。a.put(“2”,”ab”);a.put(“1”,”bc”);b.put(“2”,”ab”);b.put(“1”,”bc”); 那么遍历a的时候，先遍历到key是2的，因为2先放进去。遍历b的时候，先遍历到“1”，因为按顺序是先1后2]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDUACM1200]]></title>
    <url>%2F2016%2F09%2F21%2FHDUACM1200%2F</url>
    <content type="text"><![CDATA[题目：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354To and FroTime Limit: 2000/1000 MS (Java/Others) Memory Limit: 65536/32768 K (Java/Others)Total Submission(s): 6390 Accepted Submission(s): 4389Problem DescriptionMo and Larry have devised a way of encrypting messages. They first decide secretly on the number of columns and write the message (letters only) down the columns, padding with extra random letters so as to make a rectangular array of letters. For example, if the message is “There’s no place like home on a snowy night” and there are five columns, Mo would write downt o i o yh p k n ne l e a ir a h s ge c o n hs e m o tn l e w xNote that Mo includes only letters and writes them all in lower case. In this example, Mo used the character ‘x’ to pad the message out to make a rectangle, although he could have used any letter.Mo then sends the message to Larry by writing the letters in each row, alternating left-to-right and right-to-left. So, the above would be encrypted astoioynnkpheleaigshareconhtomesnlewxYour job is to recover for Larry the original message (along with any extra padding letters) from the encrypted one. InputThere will be multiple input sets. Input for each set will consist of two lines. The first line will contain an integer in the range 2. . . 20 indicating the number of columns used. The next line is a string of up to 200 lower case letters. The last input set is followed by a line containing a single 0, indicating end of input. OutputEach input set should generate one line of output, giving the original plaintext message, with no spaces. Sample Input5toioynnkpheleaigshareconhtomesnlewx3ttyohhieneesiaabss0 Sample Outputtheresnoplacelikehomeonasnowynightxthisistheeasyoneab SourceEast Central North America 2004 RecommendIgnatius.L | We have carefully selected several similar problems for you: 1196 1073 1161 1113 1256 解答：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788package hdu;import java.util.Scanner;/** * 做的时候没看清楚 直接把蛇形输出做了正序的转换，然后才进行二维数组存储，多了个步骤 * @author Skye * */public class Main &#123; public static void main(String[] args) &#123; // TODO Auto-generated method stub Scanner sc = new Scanner(System.in); int col = sc.nextInt(); String message = null; int rowNum,row = 0; StringBuffer result = new StringBuffer(); char[] stringArr = null; StringBuffer tmpString = new StringBuffer(); char[] tmpChar = null; boolean isChange = false; while (col != 0) &#123; message = sc.next(); rowNum = message.length()/col; // System.out.println(message); stringArr = message.toCharArray(); // ttyohhieneesiaabss for (int i = 0; i &lt; stringArr.length; i++) &#123; // System.out.println(stringArr[i]); if (i % col == 0) &#123; // 到行开头 row = i / col + 1; // 记录当前字母在第几行 if (row % 2 == 0) &#123; isChange = true; &#125; else &#123; isChange = false; &#125; &#125; if (isChange == false) &#123; if (i != 0 &amp;&amp; i % col == 0) &#123; //System.out.println(tmpString.toString()); tmpChar = tmpString.toString().toCharArray(); for (int j = tmpChar.length - 1; j &gt;= 0; j--) &#123; result.append(tmpChar[j]); &#125; tmpString = new StringBuffer(); &#125; result.append(stringArr[i]); &#125; else &#123; tmpString.append(stringArr[i]); &#125; &#125; if(isChange == true)&#123; //System.out.println(tmpString.toString()); tmpChar = tmpString.toString().toCharArray(); for (int j = tmpChar.length - 1; j &gt;= 0; j--) &#123; result.append(tmpChar[j]); &#125; tmpString = new StringBuffer(); &#125; //System.out.println(result); char[][] charArr = new char[rowNum][col]; char[] resultChar = result.toString().toCharArray(); int count = 0; for(int k = 0; k &lt; rowNum;k++)&#123; for(int p = 0; p &lt; col;p++)&#123; charArr[k][p] = resultChar[count++]; &#125; &#125; StringBuffer resultFinal = new StringBuffer(); for(int k = 0; k &lt; col;k++)&#123; for(int p = 0; p &lt; rowNum;p++)&#123; resultFinal.append(charArr[p][k]); &#125; &#125; System.out.println(resultFinal); resultFinal = new StringBuffer(); result = new StringBuffer(); col = sc.nextInt(); &#125; sc.close(); &#125;&#125; HDU另解：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import java.io.*;import java.util.*;public class Main&#123; public static void main(String[] args) &#123; Scanner input = new Scanner(System.in); while (input.hasNext()) &#123; int n = input.nextInt(); if(n==0) break; input.nextLine(); String str = input.nextLine(); char c1[] = str.toCharArray(); // 把每个字符单个存起来 char c2[][] = new char[1010][1010]; int line = c1.length / n; //记录行数 int k = 0; // 记录c1字符的位数 for (int i = 0; i &lt; line; i++) &#123; if (i % 2 == 1) &#123; for (int j = n - 1; j &gt;= 0; j--) &#123; c2[i][j] = c1[k++]; &#125; &#125; else &#123; for (int j = 0; j &lt; n; j++) &#123; c2[i][j] = c1[k++]; &#125; &#125; &#125; for (int j = 0; j &lt; n; j++) &#123; for (int i = 0; i &lt; line; i++) &#123; System.out.print(c2[i][j]); &#125; &#125; System.out.println(); &#125; &#125;&#125;]]></content>
      <categories>
        <category>ACM</category>
      </categories>
      <tags>
        <tag>ACM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java Map 按key排序和按Value排序]]></title>
    <url>%2F2016%2F08%2F31%2FJava%20Map%20%E6%8C%89key%E6%8E%92%E5%BA%8F%E5%92%8C%E6%8C%89Value%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[做推荐系统项目时，对标签评分需要对标签评分map进行排序. 理论准备 Map是键值对的集合接口，它的实现类主要包括：HashMap,TreeMap,Hashtable以及LinkedHashMap等。 TreeMap：基于红黑树（Red-Black tree）的 NavigableMap 实现，该映射根据其键的自然顺序进行排序，或者根据创建映射时提供的 Comparator 进行排序，具体取决于使用的构造方法。HashMap的值是没有顺序的，它是按照key的HashCode来实现的，对于这个无序的HashMap我们要怎么来实现排序呢？参照TreeMap的value排序。 Map.Entry返回Collections视图。 key排序TreeMap默认是升序的，如果我们需要改变排序方式，则需要使用比较器：Comparator。Comparator可以对集合对象或者数组进行排序的比较器接口，实现该接口的public compare(T o1,To2)方法即可实现排序，如下： 123456789101112131415161718192021222324252627import java.util.Comparator;import java.util.Iterator;import java.util.Map;import java.util.Set;import java.util.TreeMap;public class TreeMapTest &#123; public static void main(String[] args) &#123; Map&lt;String, String&gt; map = new TreeMap&lt;String, String&gt;( new Comparator&lt;String&gt;() &#123; public int compare(String obj1, String obj2) &#123; // 降序排序 return obj2.compareTo(obj1); &#125; &#125;); map.put("b", "ccccc"); map.put("d", "aaaaa"); map.put("c", "bbbbb"); map.put("a", "ddddd"); Set&lt;String&gt; keySet = map.keySet(); Iterator&lt;String&gt; iter = keySet.iterator(); while (iter.hasNext()) &#123; String key = iter.next(); System.out.println(key + ":" + map.get(key)); &#125; &#125;&#125; 运行如下：1234d:aaaaac:bbbbbb:ccccca:ddddd value排序上面例子是对根据TreeMap的key值来进行排序的，但是有时我们需要根据TreeMap的value来进行排序。对value排序我们就需要借助于Collections的sort(List list, Comparator&lt;? super T&gt; c)方法，该方法根据指定比较器产生的顺序对指定列表进行排序。但是有一个前提条件，那就是所有的元素都必须能够根据所提供的比较器来进行比较，如下：1234567891011121314151617181920212223242526272829303132import java.util.ArrayList;import java.util.Collections;import java.util.Comparator;import java.util.List;import java.util.Map;import java.util.Map.Entry;import java.util.TreeMap;public class TreeMapTest &#123; public static void main(String[] args) &#123; Map&lt;String, String&gt; map = new TreeMap&lt;String, String&gt;(); map.put("a", "ddddd"); map.put("c", "bbbbb"); map.put("d", "aaaaa"); map.put("b", "ccccc"); //这里将map.entrySet()转换成list List&lt;Map.Entry&lt;String,String&gt;&gt; list = new ArrayList&lt;Map.Entry&lt;String,String&gt;&gt;(map.entrySet()); //然后通过比较器来实现排序 Collections.sort(list,new Comparator&lt;Map.Entry&lt;String,String&gt;&gt;() &#123; //升序排序 public int compare(Entry&lt;String, String&gt; o1, Entry&lt;String, String&gt; o2) &#123; return o1.getValue().compareTo(o2.getValue()); &#125; &#125;); for(Map.Entry&lt;String,String&gt; mapping:list)&#123; System.out.println(mapping.getKey()+":"+mapping.getValue()); &#125; &#125;&#125; 运行结果如下：1234d:aaaaac:bbbbbb:ccccca:ddddd 参考：http://blog.csdn.net/xiaoyu714543065/article/details/38519817]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java BufferdReader读取文件乱码]]></title>
    <url>%2F2016%2F08%2F29%2FJava%20BufferdReader%E8%AF%BB%E5%8F%96%E6%96%87%E4%BB%B6%E4%B9%B1%E7%A0%81%2F</url>
    <content type="text"><![CDATA[javaBufferdReader读取文件乱码以下为读取文件方法 123456789101112131415161718192021222324252627private static void putIdGame()&#123; URL.setURLStreamHandlerFactory(new FsUrlStreamHandlerFactory()); int count = 0; try &#123; URL url = new URL(HDFS + path); InputStream gameList = url.openStream(); BufferedReader reader_url = new BufferedReader(new InputStreamReader(gameList,"UTF-8")); String inString_RL = reader_url.readLine(); while (inString_RL != null &amp;&amp; count &lt; 50) &#123; int userId; String[] str = inString_RL.split(","); count ++; map.put(str[1], str[0]); System.out.println(str[0]); inString_RL = reader_url.readLine(); &#125; reader_url.close(); &#125; catch (FileNotFoundException e) &#123; System.out.println("未找文件！"); &#125; catch (IOException e1) &#123; System.out.println("文件读写错误！"); &#125;&#125; 在InputStreamReader中加入”UTF-8”即可 Java读取文件时第一行出现乱码“？”问号在windows 环境下，使用java文件流读取文本文件时，会出现第一个字符为未知字符”?” ,其他字符完整。而且第一个字符显示为？但是用equals比对发现并非是”?”号,google之，了解到bom编码标记。使用 16进制打印输出结果： 只要出现该头的16进制编码为这种字符便可以断定该文本文件的编码方式了。 bom编码标记： bom全称是：byte order mark，汉语意思是标记字节顺序码。只是出现在：unicode字符集中，只有unicode字符集，存储时候，要求指定编码，如果不指定，windows还会用默认的：ANSI读取。常见的bom头是： UTF-8 ║ EF BB BF UTF-16LE ║ FF FE (小尾） UTF-16BE ║ FE FF （大尾） UTF-32LE ║ FF FE 00 00 UTF-32BE ║ 00 00 FE FF 解决方法： 工具将txt文件另存为UTF-8无BOM格式 123456789101112131415161718public String readerFile(InputStream in) throws IOException &#123; StringBuffer strBuff = new StringBuffer(); String temp = null; BufferedReader reader = new BufferedReader(new InputStreamReader(in,Charset.forName("utf-8"))); while ((temp = reader.readLine()) != null) &#123; byte[] by = temp.getBytes(); String header = Integer.toHexString(by[0]).toUpperCase(); //判断是否拥有无法识别的字符 if (header.equalsIgnoreCase("FFFFFFEF") || header.equalsIgnoreCase("3F")) &#123; strBuff.append(temp.substring(1) + "\n"); continue; &#125; strBuff.append(temp + "\n"); &#125; reader.close(); in.close(); return strBuff.toString(); &#125;]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDFS工具类]]></title>
    <url>%2F2016%2F08%2F26%2FHDFS%E5%B7%A5%E5%85%B7%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[花了点时间整理了验证一下在本地eclipse上操作HDFS的工具类，实现在本地通过API操作HDFS。 实现以下功能： ls rmr mkdir copyFromLocal cat copyToLocal 创建一个新文件，并写入内容 主要引用参考： http://blog.fens.me/hadoop-hdfs-api/ 类如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198package com.hdu.hdfs;import java.io.IOException;import java.net.URI;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.BlockLocation;import org.apache.hadoop.fs.FSDataInputStream;import org.apache.hadoop.fs.FSDataOutputStream;import org.apache.hadoop.fs.FileStatus;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Hdfs;import org.apache.hadoop.fs.Path;import org.apache.hadoop.hdfs.web.HsftpFileSystem;import org.apache.hadoop.io.IOUtils;import org.apache.hadoop.mapred.JobConf;/** * HDFS工具类 * * 实现功能： * hadoop fs -ls / * hadoop fs -mkdir /data * hadoop fs -rmr /data/test.txt * hadoop fs -copyFromLocal /test/test.txt /data * hadoop fs -cat /data/test.txt * hadoop fs -copyToLocal /data /test/test.txt * 创建一个新文件，并写入内容 * 重命名 * * 需要导入以下路径的所有jar包： hadoop-2.7.2\share\hadoop\common * hadoop-2.7.2\share\hadoop\common\lib hadoop-2.7.2\share\hadoop\hdfs * hadoop-2.7.2\share\hadoop\hdfs\lib hadoop-2.7.2\share\hadoop\mapreduce * * * @author Skye * */public class HdfsDAO &#123; // HDFS访问地址 private static final String HDFS = "hdfs://192.168.1.111:9000/"; // hdfs路径 private String hdfsPath; // Hadoop系统配置 private Configuration conf; public HdfsDAO(Configuration conf) &#123; this(HDFS, conf); &#125; public HdfsDAO(String hdfs, Configuration conf) &#123; this.hdfsPath = hdfs; this.conf = conf; &#125; // 启动函数 public static void main(String[] args) throws IOException &#123; JobConf conf = config(); HdfsDAO hdfs = new HdfsDAO(conf); // hdfs.mkdirs("/new"); // 可以同时建多级目录 // hdfs.mkdirs("/new/new3"); // hdfs.ls("/tuijian"); // hdfs.rmr("/new"); // 可用当前eclipse工作空间的相对路径和文件绝对路径 以及当前项目的路径不加"/" // hdfs.copyFileToHdfs("data/hive笔记.md", "/data"); // hdfs.copyFileToHdfs("/Xiaomi/MiFlashClean.cmd", "/data"); // hdfs.copyFileToHdfs("E:/推荐系统/100万用户数据/user_pay", "/data"); // hdfs.rmr("/data/MiFlashClean.cmd"); // hdfs.rmr("/data/user_pay_201606"); // hdfs.createFile("/new/createTest", "1,英雄联盟"); // hdfs.download("/data/RecommendList", "C:/Users/Skye/Desktop"); // hdfs.cat("/data/RecommendList1"); // hdfs.renameFile("/data/RecommendList", "/data/RecommendListOld"); // hdfs.ls("/data"); //hdfs.findLocationOnHadoop("/data/RecommendListOld"); &#125; // 加载Hadoop配置文件 public static JobConf config() &#123; JobConf conf = new JobConf(HdfsDAO.class); conf.setJobName("HdfsDAO"); // conf.addResource("classpath:/hadoop/core-site.xml"); // conf.addResource("classpath:/hadoop/hdfs-site.xml"); // conf.addResource("classpath:/hadoop/mapred-site.xml"); return conf; &#125; public void mkdirs(String folder) throws IOException &#123; Path path = new Path(folder); FileSystem fs = FileSystem.get(URI.create(hdfsPath), conf); if (!fs.exists(path)) &#123; fs.mkdirs(path); System.out.println("Create: " + folder); &#125; fs.close(); &#125; public void rmr(String folder) throws IOException &#123; Path path = new Path(folder); FileSystem fs = FileSystem.get(URI.create(hdfsPath), conf); fs.deleteOnExit(path); System.out.println("Delete: " + folder); fs.close(); &#125; public void ls(String folder) throws IOException &#123; Path path = new Path(folder); FileSystem fs = FileSystem.get(URI.create(hdfsPath), conf); FileStatus[] list = fs.listStatus(path); System.out.println("ls: " + folder); System.out.println("=========================================================="); for (FileStatus f : list) &#123; System.out.printf("name: %s, folder: %s, size: %d\n", f.getPath(), f.isDir(), f.getLen()); &#125; System.out.println("=========================================================="); fs.close(); &#125; public void createFile(String file, String content) throws IOException &#123; FileSystem fs = FileSystem.get(URI.create(hdfsPath), conf); byte[] buff = content.getBytes(); FSDataOutputStream os = null; try &#123; os = fs.create(new Path(file)); os.write(buff, 0, buff.length); System.out.println("Create: " + file); &#125; finally &#123; if (os != null) os.close(); &#125; fs.close(); &#125; public void copyFileToHdfs(String local, String remote) throws IOException &#123; FileSystem fs = FileSystem.get(URI.create(hdfsPath), conf); fs.copyFromLocalFile(new Path(local), new Path(remote)); System.out.println("copy from: " + local + " to " + remote); fs.close(); &#125; public void download(String remote, String local) throws IOException &#123; Path path = new Path(remote); FileSystem fs = FileSystem.get(URI.create(hdfsPath), conf); fs.copyToLocalFile(path, new Path(local)); System.out.println("download: from" + remote + " to " + local); fs.close(); &#125; public void renameFile(String oldFileName, String newFileName) throws IOException &#123; boolean isSuccess = true; FileSystem fs = FileSystem.get(URI.create(hdfsPath), conf); try &#123; isSuccess = fs.rename(new Path(oldFileName), new Path(newFileName)); &#125; catch (IOException e) &#123; isSuccess = false; &#125; System.out.println(isSuccess ? "Rename success！ " + oldFileName + " to " + newFileName : "Rename failed！" + oldFileName + " to " + newFileName); fs.close(); &#125; /** * 查看某个文件在HDFS集群的位置 * * @throws IOException */ public void findLocationOnHadoop(String filePath) throws IOException &#123; // Path targetFile=new Path(rootPath+"user/hdfsupload/AA.txt"); // FileStatus fileStaus=coreSys.getFileStatus(targetFile); Path targetFile = new Path(HDFS + filePath); FileSystem fs = FileSystem.get(URI.create(hdfsPath), conf); FileStatus fileStaus = fs.getFileStatus(targetFile); BlockLocation[] bloLocations = fs.getFileBlockLocations(fileStaus, 0, fileStaus.getLen()); for (int i = 0; i &lt; bloLocations.length; i++) &#123; System.out.println("block_" + i + "_location:" + bloLocations[i].getHosts()[0]); &#125; fs.close(); &#125; public void cat(String remoteFile) throws IOException &#123; Path path = new Path(remoteFile); FileSystem fs = FileSystem.get(URI.create(hdfsPath), conf); FSDataInputStream fsdis = null; System.out.println("cat: " + remoteFile); try &#123; fsdis = fs.open(path); IOUtils.copyBytes(fsdis, System.out, 4096, false); &#125; finally &#123; IOUtils.closeStream(fsdis); fs.close(); &#125; &#125;&#125;]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive1.2.1安装笔记]]></title>
    <url>%2F2016%2F08%2F22%2FHive%E5%AE%89%E8%A3%85%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[环境1234ubuntu 16.044台机器的Hadoop2.7.2集群Mysql安装在slave2中hive安装在master上 下载Hive123$ wget http://mirrors.cnnic.cn/apache/hive/hive-1.2.1/apache-hive-1.2.1-bin.tar.gz# 解压$ tar -zxvf apache-hive-1.2.1-bin.tar.gz /home/ubuntu/cloud 配置Hive环境变量1234567$ sudo vim /etc/profile#添加export HIVE_HOME=/home/ubuntu/cloud/apache-hive-1.2.1-binexport PATH=$PATH:$HIVE_HOME/bin$source /etc/profile 在Mysql中创建Hive用户123mysql&gt;CREATE USER 'hive' IDENTIFIED BY 'hive';mysql&gt;GRANT ALL PRIVILEGES ON *.* TO 'hive'@'%' IDENTIFIED BY 'hive' WITH GRANT OPTION;mysql&gt;flush privileges; 创建Hive数据库12$ mysql -uhive -phivemysql&gt;create database hive; 配置Hive进入Hive的conf目录,找到hive-default.xml.template，cp份为hive-site.xml123456789101112131415161718192021222324$ vim hive-site.xml# 删除configuration标签里的所有内容 添加如下内容 &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://slave2:3306/hive?createDatabaseIfNotExist=true&lt;/value&gt; &lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;hive&lt;/value&gt; &lt;description&gt;username to use against metastore database&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;hive&lt;/value&gt; &lt;description&gt;password to use against metastore database&lt;/description&gt; &lt;/property&gt; 下载mysql-connector-java-5.1.32-bin.jar这里用5.1.32版本测试不报错，5.1.38会报warn 12#将连接jar包拷贝到Hive的lib目录$ cp mysql-connector-java-5.1.32-bin.jar /home/ubuntu/cloud/apache-hive-1.2.1-bin/lib/ 若要装hive客户端可在客户端节点设置vim hive-site.xml12345678910111213141516&lt;configuration&gt;&lt;!-- thrift://&lt;host_name&gt;:&lt;port&gt; 默认端口是9083 --&gt;&lt;property&gt;&lt;name&gt;hive.metastore.uris&lt;/name&gt; &lt;value&gt;thrift://master:9083&lt;/value&gt; &lt;description&gt;Thrift uri for the remote metastore. Used by metastore client to connect to remote metastore.&lt;/description&gt;&lt;/property&gt; &lt;!-- hive表的默认存储路径 --&gt;&lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/user/hive/warehouse&lt;/value&gt; &lt;description&gt;location of default database for the warehouse&lt;/description&gt;&lt;/property&gt;&lt;/configuration&gt; Hive启动要启动metastore服务123456789$ hive --service metastore &amp;$ jps10288 RunJar #多了一个进程9365 NameNode9670 SecondaryNameNode11096 Jps9944 NodeManager9838 ResourceManager9471 DataNode 启动hive命令行123456ubuntu@master:~$ hiveLogging initialized using configuration in jar:file:/home/ubuntu/cloud/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.propertieshive&gt; show tables;OKTime taken: 0.705 seconds 启动hiveserver21hive --service hiveserver2 start &amp; 问题解决问题：创建表出先如下错误，删除表卡住 1Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:For direct MetaStore DB connections, we don't support retries at the client level. 初始化注意：初始化之前先删除hdfs上的metastore,否则会出错/user/hive/warehouse在hive服务端输入以下命令1schematool -dbType mysql -initSchema]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[oracle启动dbstart出错或无反应的解决办法及自启动]]></title>
    <url>%2F2016%2F08%2F20%2Foracle%E5%90%AF%E5%8A%A8dbstart%E5%87%BA%E9%94%99%E6%88%96%E6%97%A0%E5%8F%8D%E5%BA%94%E7%9A%84%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95%E5%8F%8A%E8%87%AA%E5%90%AF%E5%8A%A8%2F</url>
    <content type="text"><![CDATA[问题一启动dbstart 报错1ORACLE_HOME_LISTNER is not SET, unable to auto-start Oracle Net Listener Usage: /home/oracle/oracle11g/product/11.2.0/dbhome_1/bin/dbstart ORACLE_HOME linux成功安装Oracle后切换到Oracle用户后，直接使用dbstart($ORACLE_HOME/bin中)启动oracle数据库报错如上。原因是dbstart调用的tnslsnr脚本位置有错。解决办法：打开该脚本：vim $ORACLE_HOME/bin/dbstart查找“ORACLE_HOME_LISTENER”变量的定义处，修改ORACLE_HOME_LISTENER＝$1为ORACLE_HOME_LISTENER＝$ORACLE_HOME 问题二启动dbstart没有反应，即不报错也不显示启动信息原因是oracle的配置需要修改才能使用dbstart启动对应的数据实例。解决办法： sudo vim /etc/oratab将orcl:/home/oracle/oracle11g/product/11.2.0/dbhome_1:N改为orcl:/home/oracle/oracle11g/product/11.2.0/dbhome_1:Y 问题三1234&gt;dbstartCan't find init file for Database "orcl".Database "orcl" NOT started. 原因就是没有找到init文件 我的数据库实例是orcl这个文件在$ORACLE_HOME/dbs/目录下cd $ORACLE_HOME/dbs解决办法就是建立一个initorcl.ora的软连接就可以了ln -s spfileego.ora initorcl.ora Oracle自启动创建开机自动启动数据库的脚本开一个普通的字符终端连接到UbuntuServer，运行如下命令：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950# vi /etc/init.d/Oracledb文件内容如下：#!/bin/bash## /etc/init.d/oracledb## Run-level Startup script for the Oracle Instance, Listener, and# Web Interfaceexport ORACLE_HOME=/home/oracle/oracle11g/product/11.2.0/dbhome_1export ORACLE_SID=orclexport PATH=$ORACLE_HOME/bin:$PATHORA_OWNR="oracle"# if the executables do not exist -- display errorif [ ! -f $ORACLE_HOME/bin/dbstart -o ! -d $ORACLE_HOME ]thenecho "Oracle startup: cannot start"exit 1fi# depending on parameter -- startup, shutdown, restart# of the instance and listener or usage displaycase "$1" instart)# Oracle listener and instance startupecho -n "Starting Oracle: "su $ORA_OWNR -c "$ORACLE_HOME/bin/lsnrctl start"su $ORA_OWNR -c "$ORACLE_HOME/bin/dbstart"touch /var/lock/oraclesu $ORA_OWNR -c "$ORACLE_HOME/bin/emctl start dbconsole"echo "OK";;stop)# Oracle listener and instance shutdownecho -n "Shutdown Oracle: "su $ORA_OWNR -c "$ORACLE_HOME/bin/lsnrctl stop"su $ORA_OWNR -c "$ORACLE_HOME/bin/dbshut"rm -f /var/lock/oraclesu $ORA_OWNR -c "$ORACLE_HOME/bin/emctl stop dbconsole"echo "OK";;reload|restart)$0 stop$0 start;;*)echo "Usage: `basename $0` start|stop|restart|reload"exit 1esacexit 0 再运行如下命令设置权限，并放到启动脚本中去：12# chmod 755 /etc/init.d/Oracledb# update-rc.d Oracledb defaults 99 最后：# vi /etc/oratab把文件中的N改成Y，即”orcl:/opt/oracle/product/db:N”修改为”orcl:/opt/oracle/product/db:Y”。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Oracle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux日常操作笔记]]></title>
    <url>%2F2016%2F08%2F19%2FLinux%E6%97%A5%E5%B8%B8%E6%93%8D%E4%BD%9C%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[查看当前所在的工作目录的全路径 pwd12[root@localhost ~]# pwd/root 查看当前系统的时间 date12345678[root@localhost ~]# date +%Y-%m-%d2016-07-26date +%Y-%m-%d --date="-1 day" #加减也可以 month | year2016-07-25[root@localhost ~]# date -s "2016-07-28 16:12:00" ## 修改时间Thu Jul 28 16:12:00 PDT 2016 查看有谁在线（哪些人登陆到了服务器）12345who 查看当前在线[root@localhost ~]# whohadoop tty1 2016-07-26 00:01 (:0)hadoop pts/0 2016-07-26 00:49 (:0.0)root pts/1 2016-07-26 00:50 (192.168.233.1) ###last 查看最近的登陆历史记录 12345[root@localhost ~]# lastroot pts/1 192.168.233.1 Tue Jul 26 00:50 still logged in hadoop pts/0 :0.0 Tue Jul 26 00:49 still logged in hadoop tty1 :0 Tue Jul 26 00:01 still logged in reboot system boot 2.6.32-573.el6.x Tue Jul 26 07:58 - 16:23 (2+08:24) 关机/重启123456789关机（必须用root用户）shutdown -h now ## 立刻关机shutdown -h +10 ## 10分钟以后关机shutdown -h 12:00:00 ##12点整的时候关机halt # 等于立刻关机重启shutdown -r nowreboot # 等于立刻重启 清屏clear ## 或者用快捷键 ctrl + l 退出当前进程ctrl+c ##有些程序也可以用q键退出 挂起当前进程123ctrl+z ## 进程会挂起到后台bg jobid ## 让进程在后台继续执行fg jobid ## 让进程回到前台 echo相当于java中System.out.println(userName)12345[root@localhost ~]# a="hi boy"[root@localhost ~]# echo aa[root@localhost ~]# echo $ahi boy 目录操作查看目录信息123ls / ## 查看根目录下的子节点（文件夹和文件）信息ls -al ## -a是显示隐藏文件 -l是以更详细的列表形式显示ls -l ##有一个别名： ll 可以直接使用ll &lt;是两个L&gt; 切换工作目录12345cd /home/hadoop ## 切换到用户主目录cd ~ ## 切换到用户主目录cd - ## 回退到上次所在的目录cd 什么路径都不带，则回到用户的主目录 创建文件夹123mkdir aaa ## 这是相对路径的写法 mkdir /data ## 这是绝对路径的写法 mkdir -p aaa/bbb/ccc ## 级联创建目录 删除文件夹123rmdir aaa ## 可以删除空目录rm -r aaa ## 可以把aaa整个文件夹及其中的所有子节点全部删除rm -rf aaa ## 强制删除aaa 修改文件夹名称123mv aaa boymv本质上是移动mv install.log aaa/ 将当前目录下的install.log 移动到aaa文件夹中去 123456789101112rename 可以用来批量更改文件名[root@localhost aaa]# lltotal 0-rw-r--r--. 1 root root 0 Jul 28 17:33 1.txt-rw-r--r--. 1 root root 0 Jul 28 17:33 2.txt-rw-r--r--. 1 root root 0 Jul 28 17:33 3.txt[root@localhost aaa]# rename .txt .txt.bak *[root@localhost aaa]# lltotal 0-rw-r--r--. 1 root root 0 Jul 28 17:33 1.txt.bak-rw-r--r--. 1 root root 0 Jul 28 17:33 2.txt.bak-rw-r--r--. 1 root root 0 Jul 28 17:33 3.txt.bak 文件操作创建文件12345678touch somefile.1 ## 创建一个空文件echo "hi,boy" &gt; somefile.2 ## 利用重定向“&gt;”的功能，将一条指令的输出结果写入到一个文件中，会覆盖原文件内容，如果指定的文件不存在，则会创建出来echo "hi baby" &gt;&gt; somefile.2 ## 将一条指令的输出结果追加到一个文件中，不会覆盖原文件内容 vi文本编辑器最基本用法vi somefile.41 首先会进入“一般模式”，此模式只接受各种快捷键，不能编辑文件内容2 按i键，就会从一般模式进入编辑模式，此模式下，敲入的都是文件内容3 编辑完成之后，按Esc键退出编辑模式，回到一般模式；4 再按：，进入“底行命令模式”，输入wq命令，回车即可 常用快捷键一些有用的快捷键（在一般模式下使用）：a 在光标后一位开始插入A 在该行的最后插入I 在该行的最前面插入gg 直接跳到文件的首行G 直接跳到文件的末行dd 删除一行3dd 删除3行yy 复制一行3yy 复制3行p 粘贴u undov 进入字符选择模式，选择完成后，按y复制，按p粘贴ctrl+v 进入块选择模式，选择完成后，按y复制，按p粘贴shift+v 进入行选择模式，选择完成后，按y复制，按p粘贴 查找并替换1 显示行号:set nu2 隐藏行号:set nonu3 查找关键字:/you ## 效果：查找文件中出现的you，并定位到第一个找到的地方，按n可以定位到下一个匹配位置（按N定位到上一个）4 替换操作:s/sad/bbb 查找光标所在行的第一个sad，替换为bbb:%s/sad/bbb 查找文件中所有sad，替换为bbb 拷贝/删除/移动1234cp somefile.1 /home/hadoop/rm /home/hadoop/somefile.1rm -f /home/hadoop/somefile.1mv /home/hadoop/somefile.1 ../ 查看文件内容12345678910111213cat somefile 一次性将文件内容全部输出（控制台）more somefile 可以翻页查看, 下翻一页(空格) 上翻一页（b） 退出（q）less somefile 可以翻页查看,下翻一页(空格) 上翻一页（b），上翻一行(↑) 下翻一行（↓） 可以搜索关键字（/keyword）跳到文件末尾： G跳到文件首行： gg退出less ： qtail -10 install.log 查看文件尾部的10行tail +10 install.log 查看文件 10--&gt;末行tail -f install.log 小f跟踪文件的唯一inode号，就算文件改名后，还是跟踪原来这个inode表示的文件tail -F install.log 大F按照文件名来跟踪head -10 install.log 查看文件头部的10行 打包压缩1234567891011121314151617181920212223242526272829303132333435363738391、gzip压缩gzip a.txt2、解压gunzip a.txt.gzgzip -d a.txt.gz3、bzip2压缩bzip2 a4、解压bunzip2 a.bz2bzip2 -d a.bz25、打包：将指定文件或文件夹tar -cvf bak.tar ./aaa将/etc/password追加文件到bak.tar中(r)tar -rvf bak.tar /etc/password6、解压tar -xvf bak.tar7、打包并压缩tar -zcvf a.tar.gz aaa/8、解包并解压缩(重要的事情说三遍!!!)tar -zxvf a.tar.gz解压到/usr/下tar -zxvf a.tar.gz -C /usr9、查看压缩包内容tar -ztvf a.tar.gzzip/unzip10、打包并压缩成bz2tar -jcvf a.tar.bz211、解压bz2tar -jxvf a.tar.bz2 查找命令常用查找命令的使用 123456789101112131415161718192021222324251、查找可执行的命令所在的路径：which ls2、查找可执行的命令和帮助的位置：whereis ls3、从某个文件夹开始查找文件find / -name "hadooop*"find / -name "hadooop*" -ls4、查找并删除find / -name "hadooop*" -ok rm &#123;&#125; \;find / -name "hadooop*" -exec rm &#123;&#125; \;5、查找用户为hadoop的文件find /usr -user hadoop -ls6、查找用户为hadoop的文件夹find /home -user hadoop -type d -ls7、查找权限为777的文件find / -perm -777 -type d -ls8、显示命令历史history grep命令123456789101112131415161718192021222324252627282930313233343536373839404142431 基本使用查询包含hadoop的行grep hadoop /etc/passwordgrep aaa ./*.txt 2 cut截取以:分割保留第七段grep hadoop /etc/passwd | cut -d: -f73 查询不包含hadoop的行grep -v hadoop /etc/passwd4 正则表达包含hadoopgrep 'hadoop' /etc/passwd5 正则表达(点代表任意一个字符)grep 'h.*p' /etc/passwd6 正则表达以hadoop开头grep '^hadoop' /etc/passwd7 正则表达以hadoop结尾grep 'hadoop$' /etc/passwd规则：. : 任意一个字符a* : 任意多个a(零个或多个a)a? : 零个或一个aa+ : 一个或多个a.* : 任意多个任意字符\. : 转义.o\&#123;2\&#125; : o重复两次查找不是以#开头的行grep -v '^#' a.txt | grep -v '^$' 以h或r开头的grep '^[hr]' /etc/passwd不是以h和r开头的grep '^[^hr]' /etc/passwd不是以h到r开头的grep '^[^h-r]' /etc/passwd 文件权限的操作linux文件权限的描述格式解读 drwxr-xr-x （也可以用二进制表示 111 101 101 –&gt; 755） d：标识节点类型（d：文件夹 -：文件 l:链接）r：可读 w：可写 x：可执行第一组rwx： ## 表示这个文件的拥有者对它的权限：可读可写可执行第二组r-x： ## 表示这个文件的所属组用户对它的权限：可读，不可写，可执行第三组r-x： ## 表示这个文件的其他用户（相对于上面两类用户）对它的权限：可读，不可写，可执行 修改文件权限1234567891011chmod g-rw haha.dat ## 表示将haha.dat对所属组的rw权限取消chmod o-rw haha.dat ## 表示将haha.dat对其他人的rw权限取消chmod u+x haha.dat ## 表示将haha.dat对所属用户的权限增加xchmod a-x haha.dat ## 表示将haha.dat对所用户取消x权限也可以用数字的方式来修改权限chmod 664 haha.dat 就会修改成 rw-rw-r--如果要将一个文件夹的所有内容权限统一修改，则可以-R参数chmod -R 770 aaa/ 修改文件所有权1234&lt;只有root权限能执行&gt;chown angela aaa ## 改变所属用户chown :angela aaa ## 改变所属组chown angela:angela aaa/ ## 同时修改所属用户和所属组 基本的用户管理123456添加一个用户：useradd sparkpasswd spark 根据提示设置密码即可删除一个用户：userdel -r spark 加一个-r就表示把用户及用户的主目录都删除 添加用户12345678添加一个tom用户，设置它属于users组，并添加注释信息分步完成：useradd tom usermod -g users tom usermod -c "hr tom" tom一步完成：useradd -g users -c "hr tom" tom设置tom用户的密码passwd tom 修改用户12345678修改tom用户的登陆名为tomcatusermod -l tomcat tom将tomcat添加到sys和root组中usermod -G sys,root tomcat查看tomcat的组信息groups tomcat 用户组操作123456789101112添加一个叫america的组groupadd america将jerry添加到america组中usermod -g america jerry将tomcat用户从root组和sys组删除gpasswd -d tomcat rootgpasswd -d tomcat sys将america组名修改为amgroupmod -n am america 为用户配置sudo权限1234567用root编辑 vi /etc/sudoers在文件的如下位置，为hadoop添加一行即可root ALL=(ALL) ALL hadoop ALL=(ALL) ALL然后，hadoop用户就可以用sudo来执行系统级别的指令[root@localhost ~]$ sudo useradd xiaoming 系统管理操作挂载外部存储设备可以挂载光盘、硬盘、磁带、光盘镜像文件等 挂载光驱 12mkdir /mnt/cdrom 创建一个目录，用来挂载mount -t iso9660 -o ro /dev/cdrom /mnt/cdrom/ 将设备/dev/cdrom挂载到 挂载点 ： /mnt/cdrom中 挂载光盘镜像文件（.iso文件） 123mount -t iso9660 -o loop /home/hadoop/Centos-6.7.DVD.iso /mnt/centos注：挂载的资源在重启后即失效，需要重新挂载。要想自动挂载，可以将挂载信息设置到/etc/fstab配置文件中，如下：/dev/cdrom /mnt/cdrom iso9660 defaults 0 0 卸载 umountumount /mnt/cdrom 存储空间查看df -h 统计文件或文件夹的大小123du -sh /mnt/cdrom/packagesdf -h 查看磁盘的空间wc -lwc 统计文件行数、字数、字节数 系统服务管理1234service sshd statusservice sshd stop service sshd startservice sshd restart 系统启动级别管理12345678910111213vi /etc/inittab # Default runlevel. The runlevels used are: # 0 - halt (Do NOT set initdefault to this) # 1 - Single user mode # 2 - Multiuser, without NFS (The same as 3, if you do not have networking) # 3 - Full multiuser mode # 4 - unused # 5 - X11 # 6 - reboot (Do NOT set initdefault to this) # id:3:initdefault: ## 通常将默认启动级别设置为：3 进程管理1234topfreeps -ef | grep sshkill -9 SSH免密登陆配置SSH工作机制1、相关概念SSH 为 Secure Shell（安全外壳协议） 的缩写。很多ftp、pop和telnet在本质上都是不安全的，因为它们在网络上用明文传送口令和数据，别有用心的人非常容易就可以截获这些口令和数据。而SSH就是专为远程登录会话和其他网络服务提供安全性的协议。 SSH是由客户端和服务端的软件组成的服务端是一个守护进程(sshd)，他在后台运行并响应来自客户端的连接请求。客户端包含ssh程序以及像scp（远程拷贝）、slogin（远程登陆）、sftp（安全文件传输）等其他的应用程序。 2、认证机制从客户端来看，SSH提供两种级别的安全验证。 第一种级别（基于口令的安全验证）只要你知道自己帐号和口令，就可以登录到远程主机。 第二种级别（基于密钥的安全验证）需要依靠密匙，也就是你必须为自己创建一对密匙，并把公用密匙放在需要访问的服务器上。如果你要连接到SSH服务器上，客户端软件就会向服务器发出请求，请求用你的密匙进行安全验证。服务器收到请求之后，先在该服务器上你的主目录下寻找你的公用密匙，然后把它和你发送过来的公用密匙进行比较。如果两个密匙一致，服务器就用公用密匙加密“质询”（challenge）并把它发送给客户端软件。客户端软件收到“质询”之后就可以用你的私人密匙解密再把它发送给服务器。 密钥登陆方式配置假如 A 要登陆 B在A上操作：1/ 首先生成密钥对ssh-keygen (提示时，直接回车即可)2/ 再将A自己的公钥拷贝并追加到B的授权列表文件authorized_keys中ssh-copy-id B 网络管理主机名配置123456781/ 查看主机名hostname2/ 修改主机名(重启后无效)hostname hadoop3/ 修改主机名(重启后永久生效) vi /ect/sysconfig/network IP地址配置1234567891011修改IP地址1/ 方式一：setup用root输入setup命令，进入交互式修改界面2/ 方式二：修改配置文件 一般使用这种方法(重启后永久生效)vi /etc/sysconfig/network-scripts/ifcfg-eth03/ 方式三：ifconfig命令(重启后无效)ifconfig eth0 192.168.12.22 网络服务管理1 后台服务管理12345service network status 查看指定服务的状态service network stop 停止指定服务service network start 启动指定服务service network restart 重启指定服务service --status-all 查看系统中所有的后台服务 2 设置后台服务的自启配置123chkconfig 查看所有服务器自启配置chkconfig iptables off 关掉指定服务的自动启动chkconfig iptables on 开启指定服务的自动启动 转载自： http://www.kuqin.com/shuoit/20160805/352716.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux技巧</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mahout 如何进行 Precision 和 Recall 的计算]]></title>
    <url>%2F2016%2F08%2F18%2FMahout%20%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C%20Precision%20%E5%92%8C%20Recall%20%E7%9A%84%E8%AE%A1%E7%AE%97%2F</url>
    <content type="text"><![CDATA[当为某一个用户做推荐评估时，选择一个临界值，以该临界值为参照为该用户构造一个目标最大击中集（即相关项RelevantItemsIDs），然后将该用户数据中的包含在最大击中集中的物品去除，这样形成一个新的训练集。这个新的训练集中，只是去除了部分数据。然后，使用该训练集为该用户进行推荐。如果推荐的物品包含在最大击中集中，则说明击中。依照这个办法为该用户计算 Precision 和 Recall 值。依照这个办法为所有的用户计算 Precision 和 Recall 值。然后，Precision 和 Recall 的平均值作为模型的 Precision 和 Recall 值。 RecommenderIRStatsEvaluator是一个接口，用于得到推荐系统的准确率，召回率等统计指标。它定义的函数如下12345678910111213141516171819public IRStatistics evaluate(RecommenderBuilder recommenderBuilder, DataModelBuilder dataModelBuilder, DataModel dataModel, IDRescorer rescorer, int at, double relevanceThreshold, doubleevaluationPercentage) /** * @param recommenderBuilder * 它通过public Recommender buildRecommender(DataModel model)定义推荐系统创建的方式； * @param dataModelBuilder * 数据模型创建的方式，如果已经创建好了数据模型，一般这个参数可以为null * @param dataModel * 推荐系统使用的数据模型 * * @param rescorer * 推荐排序的方式，一般情况下可以为null * * @param at * 推荐几个物品（TOPN),，它用来定义计算准确率的时候，一个user可以拥有的相关项items的最大个数，相关项item定义为user对这个item的评价超过了relevanceThreshold的项 * @param relevanceThreshold * 相关临界值 ,和at一起使用定义一个user的相关项 计算临界值123//computeThreshold(prefs)这个方法主要是计算得到当前用户对物品 打分的平均值加上标准差的值double theRelevanceThreshold = Double.isNaN(relevanceThreshold) ? computeThreshold(prefs) : relevanceThreshold; getRelevantItemsIDs按照用户对物品的打分从大到小排序，将大于设定的的relevanceTheshold的值存入relevantItemIDs中，relevantItemIDs最大值为at12345678/** * getRelevantItemsIDs的实现， *1.首先得到userID的Preferences *2.创建一个FastIDSet用来保存相关项的id，大小为at *3.prefs根据值大小排序 *4.遍历pref，如果user对这个item的评价prefs.getValue(i)不小于相关阈值，则将这个item的加入相关项，最多取at个满足条件的item */FastIDSet relevantItemIDs = dataSplitter.getRelevantItemsIDs(userID, at, theRelevanceThreshold, dataModel); 12345678910111213141516public FastIDSet getRelevantItemsIDs(long userID, int at, double relevanceThreshold, DataModel dataModel) throws TasteException &#123; PreferenceArray prefs = dataModel.getPreferencesFromUser(userID); //定义最大理论击中物品集合 FastIDSet relevantItemIDs = new FastIDSet(at); //将用户的打分排序 prefs.sortByValueReversed(); for (int i = 0; i &lt; prefs.length() &amp;&amp; relevantItemIDs.size() &lt; at; i++) &#123; if (prefs.getValue(i) &gt;= relevanceThreshold) &#123; relevantItemIDs.add(prefs.getItemID(i)); &#125; &#125; return relevantItemIDs; &#125; 获取训练集这个训练集的构造规则是： 对于其它的用户，将他们所有的（item，preference）都加入训练集； 对于这个用户user，将它的除了相关项之外的其它项的喜好加入训练集； 然后，我们使用推荐算法进行推荐。推荐的时候，我们就可能给UserID 推荐移除的物品或者其他的物品。1234567FastByIDMap&lt;PreferenceArray&gt; trainingUsers = new FastByIDMap&lt;PreferenceArray&gt;(dataModel.getNumUsers()); //对所有用户进行处理 //processOtherUser :Adds a single user and all their preferences to the training model. LongPrimitiveIterator it2 = dataModel.getUserIDs(); while (it2.hasNext()) &#123; dataSplitter.processOtherUser(userID, relevantItemIDs, trainingUsers, it2.nextLong(), dataModel); &#125; 构造训练模型1DataModel trainingModel = dataModelBuilder == null ? new GenericDataModel(trainingUsers): dataModelBuilder.buildDataModel(trainingUsers); 进行推荐12345Recommender recommender = recommenderBuilder.buildRecommender(trainingModel);int intersectionSize = 0;List&lt;RecommendedItem&gt; recommendedItems = recommender.recommend(userID, at, rescorer); 计算推荐结果包含在相关项中的个数12345678List&lt;RecommendedItem&gt; recommendedItems = recommender.recommend(userID, at, rescorer); for (RecommendedItem recommendedItem : recommendedItems) &#123; if (relevantItemIDs.contains(recommendedItem.getItemID())) &#123; intersectionSize++; &#125; &#125; 计算查全率、查准率123456789int numRecommendedItems = recommendedItems.size(); // Precision if (numRecommendedItems &gt; 0) &#123; precision.addDatum((double) intersectionSize / (double) numRecommendedItems); &#125; // Recall recall.addDatum((double) intersectionSize / (double) numRelevantItems); 参考 http://pan.baidu.com/s/1pKE97wJhttp://www.myexception.cn/cloud/1983215.html]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>Mahout</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu16.04安装Oracle11g]]></title>
    <url>%2F2016%2F08%2F17%2FUbuntu16.04%E5%AE%89%E8%A3%85Oracle11g%2F</url>
    <content type="text"><![CDATA[Oracle用户创建1234sudo groupadd oinstallsudo groupadd dbasudo adduser -g oinstall -G dba -s oraclesudo passwd oracle 安装依赖1234567mkdir /tmp/libstdc++5cd /tmp/libstdc++5wget http://mirrors.kernel.org/ubuntu/pool/universe/g/gcc-3.3/libstdc++5_3.3.6-28ubuntu1_amd64.debwget http://mirrors.kernel.org/ubuntu/pool/universe/g/gcc-3.3/libstdc++5_3.3.6-28ubuntu1_i386.debsudo dpkg --force-architecture -i libstdc++5_3.3.6-28ubuntu1_i386.debsudo mv /usr/lib/libstdc++.so.5* /usr/lib32/sudo dpkg -i libstdc++5_3.3.6-28ubuntu1_amd64.deb 12345678910111213141516171819202122232425262728293031323334353637383940414243sudo apt-get updatesudo apt-get dist-upgradesudo apt-get install automakesudo apt-get install autotools-devsudo apt-get install binutilssudo apt-get install bzip2sudo apt-get install elfutilssudo apt-get install expatsudo apt-get install gawksudo apt-get install gccsudo apt-get install gcc-multilibsudo apt-get install g++-multilibsudo apt-get install ia32-libssudo apt-get install kshsudo apt-get install lesssudo apt-get install lesstif2sudo apt-get install lesstif2-devsudo apt-get install lib32z1sudo apt-get install libaio1sudo apt-get install libaio-devsudo apt-get install libc6-devsudo apt-get install libc6-dev-i386sudo apt-get install libc6-i386sudo apt-get install libelf-devsudo apt-get install libltdl-devsudo apt-get install libmotif4sudo apt-get install libodbcinstq4-1 libodbcinstq4-1:i386sudo apt-get install libpth-devsudo apt-get install libpthread-stubs0sudo apt-get install libpthread-stubs0-devsudo apt-get install libstdc++5sudo apt-get install lsb-cxxsudo apt-get install makesudo apt-get install openssh-serversudo apt-get install pdkshsudo apt-get install rlwrapsudo apt-get install rpmsudo apt-get install sysstatsudo apt-get install unixodbcsudo apt-get install unixodbc-devsudo apt-get install unzipsudo apt-get install x11-utilssudo apt-get install zlibc 修改/etc/sysctl.conf增加以下内容123456789101112kernel.sem = 250 32000 100 128kernel.shmall = 2097152kernel.shmmni = 4096kernel.shmmax=1073741824net.ipv4.ip_local_port_range = 9000 65500net.core.rmem_default = 262144net.core.rmem_max = 4194304net.core.wmem_default = 262144net.core.wmem_max = 1048576fs.aio-max-nr = 1048576fs.file-max = 6815744vm.hugetlb_shm_group = 1002 123456# 以下包安装不成功，先略过sudo apt-get install lesstif2sudo apt-get install lesstif2-devsudo apt-get install libpthread-stubs0sudo apt-get install lsb-cxxsudo apt-get install pdksh 运行一下命令更新内核参数sudo sysctl -p 修改/etc/security/limits.conf增加以下内容12345oracle soft nproc 2047oracle hard nproc 16384oracle soft nofile 1024oracle hard nofile 65536oracle soft stack 10240 修改/etc/pam.d/login增加以下内容12session required /lib/security/pam_limits.sosession required pam_limits.so 欺骗oracle的安装程序oracle本身并不支持ubuntu来安装，所以要进行欺骗oracle的安装程序（sudo执行）：123456789101112mkdir /usr/lib64ln -s /etc /etc/rc.dln -s /lib/x86_64-linux-gnu/libgcc_s.so.1 /lib64/ln -s /usr/bin/awk /bin/awkln -s /usr/bin/basename /bin/basenameln -s /usr/bin/rpm /bin/rpmln -s /usr/lib/x86_64-linux-gnu/libc_nonshared.a /usr/lib64/ln -s /usr/lib/x86_64-linux-gnu/libpthread_nonshared.a /usr/lib64/ln -s /usr/lib/x86_64-linux-gnu/libstdc++.so.6 /lib64/ln -s /usr/lib/x86_64-linux-gnu/libstdc++.so.6 /usr/lib64/vim /etc/redhat-releaseecho 'Red Hat Linux release 5' &gt; /etc/redhat-release 为Oracle配置环境变量1234567891011#oracle安装目录，第6步创建的文件夹export ORACLE_BASE=/home/oracle/oracle11g#网上说可以随便写export ORACLE_HOME=$ORACLE_BASE/product/11.2.0/dbhome_1#数据库的sidexport ORACLE_SID=orclexport ORACLE_UNQNAME=orcl#默认字符集export NLS_LANG=.AL32UTF8#环境变量export PATH=$&#123;PATH&#125;:$&#123;ORACLE_HOME&#125;/bin/:$ORACLE_HOME/lib64; 安装oracle上面的系统配置完成之后，最好重启一下服务器，使用oracle用户登陆系统。 上传下载好的oracle压缩文件到/home/oracle目录下。 进入/home/oracle目录，执行# unzip linux.x64_11gR2_database_1of2.zip和# unzip linux.x64_11gR2_database_2of2.zip，解压的文件在/home/oracle/database目录中。 设置/home/oracle/database目录的权限： 12# chown oracle:oinstall /home/oracle/database -R# chmod 775 /home/oracle/database -R 进入/home/oracle/database目录，执行$ ./runInstaller，当检查均通过，会出现oracle安装界面,一路next，有一步可以选择字符，选utf8 安装过程可能遇到的问题 Oracle安装界面乱码解决方法执行: 12exportNLS_LANG=AMERICAN_AMERICA.UTF8export LC_ALL=C Error in invoking target ‘install’ of makefile ‘/home/dong/tools/oracle11g/product/11.2.0/dbhome_1/ctx/lib/ins_ctx.mk’. See ‘/home/dong/tools/oraInventory/logs/installActions2015-01-22_09-39-03AM.log’ for details. 解决方法： 从http://download.csdn.net/detail/adnerly/9467935下载，使用rpm安装这个glibc-static-2.17-55.el7.x86_64.rpm资源，安装即可， 然后点击retry ，接着往下执行注:这是网上提供的解决方案，我的系统安装失败，我直接跳过了 Error in invoking target ‘agent nmhs’ of makefile ‘/home/dong/tools/oracle11g/product/11.2.0/dbhome_1/sysman/lib/ins_emagent.mk’ 解决方法： 打开新的终端窗口使用vi命令，打开/home/oracle/oracle11g/product/11.2.0/dbhome_1/sysman/lib/ins_emagent.mk文件，将$(MK_EMAGENT_NMECTL)修改成$(MK_EMAGENT_NMECTL)-lnnz11 即可，然后点击retry ，接着往下执行 Error in invoking target ‘all_no_orcl’ of makefile ‘/home/oracle/oracle11g/product/11.2.0/dbhome_1/rdbms/lib/ins_rdbms.mk’. See ‘/home/dong/tools/Inventory/logs/installActions2016-03-19_02-37-44PM.log’ for details. 解决办法： 打开一个新的终端，输入如下四个命令： 1234567sed -i 's/^\(TNSLSNR_LINKLINE.*\$(TNSLSNR_OFILES)\) \(\$(LINKTTLIBS)\)/\1 -Wl,--no-as-needed \2/g' $ORACLE_HOME/network/lib/env_network.mksed -i 's/^\(ORACLE_LINKLINE.*\$(ORACLE_LINKER)\) \(\$(PL_FLAGS)\)/\1 -Wl,--no-as-needed \2/g' $ORACLE_HOME/rdbms/lib/env_rdbms.mksed -i 's/^\(\$LD \$LD_RUNTIME\) \(\$LD_OPT\)/\1 -Wl,--no-as-needed \2/g' $ORACLE_HOME/bin/genorasdkshsed -i 's/^\(\s*\)\(\$(OCRLIBS_DEFAULT)\)/\1 -Wl,--no-as-needed \2/g' $ORACLE_HOME/srvm/lib/ins_srvm.mk 然后在图形界面点击‘Retry’就能继续安装了。参考 http://www.jianshu.com/p/9b2f601c275d 然后按照安装程序提示最后执行两个脚本12sudo /home/oracle/oraInventory/orainstRoot.sh sudo /home/oracle/oracle11g/product/11.2.0/dbhome_1/root.sh 创建监听，执行$ netca启动配置界面参考 http://www.jianshu.com/p/9b2f601c275d 完成之后，执行命令$ lsnrctl start启动监听服务。 创建数据库实例，执行$ dbca启动配置界面最后验证是否安装成功，浏览器访问https://192.168.1.114:1158/em 创建开机自动启动数据库的脚本开一个普通的字符终端连接到UbuntuServer，运行如下命令：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950# vi /etc/init.d/oracledb文件内容如下：#!/bin/bash## /etc/init.d/oracledb## Run-level Startup script for the Oracle Instance, Listener, and# Web Interfaceexport ORACLE_HOME=/home/oracle/oracle11g/product/11.2.0/dbhome_1export ORACLE_SID=orclexport PATH=$ORACLE_HOME/bin:$PATHORA_OWNR="oracle"# if the executables do not exist -- display errorif [ ! -f $ORACLE_HOME/bin/dbstart -o ! -d $ORACLE_HOME ]thenecho "Oracle startup: cannot start"exit 1fi# depending on parameter -- startup, shutdown, restart# of the instance and listener or usage displaycase "$1" instart)# Oracle listener and instance startupecho -n "Starting Oracle: "su $ORA_OWNR -c "$ORACLE_HOME/bin/lsnrctl start"su $ORA_OWNR -c "$ORACLE_HOME/bin/dbstart"touch /var/lock/oraclesu $ORA_OWNR -c "$ORACLE_HOME/bin/emctl start dbconsole"echo "OK";;stop)# Oracle listener and instance shutdownecho -n "Shutdown Oracle: "su $ORA_OWNR -c "$ORACLE_HOME/bin/lsnrctl stop"su $ORA_OWNR -c "$ORACLE_HOME/bin/dbshut"rm -f /var/lock/oraclesu $ORA_OWNR -c "$ORACLE_HOME/bin/emctl stop dbconsole"echo "OK";;reload|restart)$0 stop$0 start;;*)echo "Usage: `basename $0` start|stop|restart|reload"exit 1esacexit 0 再运行如下命令设置权限，并放到启动脚本中去：12# chmod 755 /etc/init.d/oracledb# update-rc.d oracledb defaults 99 最后：# vi /etc/oratab把文件中的N改成Y，即”orcl:/opt/oracle/product/db:N”修改为”orcl:/opt/oracle/product/db:Y”。 常用命令123456789101112$ ps -ef|grep ora_|grep -v grep --&gt;查看oracle进程$ ps -ef|grep tnslsnr|grep -v grep --&gt;查看oracle的监听进程$ lsnrctl start --&gt;启动监听$ dbstart --&gt;启动数据库$ dbstop --&gt;停止数据库$ emctl start dbconsole --&gt;启动em控制台$ isqlplusctl start --&gt;启动pl/sql$ sqlplus '/as sysdba' --&gt;登录sqlplus$ env --&gt;输出当前用户的环境变量$ netca --&gt;启用监听配置程序 参考文章 CentOS6.7安装Oracle 11g2R傻瓜图文教程Ubuntu 14.04安装Oracle11g 64位ubuntu16.04安装oracle11gUbuntu Server 11.04 安装 Oracle 11g r2 图解教程]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Oracle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[推荐系统评测指标—准确率(Precision)、召回率(Recall)、F值(F-Measure)]]></title>
    <url>%2F2016%2F08%2F17%2F%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E8%AF%84%E6%B5%8B%E6%8C%87%E6%A0%87%2F</url>
    <content type="text"><![CDATA[下面简单列举几种常用的推荐系统评测指标： 准确率与召回率（Precision &amp; Recall）准确率和召回率是广泛用于信息检索和统计学分类领域的两个度量值，用来评价结果的质量。其中精度是检索出相关文档数与检索出的文档总数的比率，衡量的是检索系统的查准率；召回率是指检索出的相关文档数和文档库中所有的相关文档数的比率，衡量的是检索系统的查全率。 一般来说，Precision就是检索出来的条目（比如：文档、网页等）有多少是准确的，Recall就是所有准确的条目有多少被检索出来了。 正确率、召回率和 F 值是在鱼龙混杂的环境中，选出目标的重要评价指标。不妨看看这些指标的定义先： 正确率 = 提取出的正确信息条数 / 提取出的信息条数 召回率 = 提取出的正确信息条数 / 样本中的信息条数 两者取值在0和1之间，数值越接近1，查准率或查全率就越高。 F值 = 正确率 召回率 2 / (正确率 + 召回率) （F 值即为正确率和召回率的调和平均值） 放到推荐系统中便是 准确率 = 推荐给user的Items中属于user相关项的个数 / 推荐给user的Items的总个数 召回率 = 推荐给user的Items中属于user相关项的个数 / user的所有相关项item个数 不妨举这样一个例子：某池塘有1400条鲤鱼，300只虾，300只鳖。现在以捕鲤鱼为目的。撒一大网，逮着了700条鲤鱼，200只虾，100只鳖。那么，这些指标分别如下： 正确率 = 700 / (700 + 200 + 100) = 70% 召回率 = 700 / 1400 = 50% F值 = 70% 50% 2 / (70% + 50%) = 58.3% 不妨看看如果把池子里的所有的鲤鱼、虾和鳖都一网打尽，这些指标又有何变化： 正确率 = 1400 / (1400 + 300 + 300) = 70% 召回率 = 1400 / 1400 = 100% F值 = 70% 100% 2 / (70% + 100%) = 82.35% 由此可见，正确率是评估捕获的成果中目标成果所占得比例；召回率，顾名思义，就是从关注领域中，召回目标类别的比例；而F值，则是综合这二者指标的评估指标，用于综合反映整体的指标。 当然希望检索结果Precision越高越好，同时Recall也越高越好，但事实上这两者在某些情况下有矛盾的。比如极端情况下，我们只搜索出了一个结果，且是准确的，那么Precision就是100%，但是Recall就很低；而如果我们把所有结果都返回，那么比如Recall是100%，但是Precision就会很低。因此在不同的场合中需要自己判断希望Precision比较高或是Recall比较高。如果是做实验研究，可以绘制Precision-Recall曲线来帮助分析。 综合评价指标（F-Measure）P和R指标有时候会出现的矛盾的情况，这样就需要综合考虑他们，最常见的方法就是F-Measure（又称为F-Score）。 F-Measure是Precision和Recall加权调和平均： 当参数α=1时，就是最常见的F1，也即 可知F1综合了P和R的结果，当F1较高时则能说明试验方法比较有效。 E值E值表示查准率P和查全率R的加权平均值，当其中一个为0时，E值为1，其计算公式： b越大，表示查准率的权重越大。 平均正确率（Average Precision, AP）平均正确率表示不同查全率的点上的正确率的平均。 转载自： http://bookshadow.com/weblog/2014/06/10/precision-recall-f-measure/]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu 16.04 mysql安装配置]]></title>
    <url>%2F2016%2F08%2F16%2FUbuntu%2016.04%20mysql%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[安装mysqlsudo apt-get install mysql-server mysql-client 测试是否安装成功sudo netstat -tap | grep mysql 相关操作 登录 mysql -uroot -p 检查MySQL服务器占用端口 netstat -nlt|grep 3306 检查MySQL服务器系统进程 ps -aux|grep mysql 查看数据库的字符集编码 show variables like &#39;%char%&#39;; 让MySQL服务器被远程访问 打开mysql配置文件 123sudo vim /etc/mysql/my.cnf#找到将bind-address = 127.0.0.1注销​#bind-address = 127.0.0.1 修改后，重启MySQL服务器sudo /etc/init.d/mysql restart 重新登录mysql -uroot -p 12grant all privileges on *.* to 'root'@'%' identified by 'xxxxxx';flush privileges; 检查MySQL服务器占用端口 12~ netstat -nlt|grep 3306 tcp 0 0 0.0.0.0:3306 0.0.0.0:* LISTEN 我们看到从之间的网络监听从 127.0.0.1:3306 变成 0 0.0.0.0:3306，表示MySQL已经允许远程登陆访问。 将字符编码设置为UTF-8默认情况下，MySQL的字符集是latin1，因此在存储中文的时候，会出现乱码的情况，所以我们需要把字符集统一改成UTF-8。打开mysql配置文件sudo vim /etc/mysql/my.cnf 12345678910111213a） 打开mysql配置文件： vim/etc/mysql/my.cnfb） 在[client]下追加： default-character-set=utf8c） 在[mysqld]下追加： character-set-server=utf8d） 在[mysql]下追加： default-character-set=utf8 修改后，重启MySQL服务器,并登录mysql -uroot -p 再次查看字符串编码1234567891011121314mysql&gt; show variables like '%char%';+--------------------------+----------------------------+| Variable_name | Value |+--------------------------+----------------------------+| character_set_client | utf8 || character_set_connection | utf8 || character_set_database | utf8 || character_set_filesystem | binary || character_set_results | utf8 || character_set_server | utf8 || character_set_system | utf8 || character_sets_dir | /usr/share/mysql/charsets/ |+--------------------------+----------------------------+8 rows in set (0.00 sec)]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mahout0.10.1安装]]></title>
    <url>%2F2016%2F08%2F15%2FMahout0.10.1%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[解压安装包编辑环境变量sudo vim /etc/profile123456#MAHOUTexport MAHOUT_HOME=/home/ubuntu/cloud/mahout-0.10.1export MAHOUT_CONF_DIR=$MAHOUT_HOME/confexport HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoopexport PATH=$PATH:$&#123;JAVA_HOME&#125;/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$MAHOUT_HOME/conf:$MAHOUT_HOME/bin: 更新配置source /etc/profile 输入mahout测试 出现许多算法进行kmeans算法简单运行 下载测试数据集synthetic_control.data http://archive.ics.uci.edu/ml/databases/synthetic_control/ 在hdfs上创建目录 /user/ubuntu/testdata 上传测试数据hadoop fs -put synthetic_control.data /user/ubuntu/testdata 运行mahout org.apache.mahout.clustering.syntheticcontrol.kmeans.Job 参考 http://www.cnblogs.com/zhangduo/p/4679907.html]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Mahout</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop2.7.2集群搭建]]></title>
    <url>%2F2016%2F08%2F15%2FHadoop2.7.2%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[四台电脑集群1234192.168.1.111 master192.168.1.112 slave1192.168.1.113 slave2192.168.1.114 slave3 修改hostsvim /etc/hosts 配置master到其它三台slave的免密码登陆各服务器上使用 ssh-keygen -t rsa 一路按回车就行了。刚才都作甚了呢？主要是设置ssh的密钥和密钥的存放路径。 路径为~/.ssh下。打开~/.ssh 下面有三个文件authorized_keys，已认证的keysid_rsa，私钥id_rsa.pub，公钥 三个文件。下面就是关键的地方了，（我们要做ssh认证。进行下面操作前，可以先搜关于认证和加密区别以及各自的过程。）①在master上将公钥放到authorized_keys里。命令：sudo cat id_rsa.pub &gt;&gt; authorized_keys②将master上的authorized_keys放到其他linux的~/.ssh目录下。命令：sudo scp authorized_keys ubuntu@192.168.1.112:~/.sshsudo scp authorized_keys 远程主机用户名@远程主机名或ip:存放路径。③修改authorized_keys权限，命令：chmod 644 authorized_keys④测试是否成功ssh slave1 输入用户名密码，然后退出，再次ssh slave1不用密码，直接进入系统。这就表示成功了。 安装jdk1.7 mkdir /usr/java sudo tar -zxvf jdk-7u79-linux-x64.tar.gz -C /usr/java vim /etc/profile 123export JAVA_HOME=/usr/java/jdk1.7.0_79export CLASSPATH=.:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/libexport PATH=$&#123;JAVA_HOME&#125;/bin:$PATH 刷新配置source /etc/profile 测试java -version 关闭每台机器的防火墙sudo ufw disable (重启生效) 创建/home/cloud目录mkdir ~/cloud 解压hadooptar -zxvf hadoop-2.7.2.tar.gz -C ./cloud 配置hadoop-env.sh yarn-env.sh修改JAVA_HOME值export JAVA_HOME=/usr/java/jdk1.7.0_79 创建文件夹123mkdir /home/ubuntu/cloud/hadoop-2.7.2/tmpmkdir /home/ubuntu/cloud/hadoop-2.7.2/dfs/datamkdir /home/ubuntu/cloud/hadoop-2.7.2/dfs/name 配置slaves123slave1slave2slave3 配置core-site.xml1234567891011121314151617181920212223&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://master:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;io.file.buffer.size&lt;/name&gt; &lt;value&gt;131072&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;file:/home/ubuntu/cloud/hadoop-2.7.2/tmp&lt;/value&gt; &lt;description&gt;Abase for other temporary directories.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.ubuntu.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.ubuntu.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 配置hdfs-site.xml1234567891011121314151617181920212223242526&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;master:9001&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;4&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.permissions.enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/home/ubuntu/cloud/hadoop-2.7.2/dfs/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:/home/ubuntu/cloud/hadoop-2.7.2/dfs/data&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 配置mapred-site.xml12345678910111213&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;master:10020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;master:19888&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 配置yarn-site.xml123456789101112131415161718192021222324252627282930&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt; &lt;value&gt;master:8032&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt; &lt;value&gt;master:8030&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt; &lt;value&gt;master:8031&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.admin.address&lt;/name&gt; &lt;value&gt;master:8033&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt; &lt;value&gt;master:8088&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 复制到其它节点123sudo scp -r /home/ubuntu/cloud ubuntu@slave1:~/sudo scp -r /home/ubuntu/cloud ubuntu@slave2:~/sudo scp -r /home/ubuntu/cloud ubuntu@slave3:~/ 配置环境变量vim /etc/profile12export HADOOP_HOME=/home/ubuntu/cloud/hadoop-2.7.2export PATH=$PATH:$&#123;JAVA_HOME&#125;/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin: 启动hadoop 格式化namenode hadoop namenode –format 启动hdfs start-dfs.sh 启动yarn start-yarn.sh web端查看 http://master:8088 http://master:50070]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubuntu相关设置]]></title>
    <url>%2F2016%2F08%2F15%2Fubuntu%E7%9B%B8%E5%85%B3%E8%AE%BE%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[修改时区sudo dpkg-reconfigure tzdata 出现时区列表，按照提示选择“Asia/Shanghai” 结果如下123456789101112131415161718192021222324252627282930Package configuration ┌───────────────────────┤ Configuring tzdata ├───────────────────────┐ │ Please select the city or region corresponding to your time zone. │ │ │ │ Time zone: │ │ │ │ Qyzylorda ↑ │ │ Rangoon ▒ │ │ Riyadh ▒ │ │ Sakhalin ▒ │ │ Samarkand ▒ │ │ Seoul ▒ │ │ Shanghai ▮ │ │ Singapore ▒ │ │ Srednekolymsk ▒ │ │ Taipei ↓ │ │ │ │ │ │ &lt;Ok&gt; &lt;Cancel&gt; │ │ │ └────────────────────────────────────────────────────────────────────┘ Current default time zone: 'Asia/Shanghai'Local time is now: Mon Aug 15 00:11:59 CST 2016.Universal Time is now: Sun Aug 14 16:11:59 UTC 2016.ubuntu@master:~$ dateMon Aug 15 00:12:13 CST 2016]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux技巧</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubuntu设置静态ip]]></title>
    <url>%2F2016%2F08%2F14%2Fubuntu%E8%AE%BE%E7%BD%AE%E9%9D%99%E6%80%81ip%2F</url>
    <content type="text"><![CDATA[找到文件并作如下修改sudo vim /etc/network/interfaces 12345678910# The primary network interfaceauto eno1iface eno1 inet static address 192.168.1.111 netmask 255.255.255.0 network 192.168.1.0 broadcast 192.168.1.255 gateway 192.168.1.1 # dns-* options are implemented by the resolvconf package, if installed dns-nameservers 202.101.172.35 202.101.172.46 修改dns解析因为以前是dhcp解析，所以会自动分配dns服务器地址 而一旦设置为静态ip后就没有自动获取到的dns服务器了 要自己设置一个 sudo vim /etc/resolv.conf 写上一个公网的DNS,以下为浙江杭州电信DNS 1234# Dynamic resolv.conf(5) file for glibc resolver(3) generated by resolvconf(8)# DO NOT EDIT THIS FILE BY HAND -- YOUR CHANGES WILL BE OVERWRITTENnameserver 202.101.172.35nameserver 202.101.172.46 （注意：8.8.8.8是谷歌的DNS服务器，但是解析速度慢，还是找到一个国内的dns来用） 重启网卡sudo /etc/init.d/networking restart]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM内存配置]]></title>
    <url>%2F2016%2F07%2F26%2FJVM%E5%86%85%E5%AD%98%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[参数中-vmargs的意思是设置JVM参数，所以后面的其实都是JVM的参数了，我们首先了解一下JVM内存管理的机制，然后再解释每个参数代表的含义。 堆(Heap)和非堆(Non-heap)内存按照官方的说法：“Java虚拟机具有一个堆，堆是运行时数据区域，所有类实例和数组的内存均从此处分配。堆是在Java虚拟机启动时创建的。”“在JVM中堆之外的内存称为非堆内存(Non-heapmemory)”。可以看出JVM主要管理两种类型的内存：堆和非堆。简单来说堆就是Java代码可及的内存，是留给开发人员使用的；非堆就是JVM留给自己用的，所以方法区、JVM内部处理或优化所需的内存(如JIT编译后的代码缓存)、每个类结构(如运行时常数池、字段和方法数据)以及方法和构造方法的代码都在非堆内存中。 堆内存分配JVM初始分配的内存由-Xms指定，默认是物理内存的1/64；JVM最大分配的内存由-Xmx指定，默认是物理内存的1/4。默认空余堆内存小于40%时，JVM就会增大堆直到-Xmx的最大限制；空余堆内存大于70%时，JVM会减少堆直到-Xms的最小限制。因此服务器一般设置-Xms、-Xmx相等以避免在每次GC后调整堆的大小。 非堆内存分配 JVM使用-XX:PermSize设置非堆内存初始值，默认是物理内存的1/64；由XX:MaxPermSize设置最大非堆内存的大小，默认是物理内存的1/4。 JVM内存限制(最大值)首先JVM内存限制于实际的最大物理内存(废话！呵呵)，假设物理内存无限大的话，JVM内存的最大值跟操作系统有很大的关系。简单的说就32位处理器虽然可控内存空间有4GB,但是具体的操作系统会给一个限制，这个限制一般是2GB-3GB（一般来说Windows系统下为1.5G-2G，Linux系统下为2G-3G），而64bit以上的处理器就不会有限制了。 减少jvm内存回收引起的eclipse卡的问题这个主要是jvm在client模式，进行内存回收时，会停下所有的其它工作，带回收完毕才去执行其它任务，在这期间eclipse就卡住了。所以适当的增加jvm申请的内存大小来减少其回收的次数甚至不回收，就会是卡的现象有明显改善。 主要通过以下的几个jvm参数来设置堆内存的： -Xmx512m 最大总堆内存，一般设置为物理内存的1/4 -Xms512m 初始总堆内存，一般将它设置的和最大堆内存一样大，这样就不需要根据当前堆使用情况而调整堆的大小了 -Xmn192m 年轻带堆内存，sun官方推荐为整个堆的3/8 堆内存的组成 总堆内存 = 年轻带堆内存 + 年老带堆内存 + 持久带堆内存 年轻带堆内存 对象刚创建出来时放在这里 年老带堆内存 对象在被真正会回收之前会先放在这里 持久带堆内存 class文件，元数据等放在这里 -XX:PermSize=128m 持久带堆的初始大小 -XX:MaxPermSize=128m 持久带堆的最大大小，eclipse默认为256m。如果要编译jdk这种，一定要把这个设的很大，因为它的类太多了。 eclipse运行配置Eclipse -&gt; Run -&gt; Run Configurations -&gt; Arguments -&gt; VM arguments或者 Run as -&gt; Run Configurations -&gt; Arguments -&gt; VM arguments -Xms2048m -Xmx2048m -Xms是设置内存初始化的大小(如上面的2048m)-Xmx是设置最大能够使用内存的大小（如上面的2048m, 最好不要超过物理内存）也可通过 eclipse.ini配置 参考文章：Eclipse中进行JVM内存设置JVM监控与调优JVM调优总结(这个总结得比较全面)JVM性能调优]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>JVM</tag>
        <tag>eclipse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mahout Item-based推荐的分布式实现]]></title>
    <url>%2F2016%2F07%2F25%2FMahout%20Item-based%E6%8E%A8%E8%8D%90%E5%88%86%E5%B8%83%E5%BC%8F%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[Mahout API地址：http://apache.github.io/mahout/0.10.1/docs/mahout-mr/overview-summary.html Mahout推荐算法API详解 Mahout算法框架自带的推荐器有下面这些： GenericUserBasedRecommender：基于用户的推荐器，用户数量少时速度快； GenericItemBasedRecommender：基于商品推荐器，商品数量少时速度快，尤其当外部提供了商品相似度数据后效率更好； SlopeOneRecommender：基于slope-one算法的推荐器，在线推荐或更新较快，需要事先大量预处理运算，物品数量少时较好； SVDRecommender：奇异值分解，推荐效果较好，但之前需要大量预处理运算； KnnRecommender：基于k近邻算法(KNN)，适合于物品数量较小时； TreeClusteringRecommender：基于聚类的推荐器，在线推荐较快，之前需要大量预处理运算，用户数量较少时效果好； Mahout最常用的三个推荐器是上述的前三个，本文主要讨论前两种的使用。 接口相关介绍基于用户或物品的推荐器主要包括以下几个接口： DataModel 是用户喜好信息的抽象接口，它的具体实现支持从任意类型的数据源抽取用户喜好信息。Taste 默认提供 JDBCDataModel 和 FileDataModel，分别支持从数据库和文件中读取用户的喜好信息。UserSimilarity 和 ItemSimilarity。UserSimilarity 用于定义两个用户间的相似度，它是基于协同过滤的推荐引擎的核心部分，可以用来计算用户的“邻居”，这里我们将与当前用户口味相似的用户称为他的邻居。ItemSimilarity 类似的，计算内容之间的相似度。UserNeighborhood 用于基于用户相似度的推荐方法中，推荐的内容是基于找到与当前用户喜好相似的邻居用户的方式产生的。UserNeighborhood 定义了确定邻居用户的方法，具体实现一般是基于 UserSimilarity 计算得到的。Recommender 是推荐引擎的抽象接口，Taste 中的核心组件。程序中，为它提供一个 DataModel，它可以计算出对不同用户的推荐内容。实际应用中，主要使用它的实现类 GenericUserBasedRecommender 或者 GenericItemBasedRecommender，分别实现基于用户相似度的推荐引擎或者基于内容的推荐引擎。RecommenderEvaluator：评分器。RecommenderIRStatsEvaluator：搜集推荐性能相关的指标，包括准确率、召回率等等。目前，Mahout为DataModel提供了以下几种实现： org.apache.mahout.cf.taste.impl.model.GenericDataModel org.apache.mahout.cf.taste.impl.model.GenericBooleanPrefDataModel org.apache.mahout.cf.taste.impl.model.PlusAnonymousUserDataModel org.apache.mahout.cf.taste.impl.model.file.FileDataModel org.apache.mahout.cf.taste.impl.model.hbase.HBaseDataModel org.apache.mahout.cf.taste.impl.model.cassandra.CassandraDataModel org.apache.mahout.cf.taste.impl.model.mongodb.MongoDBDataModel org.apache.mahout.cf.taste.impl.model.jdbc.SQL92JDBCDataModel org.apache.mahout.cf.taste.impl.model.jdbc.MySQLJDBCDataModel org.apache.mahout.cf.taste.impl.model.jdbc.PostgreSQLJDBCDataModel org.apache.mahout.cf.taste.impl.model.jdbc.GenericJDBCDataModel org.apache.mahout.cf.taste.impl.model.jdbc.SQL92BooleanPrefJDBCDataModel org.apache.mahout.cf.taste.impl.model.jdbc.MySQLBooleanPrefJDBCDataModel org.apache.mahout.cf.taste.impl.model.jdbc.PostgreBooleanPrefSQLJDBCDataModel org.apache.mahout.cf.taste.impl.model.jdbc.ReloadFromJDBCDataModel从类名上就可以大概猜出来每个DataModel的用途，奇怪的是竟然没有HDFS的DataModel，有人实现了一个，请参考MAHOUT-1579。 UserSimilarity 和 ItemSimilarity 相似度实现有以下几种： CityBlockSimilarity：基于Manhattan距离相似度 EuclideanDistanceSimilarity：基于欧几里德距离计算相似度 LogLikelihoodSimilarity：基于对数似然比的相似度 PearsonCorrelationSimilarity：基于皮尔逊相关系数计算相似度 SpearmanCorrelationSimilarity：基于皮尔斯曼相关系数相似度 TanimotoCoefficientSimilarity：基于谷本系数计算相似度 UncenteredCosineSimilarity：计算 Cosine 相似度以上相似度的说明，请参考Mahout推荐引擎介绍。 UserNeighborhood 主要实现有两种： NearestNUserNeighborhood：对每个用户取固定数量N个最近邻居 ThresholdUserNeighborhood：对每个用户基于一定的限制，取落在相似度限制以内的所有用户为邻居 Recommender分为以下几种实现： GenericUserBasedRecommender：基于用户的推荐引擎 GenericBooleanPrefUserBasedRecommender：基于用户的无偏好值推荐引擎 GenericItemBasedRecommender：基于物品的推荐引擎 GenericBooleanPrefItemBasedRecommender：基于物品的无偏好值推荐引擎 RecommenderEvaluator有以下几种实现： AverageAbsoluteDifferenceRecommenderEvaluator：计算平均差值 RMSRecommenderEvaluator：计算均方根差 RecommenderIRStatsEvaluator的实现类是GenericRecommenderIRStatsEvaluator。 单机运行首先，需要在maven中加入对mahout的依赖：1234567891011121314151617181920212223&lt;dependency&gt; &lt;groupId&gt;org.apache.mahout&lt;/groupId&gt; &lt;artifactId&gt;mahout-core&lt;/artifactId&gt; &lt;version&gt;0.9&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.mahout&lt;/groupId&gt; &lt;artifactId&gt;mahout-integration&lt;/artifactId&gt; &lt;version&gt;0.9&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.mahout&lt;/groupId&gt; &lt;artifactId&gt;mahout-math&lt;/artifactId&gt; &lt;version&gt;0.9&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.mahout&lt;/groupId&gt; &lt;artifactId&gt;mahout-examples&lt;/artifactId&gt; &lt;version&gt;0.9&lt;/version&gt;&lt;/dependency&gt; 基于用户的推荐，以FileDataModel为例： 123456789101112131415161718File modelFile modelFile = new File("intro.csv");DataModel model = new FileDataModel(modelFile);//用户相似度，使用基于皮尔逊相关系数计算相似度UserSimilarity similarity = new PearsonCorrelationSimilarity(model);//选择邻居用户，使用NearestNUserNeighborhood实现UserNeighborhood接口，选择邻近的4个用户UserNeighborhood neighborhood = new NearestNUserNeighborhood(4, similarity, model);Recommender recommender = new GenericUserBasedRecommender(model, neighborhood, similarity);//给用户1推荐4个物品List&lt;RecommendedItem&gt; recommendations = recommender.recommend(1, 4);for (RecommendedItem recommendation : recommendations) &#123; System.out.println(recommendation);&#125; 注意： FileDataModel要求输入文件中的字段分隔符为逗号或者制表符，如果你想使用其他分隔符，你可以扩展一个FileDataModel的实现，例如，mahout中已经提供了一个解析MoiveLens的数据集（分隔符为::）的实现GroupLensDataModel。 GenericUserBasedRecommender是基于用户的简单推荐器实现类，推荐主要参照传入的DataModel和UserNeighborhood，总体是三个步骤： 从UserNeighborhood获取当前用户Ui最相似的K个用户集合{U1, U2, …Uk}； 从这K个用户集合排除Ui的偏好商品，剩下的Item集合为{Item0, Item1, …Itemm}； 对Item集合里每个Itemj计算Ui可能偏好程度值pref(Ui, Itemj)，并把Item按此数值从高到低排序，前N个item推荐给用户Ui。 对相同用户重复获得推荐结果，我们可以改用CachingRecommender来包装GenericUserBasedRecommender对象，将推荐结果缓存起来： Recommender cachingRecommender = new CachingRecommender(recommender); 上面代码可以在main方法中直接运行，然后，我们可以获取推荐模型的评分：1234567891011121314//使用平均绝对差值获得评分RecommenderEvaluator evaluator = new AverageAbsoluteDifferenceRecommenderEvaluator();// 用RecommenderBuilder构建推荐引擎RecommenderBuilder recommenderBuilder = new RecommenderBuilder() &#123; @Override public Recommender buildRecommender(DataModel model) throws TasteException &#123; UserSimilarity similarity = new PearsonCorrelationSimilarity(model); UserNeighborhood neighborhood = new NearestNUserNeighborhood(4, similarity, model); return new GenericUserBasedRecommender(model, neighborhood, similarity); &#125;&#125;;// Use 70% of the data to train; test using the other 30%.double score = evaluator.evaluate(recommenderBuilder, null, model, 0.7, 1.0);System.out.println(score); 接下来，可以获取推荐结果的查准率和召回率：123456789101112131415RecommenderIRStatsEvaluator statsEvaluator = new GenericRecommenderIRStatsEvaluator();// Build the same recommender for testing that we did last time:RecommenderBuilder recommenderBuilder = new RecommenderBuilder() &#123; @Override public Recommender buildRecommender(DataModel model) throws TasteException &#123; UserSimilarity similarity = new PearsonCorrelationSimilarity(model); UserNeighborhood neighborhood = new NearestNUserNeighborhood(4, similarity, model); return new GenericUserBasedRecommender(model, neighborhood, similarity); &#125;&#125;;// 计算推荐4个结果时的查准率和召回率IRStatistics stats = statsEvaluator.evaluate(recommenderBuilder,null, model, null, 4, GenericRecommenderIRStatsEvaluator.CHOOSE_THRESHOLD,1.0);System.out.println(stats.getPrecision());System.out.println(stats.getRecall()); 如果是基于物品的推荐，代码大体相似，只是没有了UserNeighborhood，然后将上面代码中的User换成Item即可，完整代码如下：12345678910111213141516171819202122232425262728293031323334353637File modelFile modelFile = new File("intro.csv");DataModel model = new FileDataModel(new File(file));// Build the same recommender for testing that we did last time:RecommenderBuilder recommenderBuilder = new RecommenderBuilder() &#123; @Override public Recommender buildRecommender(DataModel model) throws TasteException &#123; ItemSimilarity similarity = new PearsonCorrelationSimilarity(model); return new GenericItemBasedRecommender(model, similarity); &#125;&#125;;//获取推荐结果List&lt;RecommendedItem&gt; recommendations = recommenderBuilder.buildRecommender(model).recommend(1, 4);for (RecommendedItem recommendation : recommendations) &#123; System.out.println(recommendation);&#125;//计算评分RecommenderEvaluator evaluator = new AverageAbsoluteDifferenceRecommenderEvaluator();// Use 70% of the data to train; test using the other 30%.double score = evaluator.evaluate(recommenderBuilder, null, model, 0.7, 1.0);System.out.println(score);//计算查全率和查准率RecommenderIRStatsEvaluator statsEvaluator = new GenericRecommenderIRStatsEvaluator();// Evaluate precision and recall "at 2":IRStatistics stats = statsEvaluator.evaluate(recommenderBuilder, null, model, null, 4, GenericRecommenderIRStatsEvaluator.CHOOSE_THRESHOLD, 1.0);System.out.println(stats.getPrecision());System.out.println(stats.getRecall()); 在Spark中运行在Spark中运行，需要将Mahout相关的jar添加到Spark的classpath中，修改/etc/spark/conf/spark-env.sh，添加下面两行代码： 12SPARK_DIST_CLASSPATH="$SPARK_DIST_CLASSPATH:/usr/lib/mahout/lib/*"SPARK_DIST_CLASSPATH="$SPARK_DIST_CLASSPATH:/usr/lib/mahout/*" 然后，以本地模式在spark-shell中运行下面代码交互测试：12345678910111213141516171819202122//注意：这里是本地目录val model = new FileDataModel(new File("intro.csv"))val evaluator = new RMSRecommenderEvaluator()val recommenderBuilder = new RecommenderBuilder &#123; override def buildRecommender(dataModel: DataModel): Recommender = &#123; val similarity = new LogLikelihoodSimilarity(dataModel) new GenericItemBasedRecommender(dataModel, similarity) &#125;&#125;val score = evaluator.evaluate(recommenderBuilder, null, model, 0.95, 0.05)println(s"Score=$score")val recommender=recommenderBuilder.buildRecommender(model)val users=trainingRatings.map(_.user).distinct().take(20)import scala.collection.JavaConversions._val result=users.par.map&#123;user=&gt; user+","+recommender.recommend(user,40).map(_.getItemID).mkString(",")&#125; https://github.com/sujitpal/mia-scala-examples上面有一个评估基于物品或是用户的各种相似度下的评分的类，叫做 RecommenderEvaluator，供大家学习参考。 分布式运行Mahout提供了org.apache.mahout.cf.taste.hadoop.item.RecommenderJob类以MapReduce的方式来实现基于物品的协同过滤，查看该类的使用说明： 12345678910111213141516171819202122232425262728293031ubuntu@Master:~/data$ mahout org.apache.mahout.cf.taste.hadoop.item.RecommenderJobMAHOUT_LOCAL is not set; adding HADOOP_CONF_DIR to classpath.Running on hadoop, using /home/ubuntu/hadoop/bin/hadoop and HADOOP_CONF_DIR=/home/ubuntu/hadoop/etc/hadoopMAHOUT-JOB: /home/ubuntu/apache-mahout-distribution-0.10.1/mahout-examples-0.10.1-job.jar16/07/25 07:41:32 WARN driver.MahoutDriver: No org.apache.mahout.cf.taste.hadoop.item.RecommenderJob.props found on classpath, will use command-line arguments only16/07/25 07:41:33 ERROR common.AbstractJob: Missing required option --similarityClassnameMissing required option --similarityClassname Usage: [--input &lt;input&gt; --output &lt;output&gt; --numRecommendations &lt;numRecommendations&gt; --usersFile &lt;usersFile&gt; --itemsFile &lt;itemsFile&gt; --filterFile &lt;filterFile&gt; --userItemFile &lt;userItemFile&gt; --booleanData &lt;booleanData&gt; --maxPrefsPerUser &lt;maxPrefsPerUser&gt; --minPrefsPerUser &lt;minPrefsPerUser&gt; --maxSimilaritiesPerItem &lt;maxSimilaritiesPerItem&gt; --maxPrefsInItemSimilarity &lt;maxPrefsInItemSimilarity&gt; --similarityClassname &lt;similarityClassname&gt; --threshold &lt;threshold&gt; --outputPathForSimilarityMatrix &lt;outputPathForSimilarityMatrix&gt; --randomSeed &lt;randomSeed&gt; --sequencefileOutput --help --tempDir &lt;tempDir&gt; --startPhase &lt;startPhase&gt; --endPhase &lt;endPhase&gt;] --similarityClassname (-s) similarityClassname Name of distributed similarity measures class to instantiate, alternatively use one of the predefined similarities ([SIMILARITY_COOCCURRENCE, SIMILARITY_LOGLIKELIHOOD, SIMILARITY_TANIMOTO_COEFFICIEN T, SIMILARITY_CITY_BLOCK, SIMILARITY_COSINE, SIMILARITY_PEARSON_CORRELATION , SIMILARITY_EUCLIDEAN_DISTANCE] ) 也可输入mahout org.apache.mahout.cf.taste.hadoop.item.RecommenderJob --help查看详细说明 可见，该类可以接收的命令行参数如下： --input(path)(-i): 存储用户偏好数据的目录，该目录下可以包含一个或多个存储用户偏好数据的文本文件；--output(path)(-o): 结算结果的输出目录--numRecommendations (integer): 为每个用户推荐的item数量，默认为10--usersFile (path): 指定一个包含了一个或多个存储userID的文件路径，仅为该路径下所有文件包含的userID做推荐计算 (该选项可选)--itemsFile (path): 指定一个包含了一个或多个存储itemID的文件路径，仅为该路径下所有文件包含的itemID做推荐计算 (该选项可选)--filterFile (path): 指定一个路径，该路径下的文件包含了[userID,itemID]值对，userID和itemID用逗号分隔。计算结果将不会为user推荐[userID,itemID]值对中包含的item (该选项可选)--booleanData (boolean): 如果输入数据不包含偏好数值，则将该参数设置为true，默认为false--maxPrefsPerUser (integer): 在最后计算推荐结果的阶段，针对每一个user使用的偏好数据的最大数量，默认为10--minPrefsPerUser (integer): 在相似度计算中，忽略所有偏好数据量少于该值的用户，默认为1--maxSimilaritiesPerItem (integer): 针对每个item的相似度最大值，默认为100--maxPrefsPerUserInItemSimilarity (integer): 在item相似度计算阶段，针对每个用户考虑的偏好数据最大数量，默认为1000--similarityClassname (classname): 向量相似度计算类outputPathForSimilarityMatrix：SimilarityMatrix输出目录--randomSeed：随机种子 –sequencefileOutput：序列文件输出路径--tempDir (path): 存储临时文件的目录，默认为当前用户的home目录下的temp目录--startPhase--endPhase--threshold (double): 忽略相似度低于该阀值的item对 一个例子如下，使用SIMILARITY_LOGLIKELIHOOD相似度推荐物品：1$ hadoop jar /usr/lib/mahout/mahout-examples-0.9-cdh5.4.0-job.jar org.apache.mahout.cf.taste.hadoop.item.RecommenderJob --input /tmp/mahout/part-00000 --output /tmp/mahout-out -s SIMILARITY_LOGLIKELIHOOD 自己运行的例子如下： 部分实验数据：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253541 25 0.01363162221 116 0.00908774811 5 0.00454387411 23 0.18629883681 17 0.02726324441 3 1.41223606021 11 0.03635099251 12 0.45438740681 120 0.00272632441 93 0.01363162221 21 0.00363509931 6 0.76882349221 47 0.00181754961 66 0.04543874071 27 0.02544569481 44 0.02453692001 315 0.00454387411 28 0.05452648881 138 0.06361423691 108 0.00454387411 1 7.26953207321 85 12.51885773591 168 0.054526488810 4 0.677200902910 6 0.677200902910 217 0.011286681710 2 1.760722347610 1 1.8735891648100 25 0.4788867023100 5 0.0047793084100 17 0.2915378128100 26 0.0047793084100 3 0.1194827101100 11 0.6987348890100 4 0.6652797301100 12 0.0047793084100 30 0.0736013495100 32 0.5257239247100 31 0.0076468934100 37 0.0430137757100 29 0.0592634242100 44 0.0009558617100 13 4.4313747540100 1 9.5461906101100 10 0.04396963731000 8 0.29020556231000 14 0.04836759371000 5 0.07255139061000 9 0.07255139061000 26 0.38694074971000 3 0.14510278111000 436 0.01209189841000 2 3.91777509071000 15 2.4304715840 1ubuntu@Master:~/data$ mahout org.apache.mahout.cf.taste.hadoop.item.RecommenderJob -i /test/item/ckm_pre_result1000000.txt -o /test/item/outputPersonCorr --similarityClassname org.apache.mahout.math.hadoop.similarity.cooccurrence.measures.PearsonCorrelationSimilarity 部分结果：123451 [196:10.482155,14:9.271373,145:8.875873,177:7.779633,360:7.537198,114:7.1917353,635:7.085346,75:6.8879533,235:6.796164,210:6.586777]2 [386:4.339762,631:4.2806735,194:4.274664,153:4.1018524,362:3.7975848,934:3.422003,195:3.0110214,188:2.7676048,30:2.6990044,746:2.693153]3 [45:4.2422075,212:4.1731844,270:3.9618893,309:3.960001,204:3.933118,275:3.6498196,321:3.6286862,179:3.487534,240:3.3450491,170:3.28568]4 [293:4.3900704,746:3.9469879,51:3.3795352,52:3.3444872,312:2.818981,24:2.719058,649:2.2690945,28:2.1947412,196:2.170363,145:2.008545]5 [590:1.1531421,332:1.1508745,336:1.134177,852:1.1335075,561:1.121143,36:1.1099223,535:1.0878772,129:1.0850264,236:1.0413511,83:1.0349866] 默认情况下，mahout使用的reduce数目为1，这样造成大数据处理时效率较低，可以通过参数mahout执行脚本中的MAHOUT_OPTS中的-Dmapred.reduce.tasks参数指定reduce数目。 上面命令运行完成之后，会在当前用户的hdfs主目录生成temp目录，该目录可由–tempDir (path)参数设置： 123456789101112$ hadoop fs -ls tempFound 10 items-rw-r--r-- 3 root hadoop 7 2015-06-10 14:42 temp/maxValues.bin-rw-r--r-- 3 root hadoop 5522717 2015-06-10 14:42 temp/norms.bindrwxr-xr-x - root hadoop 0 2015-06-10 14:41 temp/notUsed-rw-r--r-- 3 root hadoop 7 2015-06-10 14:42 temp/numNonZeroEntries.bin-rw-r--r-- 3 root hadoop 3452222 2015-06-10 14:41 temp/observationsPerColumn.bindrwxr-xr-x - root hadoop 0 2015-06-10 14:47 temp/pairwiseSimilaritydrwxr-xr-x - root hadoop 0 2015-06-10 14:52 temp/partialMultiplydrwxr-xr-x - root hadoop 0 2015-06-10 14:39 temp/preparePreferenceMatrixdrwxr-xr-x - root hadoop 0 2015-06-10 14:50 temp/similarityMatrixdrwxr-xr-x - root hadoop 0 2015-06-10 14:42 temp/weights 观察yarn的管理界面，该命令会生成9个任务，任务名称依次是： PreparePreferenceMatrixJob-ItemIDIndexMapper-Reducer PreparePreferenceMatrixJob-ToItemPrefsMapper-Reducer PreparePreferenceMatrixJob-ToItemVectorsMapper-Reducer RowSimilarityJob-CountObservationsMapper-Reducer RowSimilarityJob-VectorNormMapper-Reducer RowSimilarityJob-CooccurrencesMapper-Reducer RowSimilarityJob-UnsymmetrifyMapper-Reducer partialMultiply RecommenderJob-PartialMultiplyMapper-Reducer 从任务名称，大概可以知道每个任务在做什么，如果你的输入参数不一样，生成的任务数可能不一样，这个需要测试一下才能确认。 在hdfs上查看输出的结果，用户和推荐结果用\t分隔，推荐结果中物品之间用逗号分隔，物品后面通过冒号连接评分：12843 [10709679:4.8334665,8389878:4.833426,9133835:4.7503786,10366169:4.7503185,9007487:4.750272,8149253:4.7501993,10366165:4.750115,9780049:4.750108,8581254:4.750071,10456307:4.7500467]6253 [10117445:3.0375953,10340299:3.0340924,8321090:3.0340924,10086615:3.032164,10436801:3.0187714,9668385:3.0141575,8502110:3.013954,10476325:3.0074399,10318667:3.0004222,8320987:3.0003839] 使用Java API方式执行，请参考Mahout分步式程序开发 基于物品的协同过滤ItemCF。 在Scala或者Spark中，可以以Java API或者命令方式运行，最后还可以通过Spark来处理推荐的结果，例如：过滤、去重、补足数据，这部分内容不做介绍。 本文基本转载自：转载自JavaChen Blog，作者：JavaChen，文章地址http://blog.javachen.com/2015/06/10/collaborative-filtering-using-mahout.html 其他参考资料: 用Hadoop构建电影推荐系统(自己实现分布式) 用Mahout构建职位推荐引擎(单机) Mahout构建图书推荐系统(单机) Mahout分步式程序开发 基于物品的协同过滤ItemCF（调用接口） mahout分布式：Item-based推荐 Introduction to Item-Based Recommendations with Hadoop 使用Mahout搭建推荐系统之入门篇4-Mahout实战 基于MapReduce的ItemBase推荐算法的共现矩阵实现]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Mahout</tag>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VMWare虚拟机：“锁定文件失败”的解决方法]]></title>
    <url>%2F2016%2F07%2F19%2FVMWare%E8%99%9A%E6%8B%9F%E6%9C%BA%EF%BC%9A%E2%80%9C%E9%94%81%E5%AE%9A%E6%96%87%E4%BB%B6%E5%A4%B1%E8%B4%A5%E2%80%9D%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[问题描述如果使用VMWare虚拟机的时候突然系统崩溃蓝屏或者强行关机，有一定几率会导致无法启动，会提示：锁定文件失败，打不开磁盘或快照所依赖的磁盘。 虚拟磁盘(.vmdk)本身有一个磁盘保护机制，为了防止多台虚拟机同时访问同一个虚拟磁盘(.vmdk)带来的数据丢失和性能削减方面的隐患，每次启动虚拟机的时候虚拟机会使用扩展名为.lck（磁盘锁）文件对虚拟磁盘(.vmdk)进行锁定保护。当虚拟机关闭时.lck（磁盘锁）文件自动删除。但是可能由于您非正常关闭虚拟机，这时虚拟机还没来得及删除您系统上的.lck（磁盘锁）文件，所以当下次您启动虚拟机的时候出现了上述错误。 解决方法打开你存放虚拟机系统文件的文件夹，注意，是系统文件，不是虚拟机的安装目录。搜索关键字*.lck，删除搜索到的文件即可。]]></content>
      <categories>
        <category>计算机技巧</category>
      </categories>
      <tags>
        <tag>VMWare</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[亚马逊EC2搭建Shadowsocks]]></title>
    <url>%2F2016%2F07%2F19%2F%E4%BA%9A%E9%A9%AC%E9%80%8AEC2%E6%90%AD%E5%BB%BAShadowsocks%2F</url>
    <content type="text"><![CDATA[在网上寻找优秀的科学上网方式中发现，自己搭建Shadowscoks是非常不错的方式，并且AWS有一年的免费使用期限，到期后再买别的VPS就行了。https://segmentfault.com/a/1190000003101075上面连接的教程从注册申请AWS到搭建服务已经非常详细，自己再做一些补充。 服务器端配置在EC2上创建好Linux服务器后（本人为Ubuntu）需要对其安装环境参考官方文档使用 PuTTY 从 Windows 连接到 Linux 实例连接服务器 安装shadowsocks依赖 sudo -s // 获取超级管理员权限 apt-get update // 更新apt-get apt-get install python-pip // 安装python包管理工具pip pip install shadowsocks // 安装shadowsocks 配置shadowsocksvim /etc/shadowsocks.json 单一端口配置12345678910&#123;"server":"server_ip", #EC2实例的IP，注意这里我们不能填写公有IP，需要填写私有IP或者0.0.0.0 填0.0.0.0即可"server_port":8388, #server端监听的端口，需要在EC2实例中开放此端口"local_address": "127.0.0.1","local_port":1080,"password":"password", #密码"timeout":300,"method":"aes-256-cfb", #加密方式"fast_open": false #是否开启fast open&#125; 如果想要把VPN分享给其它人而不泄露自己的密码，也可以在配置文件中设置多端口+多密码的模式，如：12345678910111213&#123;"server":"server_ip", #EC2实例的IP，注意这里我们不能填写公有IP，需要填写私有IP或者0.0.0.0"local_address": "127.0.0.1","local_port":1080,"port_password":&#123;"8088”: “password8088”,"8089”: "password8089”&#125;"timeout":300,"method":"aes-256-cfb", #加密方式"fast_open": false #是否开启fast open&#125; 配置完成后启动Shawdowsocks1234启动：ssserver -c /etc/shadowsocks.json -d start停止：ssserver -c /etc/shadowsocks.json -d stop重启：ssserver -c /etc/shadowsocks.json -d restart查看状态：ssserver -c /etc/shadowsocks.json -d status 关闭服务器防火墙sudo ufw disable 开启AWS入站端口配置好shaodowsocks后，还需要将配置中的端口打开,这样客户端的服务才能链接得上EC2中的shadowsocks服务。在EC2网页中编辑入站规则将配置文件中的端口号（如8388）加入入站规则。服务器端配置完毕。 客户端下载shadowsocks下载地址1shadowsocks下载地址2windows10尽量使用2.3版本，否则可能出现500或者502错误http://pan.baidu.com/s/1hqIk4mS 500或者502错误 使用2.3版本 或者尝试以下命令123netsh interface ipv4 resetnetsh interface ipv6 resetnetsh winsock reset]]></content>
      <categories>
        <category>计算机技巧</category>
      </categories>
      <tags>
        <tag>科学上网</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[window下搭建eclipse运行MapReduce环境]]></title>
    <url>%2F2016%2F07%2F13%2Fwindow%E4%B8%8B%E6%90%AD%E5%BB%BAeclipse%E8%BF%90%E8%A1%8CMapReduce%E7%8E%AF%E5%A2%83%2F</url>
    <content type="text"><![CDATA[系统环境及所需文件 eclipse-jee-mars-2 hadoop2.7.2 hadoop-eclipse-plugin hadoop.dll &amp; winutils.exe 修改Master节点的hdfs-site.xml&lt;property&gt; &lt;name&gt;dfs.permissions&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; 旨在取消权限检查 1234&lt;property&gt; &lt;name&gt;dfs.web.ugi&lt;/name&gt; &lt;value&gt;Skye,supergroup&lt;/value&gt; &lt;/property&gt; 配置Hadoop插件 windows下载hadoop-2.7.2解压到某目录下，如：E:\hadoop\hadoop-2.7.2 下载hadoop-eclipse-plugin插件hadoop-eclipse-plugin，将release目录下的hadoop-eclipse-plugin-2.6.0.jar拷贝到eclipse/plugins，重启eclipse。 插件配置windows-&gt;show view-&gt;other 显示mapreduce视图 window-&gt;preferences-&gt;hadoop map/reduce 指定windows上的hadoop根目录（即：E:\hadoop\hadoop-2.7.2） 在Map/Reduce Locations 面板中，点击小象图标定义hadoop解释：MapReduce MasterHost：虚拟机hadoop master对应ipPort：hdfs-site.xml中dfs.datanode.ipc.address指定的的端口号。此处填9001DFS Master中Port：core-site.xml中fs.defaultFS指定的端口。应填9000User name：linux中运行hadoop的用户。 配置完毕查看结果 windows下运行环境配置 在系统环境变量中增加HADOOP_HOME，并在Path中加入%HADOOP_HOME%\bin 将下载下来的hadoop.dll,winutils.exe拷贝到HADOOP_HOME/bin目录下 创建 MapReduce工程并运行需要拷贝 服务器hadoop中的log4j.properties文件到工程的src目录 run on hadoop 运行时报如下错误，弄了好长一段时间，发现原因是服务器通过内网ip访问，外网无法解析。用虚拟机连接成功.12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747516/07/13 10:42:38 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.16/07/13 10:42:39 INFO mapreduce.Job: Job job_local510776960_0001 running in uber mode : false16/07/13 10:42:39 INFO mapreduce.Job: map 0% reduce 0%16/07/13 10:42:39 INFO mapred.Task: Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@3bfe5dd716/07/13 10:42:39 INFO mapred.MapTask: Processing split: hdfs://Master:9000/test/test3.txt:0+25916/07/13 10:42:39 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)16/07/13 10:42:39 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 10016/07/13 10:42:39 INFO mapred.MapTask: soft limit at 8388608016/07/13 10:42:39 INFO mapred.MapTask: bufstart = 0; bufvoid = 10485760016/07/13 10:42:39 INFO mapred.MapTask: kvstart = 26214396; length = 655360016/07/13 10:42:39 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer16/07/13 10:43:00 WARN hdfs.BlockReaderFactory: I/O error constructing remote block reader.java.net.ConnectException: Connection timed out: no further information at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) at sun.nio.ch.SocketChannelImpl.finishConnect(Unknown Source) at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206) at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531) at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436) at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777) at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694) at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355) at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:656) at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:882) at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934) at java.io.DataInputStream.read(Unknown Source) at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:59) at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216) at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174) at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:91) at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:144) at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:184) at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:556) at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80) at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91) at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145) at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787) at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341) at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243) at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source) at java.util.concurrent.FutureTask.run(Unknown Source) at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) at java.lang.Thread.run(Unknown Source)16/07/13 10:43:00 WARN hdfs.DFSClient: Failed to connect to /10.0.0.14:50010 for block, add to deadNodes and continue. java.net.ConnectException: Connection timed out: no further informationjava.net.ConnectException: Connection timed out: no further information at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) at sun.nio.ch.SocketChannelImpl.finishConnect(Unknown Source) at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206) at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531) at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3436) at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777) at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694) at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355) at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:656) at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:882) at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934) at java.io.DataInputStream.read(Unknown Source) at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:59) at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216) at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174) at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:91) at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:144) at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:184) at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:556) at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80) at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91) at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145) at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787) at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341) at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243) at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source) at java.util.concurrent.FutureTask.run(Unknown Source) at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) at java.lang.Thread.run(Unknown Source)]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mapreduce任务运行时shuffle Error]]></title>
    <url>%2F2016%2F07%2F11%2Fmapreduce%E4%BB%BB%E5%8A%A1%E8%BF%90%E8%A1%8C%E6%97%B6shuffle%20Error%2F</url>
    <content type="text"><![CDATA[本文引用参考：MapReduce任务Shuffle Error错误相关参考连接： yarn &amp; mapreduce 配置参数总结 错误描述在运行MapReduce任务的时候，出现如下错误：1234567891011121314151617Error: org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#1 at org.apache.hadoop.mapreduce.task.reduce.Shuffle.run(Shuffle.java:134) at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:376) at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:396) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556) at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)Caused by: java.lang.OutOfMemoryError: Java heap space at org.apache.hadoop.io.BoundedByteArrayOutputStream.&lt;init&gt;(BoundedByteArrayOutputStream.java:56) at org.apache.hadoop.io.BoundedByteArrayOutputStream.&lt;init&gt;(BoundedByteArrayOutputStream.java:46) at org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput.&lt;init&gt;(InMemoryMapOutput.java:63) at org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl.unconditionalReserve(MergeManagerImpl.java:297) at org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl.reserve(MergeManagerImpl.java:287) at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyMapOutput(Fetcher.java:411) at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyFromHost(Fetcher.java:341) at org.apache.hadoop.mapreduce.task.reduce.Fetcher.run(Fetcher.java:165) 解决方案 根据《Hadoop:The Definitive Guide 4th Edition》所述(P203-219)，map任务和reduce任务之间要经过一个shuffle过程，该过程复制map任务的输出作为reduce任务的输入。 具体的来说，shuffle过程的输入是：map任务的输出文件，它的输出接收者是：运行reduce任务的机子上的内存buffer，并且shuffle过程以并行方式运行。 参数mapreduce.reduce.shuffle.input.buffer.percent控制运行reduce任务的机子上多少比例的内存用作上述buffer(默认值为0.70)，参数mapreduce.reduce.shuffle.parallelcopies控制shuffle过程的并行度(默认值为5)。那么”mapreduce.reduce.shuffle.input.buffer.percent” * “mapreduce.reduce.shuffle.parallelcopies” 必须小于等于1，否则就会出现如上错误因此，我将mapreduce.reduce.shuffle.input.buffer.percent设置成值为0.1，就可以正常运行了（设置成0.2，还是会抛同样的错） job.getConfiguration().setStrings(&quot;mapreduce.reduce.shuffle.input.buffer.percent&quot;, &quot;0.1&quot;);或者在maperd-site.xml中修改1234&lt;property&gt; &lt;name&gt;mapreduce.reduce.input.buffer.percent&lt;/name&gt; &lt;value&gt;0.0&lt;/value&gt;&lt;/property&gt; 另外，可以发现如果使用两个参数的默认值，那么两者乘积为3.5，大大大于1了，为什么没有经常抛出以上的错误呢？1)首先，把默认值设为比较大，主要是基于性能考虑，将它们设为比较大，可以大大加快从map复制数据的速度2)其次，要抛出如上异常，还需满足另外一个条件，就是map任务的数据一下子准备好了等待shuffle去复制，在这种情况下，就会导致shuffle过程的“线程数量”和“内存buffer使用量”都是满负荷的值，自然就造成了内存不足的错误；而如果map任务的数据是断断续续完成的，那么没有一个时刻shuffle过程的“线程数量”和“内存buffer使用量”是满负荷值的，自然也就不会抛出如上错误 另外，如果在设置以上参数后，还是出现错误，那么有可能是运行Reduce任务的进程的内存总量不足，可以通过mapred.child.java.opts参数来调节，比如设置mapred.child.java.opts=-Xmx2024m]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce 多文件输入]]></title>
    <url>%2F2016%2F06%2F16%2FMapReduce%20%E5%A4%9A%E6%96%87%E4%BB%B6%E8%BE%93%E5%85%A5%2F</url>
    <content type="text"><![CDATA[多路径输入 FileInputFormat.addInputPath 多次调用加载不同路径 12FileInputFormat.addInputPath(job, new Path(args[0]));FileInputFormat.addInputPath(job, new Path(args[1])); FileInputFormat.addInputPaths一次调用加载 多路径字符串用逗号隔开 1FileInputFormat.addInputPaths(job, "hdfs://master:9000/cs/path1,hdfs://RS5-112:9000/cs/path2"); 多种输入**MultipleInputs可以加载不同路径的输入文件，并且每个路径可用不同的 12maperMultipleInputs.addInputPath(job, new Path("hdfs://master:9000/cs/path1"), TextInputFormat.class,MultiTypeFileInput1Mapper.class);MultipleInputs.addInputPath(job, new Path("hdfs://master:9000/cs/path3"), TextInputFormat.class,MultiTypeFileInput3Mapper.class); 网上例子：package example; import java.io.IOException; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.LongWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Job; import org.apache.hadoop.mapreduce.Mapper; import org.apache.hadoop.mapreduce.Reducer; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; import org.apache.hadoop.mapreduce.lib.input.MultipleInputs; import org.apache.hadoop.mapreduce.lib.input.TextInputFormat; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; /** * 多类型文件输入 * @author lijl * */ public class MultiTypeFileInputMR { static class MultiTypeFileInput1Mapper extends Mapper&lt;LongWritable, Text, Text, Text&gt;{ public void map(LongWritable key,Text value,Context context){ try { String[] str = value.toString().split(&quot;\\|&quot;); context.write(new Text(str[0]), new Text(str[1])); } catch (IOException e) { e.printStackTrace(); } catch (InterruptedException e) { e.printStackTrace(); } } } static class MultiTypeFileInput3Mapper extends Mapper&lt;LongWritable, Text, Text, Text&gt;{ public void map(LongWritable key,Text value,Context context){ try { String[] str = value.toString().split(&quot;&quot;); context.write(new Text(str[0]), new Text(str[1])); } catch (IOException e) { e.printStackTrace(); } catch (InterruptedException e) { e.printStackTrace(); } } } static class MultiTypeFileInputReducer extends Reducer&lt;Text, Text, Text, Text&gt;{ public void reduce(Text key,Iterable&lt;Text&gt; values,Context context){ try { for(Text value:values){ context.write(key,value); } } catch (IOException e) { e.printStackTrace(); } catch (InterruptedException e) { e.printStackTrace(); } } } public static void main(String[] args) throws IOException, InterruptedException, ClassNotFoundException { Configuration conf = new Configuration(); conf.set(&quot;mapred.textoutputformat.separator&quot;, &quot;,&quot;); Job job = new Job(conf,&quot;MultiPathFileInput&quot;); job.setJarByClass(MultiTypeFileInputMR.class); FileOutputFormat.setOutputPath(job, new Path(&quot;hdfs://RS5-112:9000/cs/path6&quot;)); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(Text.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(Text.class); job.setReducerClass(MultiTypeFileInputReducer.class); job.setNumReduceTasks(1); MultipleInputs.addInputPath(job, new Path(&quot;hdfs://RS5-112:9000/cs/path1&quot;), TextInputFormat.class,MultiTypeFileInput1Mapper.class); MultipleInputs.addInputPath(job, new Path(&quot;hdfs://RS5-112:9000/cs/path3&quot;), TextInputFormat.class,MultiTypeFileInput3Mapper.class); System.exit(job.waitForCompletion(true)?0:1); } } 自己例子QLMapper.java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051package com.hdu.mr;import java.io.IOException;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;public class QLMapper extends Mapper&lt;LongWritable, Text, Text, LongWritable&gt; &#123; String[] mbUnlike = &#123; "盒子", "助手", "输入法", "平台" &#125;; String mbdylxLike = "游戏"; String mbdylxUnlike = "网页游戏"; String delWeb = "访问网站"; Text outputValue = new Text(); @Override protected void map(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, LongWritable&gt;.Context context) throws IOException, InterruptedException &#123; // 接收数据v1 String line = value.toString(); // 切分数据 String[] words = line.split(""); // String[] words = line.split("\t"); boolean flag = true; for (int i = 0; i &lt; 4; i++) &#123; if (words.length &lt; 5) &#123; // 过滤 长度小于4的信息 即访问网站等 flag = false; break; &#125; if (words[3].indexOf(mbUnlike[i]) != -1) &#123; // 有其中一个则为false flag = false; break; &#125; &#125; if (flag == true) &#123; if (words[4].indexOf(mbdylxUnlike) != -1) &#123; // 有网页游戏则为false flag = false; &#125; else if (words[4].indexOf(mbdylxLike) == -1) &#123; // 没有游戏则为false flag = false; &#125; &#125; if (flag == true) &#123; outputValue.set(line); context.write(outputValue, new LongWritable(1L)); &#125; &#125;&#125; QLReducer.java1234567891011121314151617181920package com.hdu.mr;import java.io.IOException;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;public class QLReducer extends Reducer&lt;Text, LongWritable, Text, NullWritable&gt; &#123; @Override protected void reduce(Text key, Iterable&lt;LongWritable&gt; values, Reducer&lt;Text, LongWritable, Text, NullWritable&gt;.Context context) throws IOException, InterruptedException &#123; // 接收数据 // 输出 context.write(key, NullWritable.get()); &#125;&#125; DataClean.java1234567891011121314151617181920package com.hdu.mr;import java.io.IOException;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;public class QLReducer extends Reducer&lt;Text, LongWritable, Text, NullWritable&gt; &#123; @Override protected void reduce(Text key, Iterable&lt;LongWritable&gt; values, Reducer&lt;Text, LongWritable, Text, NullWritable&gt;.Context context) throws IOException, InterruptedException &#123; // 接收数据 // 输出 context.write(key, NullWritable.get()); &#125;&#125;]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HBase集群搭建]]></title>
    <url>%2F2016%2F06%2F16%2FHBase%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[1. 上传hbase安装包2. 解压3. 配置hbase集群，要修改3个文件（首先zk集群已经安装好了）注意：要把hadoop的hdfs-site.xml和core-site.xml 放到hbase/conf下 vim hbase-env.shexport JAVA_HOME=/usr/java/jdk1.7.0_55 //告诉hbase使用外部的zk export HBASE_MANAGES_ZK=false vim hbase-site.xml &lt;configuration&gt; &lt;!-- 指定hbase在HDFS上存储的路径 --&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://ns1/hbase&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定hbase是分布式的 --&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定zk的地址，多个用“,”分割 --&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;itcast04:2181,itcast05:2181,itcast06:2181&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; vim regionserversitcast03 itcast04 itcast05 itcast06 4. 将配置好的HBase拷贝到每一个节点并同步时间scp -r /itcast/hbase-0.96.2-hadoop2/ itcast02:/itcast/ scp -r /itcast/hbase-0.96.2-hadoop2/ itcast03:/itcast/ scp -r /itcast/hbase-0.96.2-hadoop2/ itcast04:/itcast/ scp -r /itcast/hbase-0.96.2-hadoop2/ itcast05:/itcast/ scp -r /itcast/hbase-0.96.2-hadoop2/ itcast06:/itcast/ 5. 启动所有的hbase分别启动zk ./zkServer.sh start 启动hbase集群 start-dfs.sh 启动hbase，在主节点上运行： start-hbase.sh 6.通过浏览器访问hbase管理页面192.168.1.201:60010 7.为保证集群的可靠性，要启动多个HMaster,在itcast02上启动hbase-daemon.sh start master]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>HBase</tag>
      </tags>
  </entry>
</search>